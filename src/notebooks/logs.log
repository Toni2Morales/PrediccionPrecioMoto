2023-04-26 17:46:56,503:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-26 17:46:56,503:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-26 17:46:56,503:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-26 17:46:56,503:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-26 17:46:59,013:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-04-26 17:47:32,860:INFO:PyCaret RegressionExperiment
2023-04-26 17:47:32,860:INFO:Logging name: price
2023-04-26 17:47:32,860:INFO:ML Usecase: MLUsecase.REGRESSION
2023-04-26 17:47:32,860:INFO:version 3.0.0
2023-04-26 17:47:32,861:INFO:Initializing setup()
2023-04-26 17:47:32,861:INFO:self.USI: dd7d
2023-04-26 17:47:32,861:INFO:self._variable_keys: {'exp_id', 'n_jobs_param', 'seed', 'log_plots_param', 'X', 'X_test', 'X_train', '_ml_usecase', 'USI', 'exp_name_log', 'y_test', 'gpu_n_jobs_param', 'gpu_param', 'fold_groups_param', 'y', 'data', 'fold_shuffle_param', 'y_train', 'fold_generator', 'target_param', 'pipeline', 'html_param', 'transform_target_param', 'idx', 'memory', '_available_plots', 'logging_param'}
2023-04-26 17:47:32,861:INFO:Checking environment
2023-04-26 17:47:32,862:INFO:python_version: 3.7.4
2023-04-26 17:47:32,862:INFO:python_build: ('tags/v3.7.4:e09359112e', 'Jul  8 2019 20:34:20')
2023-04-26 17:47:32,862:INFO:machine: AMD64
2023-04-26 17:47:32,862:INFO:platform: Windows-10-10.0.19041-SP0
2023-04-26 17:47:32,865:INFO:Memory: svmem(total=16907886592, available=2077110272, percent=87.7, used=14830776320, free=2077110272)
2023-04-26 17:47:32,865:INFO:Physical Core: 4
2023-04-26 17:47:32,866:INFO:Logical Core: 8
2023-04-26 17:47:32,866:INFO:Checking libraries
2023-04-26 17:47:32,866:INFO:System:
2023-04-26 17:47:32,866:INFO:    python: 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]
2023-04-26 17:47:32,866:INFO:executable: c:\Users\tonim\AppData\Local\Programs\Python\Python37\python.exe
2023-04-26 17:47:32,866:INFO:   machine: Windows-10-10.0.19041-SP0
2023-04-26 17:47:32,866:INFO:PyCaret required dependencies:
2023-04-26 17:47:32,867:INFO:                 pip: 22.2.2
2023-04-26 17:47:32,867:INFO:          setuptools: 65.3.0
2023-04-26 17:47:32,867:INFO:             pycaret: 3.0.0
2023-04-26 17:47:32,867:INFO:             IPython: 7.34.0
2023-04-26 17:47:32,867:INFO:          ipywidgets: 7.7.1
2023-04-26 17:47:32,867:INFO:                tqdm: 4.64.0
2023-04-26 17:47:32,867:INFO:               numpy: 1.21.6
2023-04-26 17:47:32,867:INFO:              pandas: 1.3.5
2023-04-26 17:47:32,867:INFO:              jinja2: 3.1.2
2023-04-26 17:47:32,867:INFO:               scipy: 1.5.4
2023-04-26 17:47:32,867:INFO:              joblib: 1.2.0
2023-04-26 17:47:32,867:INFO:             sklearn: 1.0.2
2023-04-26 17:47:32,867:INFO:                pyod: 1.0.9
2023-04-26 17:47:32,867:INFO:            imblearn: 0.9.0
2023-04-26 17:47:32,867:INFO:   category_encoders: 2.6.0
2023-04-26 17:47:32,867:INFO:            lightgbm: 3.3.5
2023-04-26 17:47:32,867:INFO:               numba: 0.56.0
2023-04-26 17:47:32,867:INFO:            requests: 2.28.1
2023-04-26 17:47:32,867:INFO:          matplotlib: 3.5.2
2023-04-26 17:47:32,867:INFO:          scikitplot: 0.3.7
2023-04-26 17:47:32,867:INFO:         yellowbrick: 1.5
2023-04-26 17:47:32,867:INFO:              plotly: 5.9.0
2023-04-26 17:47:32,868:INFO:             kaleido: 0.2.1
2023-04-26 17:47:32,868:INFO:         statsmodels: 0.13.2
2023-04-26 17:47:32,868:INFO:              sktime: 0.17.1
2023-04-26 17:47:32,868:INFO:               tbats: 1.1.3
2023-04-26 17:47:32,868:INFO:            pmdarima: 2.0.1
2023-04-26 17:47:32,868:INFO:              psutil: 5.9.1
2023-04-26 17:47:32,868:INFO:PyCaret optional dependencies:
2023-04-26 17:47:32,876:INFO:                shap: 0.41.0
2023-04-26 17:47:32,876:INFO:           interpret: Not installed
2023-04-26 17:47:32,876:INFO:                umap: Not installed
2023-04-26 17:47:32,876:INFO:    pandas_profiling: 3.3.0
2023-04-26 17:47:32,876:INFO:  explainerdashboard: Not installed
2023-04-26 17:47:32,876:INFO:             autoviz: Not installed
2023-04-26 17:47:32,876:INFO:           fairlearn: Not installed
2023-04-26 17:47:32,876:INFO:             xgboost: 1.6.2
2023-04-26 17:47:32,876:INFO:            catboost: Not installed
2023-04-26 17:47:32,876:INFO:              kmodes: Not installed
2023-04-26 17:47:32,876:INFO:             mlxtend: Not installed
2023-04-26 17:47:32,877:INFO:       statsforecast: Not installed
2023-04-26 17:47:32,877:INFO:        tune_sklearn: Not installed
2023-04-26 17:47:32,877:INFO:                 ray: Not installed
2023-04-26 17:47:32,877:INFO:            hyperopt: 0.2.7
2023-04-26 17:47:32,877:INFO:              optuna: Not installed
2023-04-26 17:47:32,877:INFO:               skopt: Not installed
2023-04-26 17:47:32,877:INFO:              mlflow: 1.30.1
2023-04-26 17:47:32,877:INFO:              gradio: Not installed
2023-04-26 17:47:32,877:INFO:             fastapi: Not installed
2023-04-26 17:47:32,877:INFO:             uvicorn: Not installed
2023-04-26 17:47:32,877:INFO:              m2cgen: Not installed
2023-04-26 17:47:32,877:INFO:           evidently: Not installed
2023-04-26 17:47:32,877:INFO:               fugue: Not installed
2023-04-26 17:47:32,877:INFO:           streamlit: 1.11.0
2023-04-26 17:47:32,877:INFO:             prophet: Not installed
2023-04-26 17:47:32,877:INFO:None
2023-04-26 17:47:32,878:INFO:Set up data.
2023-04-26 17:47:51,789:INFO:PyCaret RegressionExperiment
2023-04-26 17:47:51,789:INFO:Logging name: price
2023-04-26 17:47:51,789:INFO:ML Usecase: MLUsecase.REGRESSION
2023-04-26 17:47:51,789:INFO:version 3.0.0
2023-04-26 17:47:51,790:INFO:Initializing setup()
2023-04-26 17:47:51,790:INFO:self.USI: 41bc
2023-04-26 17:47:51,790:INFO:self._variable_keys: {'exp_id', 'n_jobs_param', 'seed', 'log_plots_param', 'X', 'X_test', 'X_train', '_ml_usecase', 'USI', 'exp_name_log', 'y_test', 'gpu_n_jobs_param', 'gpu_param', 'fold_groups_param', 'y', 'data', 'fold_shuffle_param', 'y_train', 'fold_generator', 'target_param', 'pipeline', 'html_param', 'transform_target_param', 'idx', 'memory', '_available_plots', 'logging_param'}
2023-04-26 17:47:51,790:INFO:Checking environment
2023-04-26 17:47:51,790:INFO:python_version: 3.7.4
2023-04-26 17:47:51,790:INFO:python_build: ('tags/v3.7.4:e09359112e', 'Jul  8 2019 20:34:20')
2023-04-26 17:47:51,790:INFO:machine: AMD64
2023-04-26 17:47:51,790:INFO:platform: Windows-10-10.0.19041-SP0
2023-04-26 17:47:51,790:INFO:Memory: svmem(total=16907886592, available=2017890304, percent=88.1, used=14889996288, free=2017890304)
2023-04-26 17:47:51,790:INFO:Physical Core: 4
2023-04-26 17:47:51,790:INFO:Logical Core: 8
2023-04-26 17:47:51,790:INFO:Checking libraries
2023-04-26 17:47:51,790:INFO:System:
2023-04-26 17:47:51,790:INFO:    python: 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]
2023-04-26 17:47:51,790:INFO:executable: c:\Users\tonim\AppData\Local\Programs\Python\Python37\python.exe
2023-04-26 17:47:51,791:INFO:   machine: Windows-10-10.0.19041-SP0
2023-04-26 17:47:51,791:INFO:PyCaret required dependencies:
2023-04-26 17:47:51,791:INFO:                 pip: 22.2.2
2023-04-26 17:47:51,791:INFO:          setuptools: 65.3.0
2023-04-26 17:47:51,791:INFO:             pycaret: 3.0.0
2023-04-26 17:47:51,791:INFO:             IPython: 7.34.0
2023-04-26 17:47:51,791:INFO:          ipywidgets: 7.7.1
2023-04-26 17:47:51,792:INFO:                tqdm: 4.64.0
2023-04-26 17:47:51,792:INFO:               numpy: 1.21.6
2023-04-26 17:47:51,792:INFO:              pandas: 1.3.5
2023-04-26 17:47:51,792:INFO:              jinja2: 3.1.2
2023-04-26 17:47:51,792:INFO:               scipy: 1.5.4
2023-04-26 17:47:51,792:INFO:              joblib: 1.2.0
2023-04-26 17:47:51,793:INFO:             sklearn: 1.0.2
2023-04-26 17:47:51,793:INFO:                pyod: 1.0.9
2023-04-26 17:47:51,793:INFO:            imblearn: 0.9.0
2023-04-26 17:47:51,793:INFO:   category_encoders: 2.6.0
2023-04-26 17:47:51,793:INFO:            lightgbm: 3.3.5
2023-04-26 17:47:51,793:INFO:               numba: 0.56.0
2023-04-26 17:47:51,793:INFO:            requests: 2.28.1
2023-04-26 17:47:51,793:INFO:          matplotlib: 3.5.2
2023-04-26 17:47:51,793:INFO:          scikitplot: 0.3.7
2023-04-26 17:47:51,793:INFO:         yellowbrick: 1.5
2023-04-26 17:47:51,794:INFO:              plotly: 5.9.0
2023-04-26 17:47:51,794:INFO:             kaleido: 0.2.1
2023-04-26 17:47:51,794:INFO:         statsmodels: 0.13.2
2023-04-26 17:47:51,794:INFO:              sktime: 0.17.1
2023-04-26 17:47:51,794:INFO:               tbats: 1.1.3
2023-04-26 17:47:51,794:INFO:            pmdarima: 2.0.1
2023-04-26 17:47:51,794:INFO:              psutil: 5.9.1
2023-04-26 17:47:51,794:INFO:PyCaret optional dependencies:
2023-04-26 17:47:51,794:INFO:                shap: 0.41.0
2023-04-26 17:47:51,795:INFO:           interpret: Not installed
2023-04-26 17:47:51,795:INFO:                umap: Not installed
2023-04-26 17:47:51,795:INFO:    pandas_profiling: 3.3.0
2023-04-26 17:47:51,795:INFO:  explainerdashboard: Not installed
2023-04-26 17:47:51,795:INFO:             autoviz: Not installed
2023-04-26 17:47:51,795:INFO:           fairlearn: Not installed
2023-04-26 17:47:51,795:INFO:             xgboost: 1.6.2
2023-04-26 17:47:51,795:INFO:            catboost: Not installed
2023-04-26 17:47:51,795:INFO:              kmodes: Not installed
2023-04-26 17:47:51,795:INFO:             mlxtend: Not installed
2023-04-26 17:47:51,795:INFO:       statsforecast: Not installed
2023-04-26 17:47:51,795:INFO:        tune_sklearn: Not installed
2023-04-26 17:47:51,795:INFO:                 ray: Not installed
2023-04-26 17:47:51,796:INFO:            hyperopt: 0.2.7
2023-04-26 17:47:51,796:INFO:              optuna: Not installed
2023-04-26 17:47:51,796:INFO:               skopt: Not installed
2023-04-26 17:47:51,796:INFO:              mlflow: 1.30.1
2023-04-26 17:47:51,796:INFO:              gradio: Not installed
2023-04-26 17:47:51,796:INFO:             fastapi: Not installed
2023-04-26 17:47:51,796:INFO:             uvicorn: Not installed
2023-04-26 17:47:51,796:INFO:              m2cgen: Not installed
2023-04-26 17:47:51,796:INFO:           evidently: Not installed
2023-04-26 17:47:51,796:INFO:               fugue: Not installed
2023-04-26 17:47:51,796:INFO:           streamlit: 1.11.0
2023-04-26 17:47:51,796:INFO:             prophet: Not installed
2023-04-26 17:47:51,796:INFO:None
2023-04-26 17:47:51,796:INFO:Set up data.
2023-04-26 17:47:52,309:INFO:Set up train/test split.
2023-04-26 17:47:52,601:INFO:Set up index.
2023-04-26 17:47:52,606:INFO:Set up folding strategy.
2023-04-26 17:47:52,606:INFO:Assigning column types.
2023-04-26 17:47:52,803:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-26 17:47:52,804:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-26 17:47:52,811:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-26 17:47:52,816:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-26 17:47:53,121:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-26 17:47:53,161:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-26 17:47:53,163:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:47:53,507:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:47:53,508:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-26 17:47:53,513:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-26 17:47:53,518:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-26 17:47:53,798:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-26 17:47:53,837:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-26 17:47:53,839:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:47:53,843:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:47:53,843:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-04-26 17:47:53,848:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-26 17:47:53,851:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-26 17:47:57,561:INFO:PyCaret RegressionExperiment
2023-04-26 17:47:57,561:INFO:Logging name: price
2023-04-26 17:47:57,561:INFO:ML Usecase: MLUsecase.REGRESSION
2023-04-26 17:47:57,562:INFO:version 3.0.0
2023-04-26 17:47:57,562:INFO:Initializing setup()
2023-04-26 17:47:57,562:INFO:self.USI: 86c2
2023-04-26 17:47:57,562:INFO:self._variable_keys: {'exp_id', 'n_jobs_param', 'seed', 'log_plots_param', 'X', 'X_test', 'X_train', '_ml_usecase', 'USI', 'exp_name_log', 'y_test', 'gpu_n_jobs_param', 'gpu_param', 'fold_groups_param', 'y', 'data', 'fold_shuffle_param', 'y_train', 'fold_generator', 'target_param', 'pipeline', 'html_param', 'transform_target_param', 'idx', 'memory', '_available_plots', 'logging_param'}
2023-04-26 17:47:57,562:INFO:Checking environment
2023-04-26 17:47:57,562:INFO:python_version: 3.7.4
2023-04-26 17:47:57,562:INFO:python_build: ('tags/v3.7.4:e09359112e', 'Jul  8 2019 20:34:20')
2023-04-26 17:47:57,562:INFO:machine: AMD64
2023-04-26 17:47:57,562:INFO:platform: Windows-10-10.0.19041-SP0
2023-04-26 17:47:57,562:INFO:Memory: svmem(total=16907886592, available=1904226304, percent=88.7, used=15003660288, free=1904226304)
2023-04-26 17:47:57,562:INFO:Physical Core: 4
2023-04-26 17:47:57,562:INFO:Logical Core: 8
2023-04-26 17:47:57,562:INFO:Checking libraries
2023-04-26 17:47:57,562:INFO:System:
2023-04-26 17:47:57,562:INFO:    python: 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]
2023-04-26 17:47:57,562:INFO:executable: c:\Users\tonim\AppData\Local\Programs\Python\Python37\python.exe
2023-04-26 17:47:57,562:INFO:   machine: Windows-10-10.0.19041-SP0
2023-04-26 17:47:57,562:INFO:PyCaret required dependencies:
2023-04-26 17:47:57,562:INFO:                 pip: 22.2.2
2023-04-26 17:47:57,562:INFO:          setuptools: 65.3.0
2023-04-26 17:47:57,562:INFO:             pycaret: 3.0.0
2023-04-26 17:47:57,563:INFO:             IPython: 7.34.0
2023-04-26 17:47:57,563:INFO:          ipywidgets: 7.7.1
2023-04-26 17:47:57,563:INFO:                tqdm: 4.64.0
2023-04-26 17:47:57,563:INFO:               numpy: 1.21.6
2023-04-26 17:47:57,563:INFO:              pandas: 1.3.5
2023-04-26 17:47:57,563:INFO:              jinja2: 3.1.2
2023-04-26 17:47:57,563:INFO:               scipy: 1.5.4
2023-04-26 17:47:57,563:INFO:              joblib: 1.2.0
2023-04-26 17:47:57,563:INFO:             sklearn: 1.0.2
2023-04-26 17:47:57,563:INFO:                pyod: 1.0.9
2023-04-26 17:47:57,563:INFO:            imblearn: 0.9.0
2023-04-26 17:47:57,563:INFO:   category_encoders: 2.6.0
2023-04-26 17:47:57,563:INFO:            lightgbm: 3.3.5
2023-04-26 17:47:57,563:INFO:               numba: 0.56.0
2023-04-26 17:47:57,563:INFO:            requests: 2.28.1
2023-04-26 17:47:57,563:INFO:          matplotlib: 3.5.2
2023-04-26 17:47:57,563:INFO:          scikitplot: 0.3.7
2023-04-26 17:47:57,563:INFO:         yellowbrick: 1.5
2023-04-26 17:47:57,563:INFO:              plotly: 5.9.0
2023-04-26 17:47:57,563:INFO:             kaleido: 0.2.1
2023-04-26 17:47:57,563:INFO:         statsmodels: 0.13.2
2023-04-26 17:47:57,563:INFO:              sktime: 0.17.1
2023-04-26 17:47:57,563:INFO:               tbats: 1.1.3
2023-04-26 17:47:57,563:INFO:            pmdarima: 2.0.1
2023-04-26 17:47:57,563:INFO:              psutil: 5.9.1
2023-04-26 17:47:57,563:INFO:PyCaret optional dependencies:
2023-04-26 17:47:57,564:INFO:                shap: 0.41.0
2023-04-26 17:47:57,564:INFO:           interpret: Not installed
2023-04-26 17:47:57,564:INFO:                umap: Not installed
2023-04-26 17:47:57,564:INFO:    pandas_profiling: 3.3.0
2023-04-26 17:47:57,564:INFO:  explainerdashboard: Not installed
2023-04-26 17:47:57,564:INFO:             autoviz: Not installed
2023-04-26 17:47:57,564:INFO:           fairlearn: Not installed
2023-04-26 17:47:57,564:INFO:             xgboost: 1.6.2
2023-04-26 17:47:57,564:INFO:            catboost: Not installed
2023-04-26 17:47:57,564:INFO:              kmodes: Not installed
2023-04-26 17:47:57,564:INFO:             mlxtend: Not installed
2023-04-26 17:47:57,564:INFO:       statsforecast: Not installed
2023-04-26 17:47:57,564:INFO:        tune_sklearn: Not installed
2023-04-26 17:47:57,564:INFO:                 ray: Not installed
2023-04-26 17:47:57,564:INFO:            hyperopt: 0.2.7
2023-04-26 17:47:57,564:INFO:              optuna: Not installed
2023-04-26 17:47:57,564:INFO:               skopt: Not installed
2023-04-26 17:47:57,564:INFO:              mlflow: 1.30.1
2023-04-26 17:47:57,564:INFO:              gradio: Not installed
2023-04-26 17:47:57,564:INFO:             fastapi: Not installed
2023-04-26 17:47:57,564:INFO:             uvicorn: Not installed
2023-04-26 17:47:57,564:INFO:              m2cgen: Not installed
2023-04-26 17:47:57,564:INFO:           evidently: Not installed
2023-04-26 17:47:57,564:INFO:               fugue: Not installed
2023-04-26 17:47:57,564:INFO:           streamlit: 1.11.0
2023-04-26 17:47:57,564:INFO:             prophet: Not installed
2023-04-26 17:47:57,564:INFO:None
2023-04-26 17:47:57,564:INFO:Set up data.
2023-04-26 17:47:57,891:INFO:Set up train/test split.
2023-04-26 17:47:58,125:INFO:Set up index.
2023-04-26 17:47:58,130:INFO:Set up folding strategy.
2023-04-26 17:47:58,130:INFO:Assigning column types.
2023-04-26 17:47:58,309:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-26 17:47:58,309:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-26 17:47:58,313:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-26 17:47:58,317:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-26 17:47:58,622:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-26 17:47:58,662:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-26 17:47:58,662:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:47:58,664:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:47:58,665:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-26 17:47:58,669:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-26 17:47:58,673:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-26 17:47:58,981:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-26 17:47:59,030:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-26 17:47:59,031:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:47:59,033:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:47:59,033:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-04-26 17:47:59,040:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-26 17:47:59,044:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-26 17:47:59,360:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-26 17:47:59,402:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-26 17:47:59,403:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:47:59,405:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:47:59,410:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-26 17:47:59,414:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-26 17:47:59,697:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-26 17:47:59,763:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-26 17:47:59,764:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:47:59,766:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:47:59,766:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-04-26 17:47:59,777:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-26 17:48:00,050:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-26 17:48:00,091:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-26 17:48:00,092:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:48:00,094:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:48:00,104:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-26 17:48:00,419:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-26 17:48:00,461:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-26 17:48:00,461:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:48:00,465:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:48:00,465:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-04-26 17:48:00,760:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-26 17:48:00,799:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-26 17:48:00,800:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:48:00,803:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:48:02,197:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-26 17:48:02,239:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-26 17:48:02,240:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:48:02,242:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:48:02,242:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-26 17:48:02,528:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-26 17:48:02,569:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:48:02,572:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:48:02,873:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-26 17:48:02,923:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:48:02,926:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:48:02,926:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-04-26 17:48:03,252:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:48:03,255:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:48:03,580:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:48:03,583:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:48:03,598:INFO:Preparing preprocessing pipeline...
2023-04-26 17:48:03,598:INFO:Set up simple imputation.
2023-04-26 17:48:03,625:INFO:Set up column name cleaning.
2023-04-26 17:48:04,525:INFO:Finished creating preprocessing pipeline.
2023-04-26 17:48:04,539:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power',
                                             'bike_name_BMW F750 GS 850cc',
                                             'bike_name_BMW G 310 GS',
                                             'bike_name_BMW G 310 R',
                                             'bike_name_BMW S 1000 RR Pro',
                                             'bike_name_BMW S 1000 XR Pro',
                                             'bike_name_Bajaj  Pulsar 180cc',
                                             'bike_name_Bajaj Avenger 150cc',...
                                             'bike_name_Bajaj CT 100 Alloy',
                                             'bike_name_Bajaj CT 100 B',
                                             'bike_name_Bajaj CT 100 ES Alloy',
                                             'bike_name_Bajaj CT 100 KS Alloy',
                                             'bike_name_Bajaj CT 100 Spoke', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-04-26 17:48:04,539:INFO:Creating final display dataframe.
2023-04-26 17:48:08,001:INFO:Setup _display_container:                     Description         Value
0                    Session id           123
1                        Target         price
2                   Target type    Regression
3           Original data shape  (32648, 942)
4        Transformed data shape  (32648, 942)
5   Transformed train set shape  (22853, 942)
6    Transformed test set shape   (9795, 942)
7              Numeric features           941
8                    Preprocess          True
9               Imputation type        simple
10           Numeric imputation          mean
11       Categorical imputation          mode
12               Fold Generator         KFold
13                  Fold Number            10
14                     CPU Jobs            -1
15                      Use GPU         False
16               Log Experiment  MlflowLogger
17              Experiment Name         price
18                          USI          86c2
2023-04-26 17:48:08,548:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:48:08,550:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:48:08,876:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 17:48:08,880:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 17:48:08,880:INFO:Logging experiment in loggers
2023-04-26 17:48:11,093:INFO:SubProcess save_model() called ==================================
2023-04-26 17:48:11,113:INFO:Initializing save_model()
2023-04-26 17:48:11,113:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power',
                                             'bike_name_BMW F750 GS 850cc',
                                             'bike_name_BMW G 310 GS',
                                             'bike_name_BMW G 310 R',
                                             'bike_name_BMW S 1000 RR Pro',
                                             'bike_name_BMW S 1000 XR Pro',
                                             'bike_name_Bajaj  Pulsar 180cc',
                                             'bike_name_Bajaj Avenger 150cc',...
                                             'bike_name_Bajaj CT 100 Alloy',
                                             'bike_name_Bajaj CT 100 B',
                                             'bike_name_Bajaj CT 100 ES Alloy',
                                             'bike_name_Bajaj CT 100 KS Alloy',
                                             'bike_name_Bajaj CT 100 Spoke', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))]), model_name=C:\Users\tonim\AppData\Local\Temp\tmpngc3riv6\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power',
                                             'bike_name_BMW F750 GS 850cc',
                                             'bike_name_BMW G 310 GS',
                                             'bike_name_BMW G 310 R',
                                             'bike_name_BMW S 1000 RR Pro',
                                             'bike_name_BMW S 1000 XR Pro',
                                             'bike_name_Bajaj  Pulsar 180cc',
                                             'bike_name_Bajaj Avenger 150cc',...
                                             'bike_name_Bajaj CT 100 Alloy',
                                             'bike_name_Bajaj CT 100 B',
                                             'bike_name_Bajaj CT 100 ES Alloy',
                                             'bike_name_Bajaj CT 100 KS Alloy',
                                             'bike_name_Bajaj CT 100 Spoke', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2023-04-26 17:48:11,113:INFO:Adding model into prep_pipe
2023-04-26 17:48:11,118:WARNING:Only Model saved as it was a pipeline.
2023-04-26 17:48:11,136:INFO:C:\Users\tonim\AppData\Local\Temp\tmpngc3riv6\Transformation Pipeline.pkl saved in current working directory
2023-04-26 17:48:11,149:INFO:Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power',
                                             'bike_name_BMW F750 GS 850cc',
                                             'bike_name_BMW G 310 GS',
                                             'bike_name_BMW G 310 R',
                                             'bike_name_BMW S 1000 RR Pro',
                                             'bike_name_BMW S 1000 XR Pro',
                                             'bike_name_Bajaj  Pulsar 180cc',
                                             'bike_name_Bajaj Avenger 150cc',...
                                             'bike_name_Bajaj CT 100 Alloy',
                                             'bike_name_Bajaj CT 100 B',
                                             'bike_name_Bajaj CT 100 ES Alloy',
                                             'bike_name_Bajaj CT 100 KS Alloy',
                                             'bike_name_Bajaj CT 100 Spoke', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-04-26 17:48:11,149:INFO:save_model() successfully completed......................................
2023-04-26 17:48:11,320:INFO:SubProcess save_model() end ==================================
2023-04-26 17:48:11,331:INFO:setup() successfully completed in 11.35s...............
2023-04-26 17:48:36,304:INFO:Initializing compare_models()
2023-04-26 17:48:36,304:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-04-26 17:48:36,304:INFO:Checking exceptions
2023-04-26 17:48:36,431:INFO:Preparing display monitor
2023-04-26 17:48:36,487:INFO:Initializing Linear Regression
2023-04-26 17:48:36,487:INFO:Total runtime is 0.0 minutes
2023-04-26 17:48:36,491:INFO:SubProcess create_model() called ==================================
2023-04-26 17:48:36,492:INFO:Initializing create_model()
2023-04-26 17:48:36,492:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:48:36,492:INFO:Checking exceptions
2023-04-26 17:48:36,492:INFO:Importing libraries
2023-04-26 17:48:36,492:INFO:Copying training dataset
2023-04-26 17:48:36,839:INFO:Defining folds
2023-04-26 17:48:36,840:INFO:Declaring metric variables
2023-04-26 17:48:36,843:INFO:Importing untrained model
2023-04-26 17:48:36,847:INFO:Linear Regression Imported successfully
2023-04-26 17:48:36,856:INFO:Starting cross validation
2023-04-26 17:48:36,898:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:49:02,709:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 3.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-26 17:49:02,709:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 3.29s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-26 17:49:04,450:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-26 17:49:04,466:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-26 17:49:04,679:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-26 17:49:04,687:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-26 17:49:17,189:INFO:Calculating mean and std
2023-04-26 17:49:17,197:INFO:Creating metrics dataframe
2023-04-26 17:49:17,243:INFO:Uploading results into container
2023-04-26 17:49:17,244:INFO:Uploading model into container now
2023-04-26 17:49:17,246:INFO:_master_model_container: 1
2023-04-26 17:49:17,246:INFO:_display_container: 2
2023-04-26 17:49:17,248:INFO:LinearRegression(n_jobs=-1)
2023-04-26 17:49:17,248:INFO:create_model() successfully completed......................................
2023-04-26 17:49:17,625:INFO:SubProcess create_model() end ==================================
2023-04-26 17:49:17,625:INFO:Creating metrics dataframe
2023-04-26 17:49:17,642:INFO:Initializing Lasso Regression
2023-04-26 17:49:17,642:INFO:Total runtime is 0.6859089612960816 minutes
2023-04-26 17:49:17,648:INFO:SubProcess create_model() called ==================================
2023-04-26 17:49:17,648:INFO:Initializing create_model()
2023-04-26 17:49:17,648:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:49:17,649:INFO:Checking exceptions
2023-04-26 17:49:17,649:INFO:Importing libraries
2023-04-26 17:49:17,649:INFO:Copying training dataset
2023-04-26 17:49:17,973:INFO:Defining folds
2023-04-26 17:49:17,973:INFO:Declaring metric variables
2023-04-26 17:49:17,976:INFO:Importing untrained model
2023-04-26 17:49:17,983:INFO:Lasso Regression Imported successfully
2023-04-26 17:49:17,990:INFO:Starting cross validation
2023-04-26 17:49:17,996:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:49:51,757:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.512e+12, tolerance: 1.715e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:49:51,919:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.640e+12, tolerance: 1.730e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:49:52,558:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.594e+12, tolerance: 1.722e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:49:54,325:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.564e+12, tolerance: 1.705e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:49:54,433:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.679e+12, tolerance: 1.728e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:49:54,772:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.515e+12, tolerance: 1.703e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:49:55,088:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.607e+12, tolerance: 1.668e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:49:55,960:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.663e+12, tolerance: 1.763e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:50:08,280:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.668e+12, tolerance: 1.731e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:50:09,324:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.608e+12, tolerance: 1.668e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:50:09,682:INFO:Calculating mean and std
2023-04-26 17:50:09,684:INFO:Creating metrics dataframe
2023-04-26 17:50:09,725:INFO:Uploading results into container
2023-04-26 17:50:09,726:INFO:Uploading model into container now
2023-04-26 17:50:09,726:INFO:_master_model_container: 2
2023-04-26 17:50:09,726:INFO:_display_container: 2
2023-04-26 17:50:09,726:INFO:Lasso(random_state=123)
2023-04-26 17:50:09,726:INFO:create_model() successfully completed......................................
2023-04-26 17:50:09,867:INFO:SubProcess create_model() end ==================================
2023-04-26 17:50:09,867:INFO:Creating metrics dataframe
2023-04-26 17:50:09,893:INFO:Initializing Ridge Regression
2023-04-26 17:50:09,893:INFO:Total runtime is 1.556760068734487 minutes
2023-04-26 17:50:09,897:INFO:SubProcess create_model() called ==================================
2023-04-26 17:50:09,897:INFO:Initializing create_model()
2023-04-26 17:50:09,897:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:50:09,897:INFO:Checking exceptions
2023-04-26 17:50:09,897:INFO:Importing libraries
2023-04-26 17:50:09,898:INFO:Copying training dataset
2023-04-26 17:50:10,196:INFO:Defining folds
2023-04-26 17:50:10,196:INFO:Declaring metric variables
2023-04-26 17:50:10,200:INFO:Importing untrained model
2023-04-26 17:50:10,207:INFO:Ridge Regression Imported successfully
2023-04-26 17:50:10,217:INFO:Starting cross validation
2023-04-26 17:50:10,221:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:50:11,853:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.65249e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-26 17:50:12,064:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.03408e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-26 17:50:12,217:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.95142e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-26 17:50:12,307:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.38438e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-26 17:50:12,330:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.18688e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-26 17:50:12,366:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.86398e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-26 17:50:12,410:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.51035e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-26 17:50:12,416:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.5528e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-26 17:50:14,247:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.26095e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-26 17:50:14,289:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.3952e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-26 17:50:14,732:INFO:Calculating mean and std
2023-04-26 17:50:14,735:INFO:Creating metrics dataframe
2023-04-26 17:50:14,788:INFO:Uploading results into container
2023-04-26 17:50:14,788:INFO:Uploading model into container now
2023-04-26 17:50:14,789:INFO:_master_model_container: 3
2023-04-26 17:50:14,789:INFO:_display_container: 2
2023-04-26 17:50:14,789:INFO:Ridge(random_state=123)
2023-04-26 17:50:14,789:INFO:create_model() successfully completed......................................
2023-04-26 17:50:14,931:INFO:SubProcess create_model() end ==================================
2023-04-26 17:50:14,932:INFO:Creating metrics dataframe
2023-04-26 17:50:14,968:INFO:Initializing Elastic Net
2023-04-26 17:50:14,968:INFO:Total runtime is 1.6413450042406719 minutes
2023-04-26 17:50:14,972:INFO:SubProcess create_model() called ==================================
2023-04-26 17:50:14,973:INFO:Initializing create_model()
2023-04-26 17:50:14,973:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:50:14,973:INFO:Checking exceptions
2023-04-26 17:50:14,973:INFO:Importing libraries
2023-04-26 17:50:14,973:INFO:Copying training dataset
2023-04-26 17:50:15,271:INFO:Defining folds
2023-04-26 17:50:15,272:INFO:Declaring metric variables
2023-04-26 17:50:15,276:INFO:Importing untrained model
2023-04-26 17:50:15,281:INFO:Elastic Net Imported successfully
2023-04-26 17:50:15,288:INFO:Starting cross validation
2023-04-26 17:50:15,294:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:51:07,444:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.854e+13, tolerance: 1.730e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:51:07,451:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.788e+13, tolerance: 1.668e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:51:07,489:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.845e+13, tolerance: 1.705e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:51:07,678:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.595e+13, tolerance: 1.703e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:51:08,054:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.869e+13, tolerance: 1.715e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:51:08,176:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.879e+13, tolerance: 1.728e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:51:08,204:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.854e+13, tolerance: 1.763e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:51:08,888:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+13, tolerance: 1.722e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:51:31,663:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.742e+13, tolerance: 1.731e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:51:31,728:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.756e+13, tolerance: 1.668e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-26 17:51:32,299:INFO:Calculating mean and std
2023-04-26 17:51:32,300:INFO:Creating metrics dataframe
2023-04-26 17:51:32,364:INFO:Uploading results into container
2023-04-26 17:51:32,365:INFO:Uploading model into container now
2023-04-26 17:51:32,365:INFO:_master_model_container: 4
2023-04-26 17:51:32,365:INFO:_display_container: 2
2023-04-26 17:51:32,366:INFO:ElasticNet(random_state=123)
2023-04-26 17:51:32,366:INFO:create_model() successfully completed......................................
2023-04-26 17:51:32,518:INFO:SubProcess create_model() end ==================================
2023-04-26 17:51:32,518:INFO:Creating metrics dataframe
2023-04-26 17:51:32,539:INFO:Initializing Least Angle Regression
2023-04-26 17:51:32,539:INFO:Total runtime is 2.934198681513468 minutes
2023-04-26 17:51:32,543:INFO:SubProcess create_model() called ==================================
2023-04-26 17:51:32,543:INFO:Initializing create_model()
2023-04-26 17:51:32,543:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:51:32,543:INFO:Checking exceptions
2023-04-26 17:51:32,543:INFO:Importing libraries
2023-04-26 17:51:32,544:INFO:Copying training dataset
2023-04-26 17:51:32,875:INFO:Defining folds
2023-04-26 17:51:32,875:INFO:Declaring metric variables
2023-04-26 17:51:32,880:INFO:Importing untrained model
2023-04-26 17:51:32,885:INFO:Least Angle Regression Imported successfully
2023-04-26 17:51:32,895:INFO:Starting cross validation
2023-04-26 17:51:32,900:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:51:33,987:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:51:34,148:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:51:34,208:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:51:34,290:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:51:34,343:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:51:34,345:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:51:34,353:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:51:34,519:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:51:35,210:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.916e+01, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:35,212:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.806e+01, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:35,222:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.350e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:35,224:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-26 17:51:35,287:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.153e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:35,290:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.141e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:35,479:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=5.641e+00, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:35,757:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 225 iterations, i.e. alpha=3.457e+00, with an active set of 222 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:35,863:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=7.997e+00, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:35,944:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.511e+01, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:35,964:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.388e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:35,970:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-26 17:51:35,975:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-26 17:51:36,030:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.004e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:36,065:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 214 iterations, i.e. alpha=2.115e+00, with an active set of 214 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:36,098:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=5.183e+00, with an active set of 125 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:36,202:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 298 iterations, i.e. alpha=1.049e+02, with an active set of 254 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:36,231:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-04-26 17:51:36,251:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 207 iterations, i.e. alpha=2.254e+00, with an active set of 204 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:36,264:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=8.057e+00, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:36,286:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-26 17:51:36,570:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 404 iterations, i.e. alpha=1.687e+09, with an active set of 366 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:36,680:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 423 iterations, i.e. alpha=7.477e-01, with an active set of 420 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:36,776:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 341 iterations, i.e. alpha=2.074e+01, with an active set of 318 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:36,831:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 348 iterations, i.e. alpha=1.024e+00, with an active set of 348 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:36,987:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 379 iterations, i.e. alpha=9.055e-01, with an active set of 379 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:37,061:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 478 iterations, i.e. alpha=1.028e+00, with an active set of 445 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:37,121:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-26 17:51:37,231:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-04-26 17:51:37,232:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:739: RuntimeWarning: overflow encountered in true_divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-04-26 17:51:37,233:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:740: RuntimeWarning: divide by zero encountered in true_divide
  gamma_ = min(g1, g2, C / AA)

2023-04-26 17:51:37,233:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-04-26 17:51:37,302:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 483 iterations, i.e. alpha=5.383e-01, with an active set of 483 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:37,316:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 488 iterations, i.e. alpha=5.338e-01, with an active set of 488 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:37,327:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:51:39,021:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-26 17:51:39,130:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:51:39,205:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-26 17:51:39,298:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:51:39,934:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.42s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:51:39,985:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:51:40,449:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.35s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:51:40,834:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:51:41,071:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:51:43,701:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:51:44,554:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:51:44,902:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.996e+01, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:44,904:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-26 17:51:44,929:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.617e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:44,979:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.112e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:45,048:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=5.487e+00, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:45,191:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 193 iterations, i.e. alpha=2.384e+00, with an active set of 193 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:45,783:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 425 iterations, i.e. alpha=7.248e-01, with an active set of 420 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:46,265:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 182 iterations, i.e. alpha=3.131e+00, with an active set of 174 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:46,480:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-04-26 17:51:47,137:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:51:48,804:INFO:Calculating mean and std
2023-04-26 17:51:48,810:INFO:Creating metrics dataframe
2023-04-26 17:51:48,997:INFO:Uploading results into container
2023-04-26 17:51:48,999:INFO:Uploading model into container now
2023-04-26 17:51:49,000:INFO:_master_model_container: 5
2023-04-26 17:51:49,000:INFO:_display_container: 2
2023-04-26 17:51:49,002:INFO:Lars(random_state=123)
2023-04-26 17:51:49,002:INFO:create_model() successfully completed......................................
2023-04-26 17:51:49,271:INFO:SubProcess create_model() end ==================================
2023-04-26 17:51:49,271:INFO:Creating metrics dataframe
2023-04-26 17:51:49,309:INFO:Initializing Lasso Least Angle Regression
2023-04-26 17:51:49,310:INFO:Total runtime is 3.2137197057406106 minutes
2023-04-26 17:51:49,318:INFO:SubProcess create_model() called ==================================
2023-04-26 17:51:49,319:INFO:Initializing create_model()
2023-04-26 17:51:49,320:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:51:49,320:INFO:Checking exceptions
2023-04-26 17:51:49,320:INFO:Importing libraries
2023-04-26 17:51:49,321:INFO:Copying training dataset
2023-04-26 17:51:49,841:INFO:Defining folds
2023-04-26 17:51:49,841:INFO:Declaring metric variables
2023-04-26 17:51:49,846:INFO:Importing untrained model
2023-04-26 17:51:49,852:INFO:Lasso Least Angle Regression Imported successfully
2023-04-26 17:51:49,861:INFO:Starting cross validation
2023-04-26 17:51:49,871:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:51:51,493:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-26 17:51:51,561:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-26 17:51:51,586:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-26 17:51:51,609:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-26 17:51:51,704:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-26 17:51:51,706:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-26 17:51:51,774:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-26 17:51:51,833:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-26 17:51:54,403:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=8.057e+00, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:54,456:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.916e+01, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:54,459:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.806e+01, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:54,471:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.350e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:54,473:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-26 17:51:54,528:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.153e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:54,604:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.141e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:54,673:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=5.641e+00, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:54,718:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 194 iterations, alpha=2.710e+00, previous alpha=2.603e+00, with an active set of 191 regressors.
  ConvergenceWarning,

2023-04-26 17:51:54,723:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-26 17:51:54,748:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 162 iterations, alpha=4.020e+00, previous alpha=3.863e+00, with an active set of 163 regressors.
  ConvergenceWarning,

2023-04-26 17:51:54,771:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 98 iterations, alpha=7.649e+00, previous alpha=7.642e+00, with an active set of 93 regressors.
  ConvergenceWarning,

2023-04-26 17:51:54,895:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=7.997e+00, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:54,900:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.511e+01, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:54,911:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.388e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:54,914:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-26 17:51:54,954:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.004e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:54,954:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-26 17:51:54,956:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 84 iterations, alpha=9.913e+00, previous alpha=9.849e+00, with an active set of 83 regressors.
  ConvergenceWarning,

2023-04-26 17:51:55,032:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=4.191e+00, with an active set of 138 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:55,033:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 134 iterations, alpha=4.562e+00, previous alpha=4.550e+00, with an active set of 133 regressors.
  ConvergenceWarning,

2023-04-26 17:51:55,042:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 214 iterations, i.e. alpha=2.115e+00, with an active set of 214 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:55,052:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 221 iterations, alpha=2.006e+00, previous alpha=1.987e+00, with an active set of 220 regressors.
  ConvergenceWarning,

2023-04-26 17:51:55,087:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 174 iterations, i.e. alpha=2.994e+00, with an active set of 170 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:55,214:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-04-26 17:51:55,245:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 348 iterations, i.e. alpha=1.024e+00, with an active set of 348 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:55,856:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:51:56,158:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:51:56,166:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:51:57,702:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-26 17:51:57,811:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-26 17:51:58,553:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.996e+01, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:58,554:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-26 17:51:58,560:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.617e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:58,590:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.112e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:58,624:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=5.487e+00, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:58,708:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 86 iterations, alpha=8.895e+00, previous alpha=8.885e+00, with an active set of 85 regressors.
  ConvergenceWarning,

2023-04-26 17:51:58,789:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 193 iterations, i.e. alpha=2.384e+00, with an active set of 193 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-26 17:51:58,807:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 196 iterations, alpha=2.382e+00, previous alpha=2.338e+00, with an active set of 195 regressors.
  ConvergenceWarning,

2023-04-26 17:51:59,272:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:51:59,560:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:52:00,633:INFO:Calculating mean and std
2023-04-26 17:52:00,636:INFO:Creating metrics dataframe
2023-04-26 17:52:00,792:INFO:Uploading results into container
2023-04-26 17:52:00,793:INFO:Uploading model into container now
2023-04-26 17:52:00,794:INFO:_master_model_container: 6
2023-04-26 17:52:00,794:INFO:_display_container: 2
2023-04-26 17:52:00,797:INFO:LassoLars(random_state=123)
2023-04-26 17:52:00,797:INFO:create_model() successfully completed......................................
2023-04-26 17:52:01,078:INFO:SubProcess create_model() end ==================================
2023-04-26 17:52:01,079:INFO:Creating metrics dataframe
2023-04-26 17:52:01,109:INFO:Initializing Orthogonal Matching Pursuit
2023-04-26 17:52:01,110:INFO:Total runtime is 3.410386367638906 minutes
2023-04-26 17:52:01,117:INFO:SubProcess create_model() called ==================================
2023-04-26 17:52:01,118:INFO:Initializing create_model()
2023-04-26 17:52:01,118:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:52:01,119:INFO:Checking exceptions
2023-04-26 17:52:01,119:INFO:Importing libraries
2023-04-26 17:52:01,119:INFO:Copying training dataset
2023-04-26 17:52:01,677:INFO:Defining folds
2023-04-26 17:52:01,678:INFO:Declaring metric variables
2023-04-26 17:52:01,689:INFO:Importing untrained model
2023-04-26 17:52:01,773:INFO:Orthogonal Matching Pursuit Imported successfully
2023-04-26 17:52:01,796:INFO:Starting cross validation
2023-04-26 17:52:01,815:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:52:03,761:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:52:03,820:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:52:03,895:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:52:03,909:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:52:03,942:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:52:03,953:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:52:04,025:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:52:04,042:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:52:06,006:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:52:06,094:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:52:06,213:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:52:06,311:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:52:08,429:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:52:08,435:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-26 17:52:09,829:INFO:Calculating mean and std
2023-04-26 17:52:09,831:INFO:Creating metrics dataframe
2023-04-26 17:52:09,922:INFO:Uploading results into container
2023-04-26 17:52:09,923:INFO:Uploading model into container now
2023-04-26 17:52:09,923:INFO:_master_model_container: 7
2023-04-26 17:52:09,923:INFO:_display_container: 2
2023-04-26 17:52:09,924:INFO:OrthogonalMatchingPursuit()
2023-04-26 17:52:09,924:INFO:create_model() successfully completed......................................
2023-04-26 17:52:10,087:INFO:SubProcess create_model() end ==================================
2023-04-26 17:52:10,087:INFO:Creating metrics dataframe
2023-04-26 17:52:10,106:INFO:Initializing Bayesian Ridge
2023-04-26 17:52:10,106:INFO:Total runtime is 3.5603180726369223 minutes
2023-04-26 17:52:10,110:INFO:SubProcess create_model() called ==================================
2023-04-26 17:52:10,110:INFO:Initializing create_model()
2023-04-26 17:52:10,110:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:52:10,111:INFO:Checking exceptions
2023-04-26 17:52:10,111:INFO:Importing libraries
2023-04-26 17:52:10,111:INFO:Copying training dataset
2023-04-26 17:52:10,440:INFO:Defining folds
2023-04-26 17:52:10,441:INFO:Declaring metric variables
2023-04-26 17:52:10,445:INFO:Importing untrained model
2023-04-26 17:52:10,450:INFO:Bayesian Ridge Imported successfully
2023-04-26 17:52:10,457:INFO:Starting cross validation
2023-04-26 17:52:10,463:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:52:50,821:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:53:09,974:INFO:Calculating mean and std
2023-04-26 17:53:09,977:INFO:Creating metrics dataframe
2023-04-26 17:53:10,080:INFO:Uploading results into container
2023-04-26 17:53:10,081:INFO:Uploading model into container now
2023-04-26 17:53:10,123:INFO:_master_model_container: 8
2023-04-26 17:53:10,123:INFO:_display_container: 2
2023-04-26 17:53:10,124:INFO:BayesianRidge()
2023-04-26 17:53:10,124:INFO:create_model() successfully completed......................................
2023-04-26 17:53:10,605:INFO:SubProcess create_model() end ==================================
2023-04-26 17:53:10,605:INFO:Creating metrics dataframe
2023-04-26 17:53:10,662:INFO:Initializing Passive Aggressive Regressor
2023-04-26 17:53:10,662:INFO:Total runtime is 4.569578317801158 minutes
2023-04-26 17:53:10,667:INFO:SubProcess create_model() called ==================================
2023-04-26 17:53:10,668:INFO:Initializing create_model()
2023-04-26 17:53:10,668:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:53:10,668:INFO:Checking exceptions
2023-04-26 17:53:10,668:INFO:Importing libraries
2023-04-26 17:53:10,668:INFO:Copying training dataset
2023-04-26 17:53:10,966:INFO:Defining folds
2023-04-26 17:53:10,966:INFO:Declaring metric variables
2023-04-26 17:53:11,008:INFO:Importing untrained model
2023-04-26 17:53:11,015:INFO:Passive Aggressive Regressor Imported successfully
2023-04-26 17:53:11,024:INFO:Starting cross validation
2023-04-26 17:53:11,028:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:53:15,651:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:53:19,257:INFO:Calculating mean and std
2023-04-26 17:53:19,259:INFO:Creating metrics dataframe
2023-04-26 17:53:19,339:INFO:Uploading results into container
2023-04-26 17:53:19,340:INFO:Uploading model into container now
2023-04-26 17:53:19,340:INFO:_master_model_container: 9
2023-04-26 17:53:19,340:INFO:_display_container: 2
2023-04-26 17:53:19,340:INFO:PassiveAggressiveRegressor(random_state=123)
2023-04-26 17:53:19,340:INFO:create_model() successfully completed......................................
2023-04-26 17:53:19,486:INFO:SubProcess create_model() end ==================================
2023-04-26 17:53:19,487:INFO:Creating metrics dataframe
2023-04-26 17:53:20,075:INFO:Initializing Huber Regressor
2023-04-26 17:53:20,075:INFO:Total runtime is 4.7264670332272845 minutes
2023-04-26 17:53:20,080:INFO:SubProcess create_model() called ==================================
2023-04-26 17:53:20,081:INFO:Initializing create_model()
2023-04-26 17:53:20,081:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:53:20,082:INFO:Checking exceptions
2023-04-26 17:53:20,082:INFO:Importing libraries
2023-04-26 17:53:20,082:INFO:Copying training dataset
2023-04-26 17:53:20,386:INFO:Defining folds
2023-04-26 17:53:20,386:INFO:Declaring metric variables
2023-04-26 17:53:20,391:INFO:Importing untrained model
2023-04-26 17:53:20,395:INFO:Huber Regressor Imported successfully
2023-04-26 17:53:20,403:INFO:Starting cross validation
2023-04-26 17:53:20,408:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:53:56,451:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:53:57,762:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:54:11,033:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:54:32,587:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-04-26 17:54:36,100:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-04-26 17:54:36,777:INFO:Calculating mean and std
2023-04-26 17:54:36,780:INFO:Creating metrics dataframe
2023-04-26 17:54:36,865:INFO:Uploading results into container
2023-04-26 17:54:36,865:INFO:Uploading model into container now
2023-04-26 17:54:36,866:INFO:_master_model_container: 10
2023-04-26 17:54:36,866:INFO:_display_container: 2
2023-04-26 17:54:36,866:INFO:HuberRegressor()
2023-04-26 17:54:36,866:INFO:create_model() successfully completed......................................
2023-04-26 17:54:37,022:INFO:SubProcess create_model() end ==================================
2023-04-26 17:54:37,022:INFO:Creating metrics dataframe
2023-04-26 17:54:37,042:INFO:Initializing K Neighbors Regressor
2023-04-26 17:54:37,042:INFO:Total runtime is 6.009256402651468 minutes
2023-04-26 17:54:37,045:INFO:SubProcess create_model() called ==================================
2023-04-26 17:54:37,046:INFO:Initializing create_model()
2023-04-26 17:54:37,046:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:54:37,046:INFO:Checking exceptions
2023-04-26 17:54:37,046:INFO:Importing libraries
2023-04-26 17:54:37,046:INFO:Copying training dataset
2023-04-26 17:54:37,331:INFO:Defining folds
2023-04-26 17:54:37,331:INFO:Declaring metric variables
2023-04-26 17:54:37,335:INFO:Importing untrained model
2023-04-26 17:54:37,340:INFO:K Neighbors Regressor Imported successfully
2023-04-26 17:54:37,349:INFO:Starting cross validation
2023-04-26 17:54:37,356:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:54:39,135:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:54:39,240:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:54:53,130:INFO:Calculating mean and std
2023-04-26 17:54:53,132:INFO:Creating metrics dataframe
2023-04-26 17:54:53,220:INFO:Uploading results into container
2023-04-26 17:54:53,220:INFO:Uploading model into container now
2023-04-26 17:54:53,221:INFO:_master_model_container: 11
2023-04-26 17:54:53,221:INFO:_display_container: 2
2023-04-26 17:54:53,221:INFO:KNeighborsRegressor(n_jobs=-1)
2023-04-26 17:54:53,221:INFO:create_model() successfully completed......................................
2023-04-26 17:54:53,382:INFO:SubProcess create_model() end ==================================
2023-04-26 17:54:53,382:INFO:Creating metrics dataframe
2023-04-26 17:54:53,404:INFO:Initializing Decision Tree Regressor
2023-04-26 17:54:53,404:INFO:Total runtime is 6.28194727897644 minutes
2023-04-26 17:54:53,408:INFO:SubProcess create_model() called ==================================
2023-04-26 17:54:53,409:INFO:Initializing create_model()
2023-04-26 17:54:53,409:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:54:53,409:INFO:Checking exceptions
2023-04-26 17:54:53,409:INFO:Importing libraries
2023-04-26 17:54:53,409:INFO:Copying training dataset
2023-04-26 17:54:53,734:INFO:Defining folds
2023-04-26 17:54:53,734:INFO:Declaring metric variables
2023-04-26 17:54:53,738:INFO:Importing untrained model
2023-04-26 17:54:53,743:INFO:Decision Tree Regressor Imported successfully
2023-04-26 17:54:53,751:INFO:Starting cross validation
2023-04-26 17:54:53,755:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:54:58,451:INFO:Calculating mean and std
2023-04-26 17:54:58,455:INFO:Creating metrics dataframe
2023-04-26 17:54:58,599:INFO:Uploading results into container
2023-04-26 17:54:58,599:INFO:Uploading model into container now
2023-04-26 17:54:58,600:INFO:_master_model_container: 12
2023-04-26 17:54:58,600:INFO:_display_container: 2
2023-04-26 17:54:58,600:INFO:DecisionTreeRegressor(random_state=123)
2023-04-26 17:54:58,600:INFO:create_model() successfully completed......................................
2023-04-26 17:54:58,747:INFO:SubProcess create_model() end ==================================
2023-04-26 17:54:58,748:INFO:Creating metrics dataframe
2023-04-26 17:54:58,767:INFO:Initializing Random Forest Regressor
2023-04-26 17:54:58,768:INFO:Total runtime is 6.371347216765085 minutes
2023-04-26 17:54:58,771:INFO:SubProcess create_model() called ==================================
2023-04-26 17:54:58,771:INFO:Initializing create_model()
2023-04-26 17:54:58,771:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:54:58,771:INFO:Checking exceptions
2023-04-26 17:54:58,772:INFO:Importing libraries
2023-04-26 17:54:58,772:INFO:Copying training dataset
2023-04-26 17:54:59,089:INFO:Defining folds
2023-04-26 17:54:59,089:INFO:Declaring metric variables
2023-04-26 17:54:59,093:INFO:Importing untrained model
2023-04-26 17:54:59,100:INFO:Random Forest Regressor Imported successfully
2023-04-26 17:54:59,107:INFO:Starting cross validation
2023-04-26 17:54:59,114:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:56:01,743:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:56:02,031:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:56:03,187:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:56:03,289:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:56:04,155:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:56:21,764:INFO:Calculating mean and std
2023-04-26 17:56:21,766:INFO:Creating metrics dataframe
2023-04-26 17:56:21,882:INFO:Uploading results into container
2023-04-26 17:56:21,883:INFO:Uploading model into container now
2023-04-26 17:56:21,883:INFO:_master_model_container: 13
2023-04-26 17:56:21,883:INFO:_display_container: 2
2023-04-26 17:56:21,884:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-04-26 17:56:21,884:INFO:create_model() successfully completed......................................
2023-04-26 17:56:22,035:INFO:SubProcess create_model() end ==================================
2023-04-26 17:56:22,035:INFO:Creating metrics dataframe
2023-04-26 17:56:22,053:INFO:Initializing Extra Trees Regressor
2023-04-26 17:56:22,053:INFO:Total runtime is 7.7594397306442255 minutes
2023-04-26 17:56:22,058:INFO:SubProcess create_model() called ==================================
2023-04-26 17:56:22,058:INFO:Initializing create_model()
2023-04-26 17:56:22,059:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:56:22,059:INFO:Checking exceptions
2023-04-26 17:56:22,059:INFO:Importing libraries
2023-04-26 17:56:22,059:INFO:Copying training dataset
2023-04-26 17:56:22,362:INFO:Defining folds
2023-04-26 17:56:22,362:INFO:Declaring metric variables
2023-04-26 17:56:22,368:INFO:Importing untrained model
2023-04-26 17:56:22,374:INFO:Extra Trees Regressor Imported successfully
2023-04-26 17:56:22,385:INFO:Starting cross validation
2023-04-26 17:56:22,392:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:58:16,314:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:58:16,543:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:58:16,848:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:58:16,924:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:58:17,047:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-26 17:58:17,774:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:58:17,817:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:58:18,205:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:58:18,218:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:58:18,227:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:58:18,229:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 17:58:52,575:INFO:Calculating mean and std
2023-04-26 17:58:52,579:INFO:Creating metrics dataframe
2023-04-26 17:58:52,799:INFO:Uploading results into container
2023-04-26 17:58:52,800:INFO:Uploading model into container now
2023-04-26 17:58:52,801:INFO:_master_model_container: 14
2023-04-26 17:58:52,801:INFO:_display_container: 2
2023-04-26 17:58:52,801:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-04-26 17:58:52,802:INFO:create_model() successfully completed......................................
2023-04-26 17:58:53,001:INFO:SubProcess create_model() end ==================================
2023-04-26 17:58:53,001:INFO:Creating metrics dataframe
2023-04-26 17:58:53,025:INFO:Initializing AdaBoost Regressor
2023-04-26 17:58:53,025:INFO:Total runtime is 10.275629615783691 minutes
2023-04-26 17:58:53,030:INFO:SubProcess create_model() called ==================================
2023-04-26 17:58:53,031:INFO:Initializing create_model()
2023-04-26 17:58:53,031:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:58:53,031:INFO:Checking exceptions
2023-04-26 17:58:53,032:INFO:Importing libraries
2023-04-26 17:58:53,032:INFO:Copying training dataset
2023-04-26 17:58:53,505:INFO:Defining folds
2023-04-26 17:58:53,506:INFO:Declaring metric variables
2023-04-26 17:58:53,512:INFO:Importing untrained model
2023-04-26 17:58:53,517:INFO:AdaBoost Regressor Imported successfully
2023-04-26 17:58:53,526:INFO:Starting cross validation
2023-04-26 17:58:53,533:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 17:59:43,626:INFO:Calculating mean and std
2023-04-26 17:59:43,629:INFO:Creating metrics dataframe
2023-04-26 17:59:43,743:INFO:Uploading results into container
2023-04-26 17:59:43,744:INFO:Uploading model into container now
2023-04-26 17:59:43,744:INFO:_master_model_container: 15
2023-04-26 17:59:43,744:INFO:_display_container: 2
2023-04-26 17:59:43,744:INFO:AdaBoostRegressor(random_state=123)
2023-04-26 17:59:43,744:INFO:create_model() successfully completed......................................
2023-04-26 17:59:43,906:INFO:SubProcess create_model() end ==================================
2023-04-26 17:59:43,906:INFO:Creating metrics dataframe
2023-04-26 17:59:43,924:INFO:Initializing Gradient Boosting Regressor
2023-04-26 17:59:43,925:INFO:Total runtime is 11.123969944318135 minutes
2023-04-26 17:59:43,929:INFO:SubProcess create_model() called ==================================
2023-04-26 17:59:43,929:INFO:Initializing create_model()
2023-04-26 17:59:43,929:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 17:59:43,930:INFO:Checking exceptions
2023-04-26 17:59:43,930:INFO:Importing libraries
2023-04-26 17:59:43,930:INFO:Copying training dataset
2023-04-26 17:59:44,229:INFO:Defining folds
2023-04-26 17:59:44,229:INFO:Declaring metric variables
2023-04-26 17:59:44,234:INFO:Importing untrained model
2023-04-26 17:59:44,239:INFO:Gradient Boosting Regressor Imported successfully
2023-04-26 17:59:44,248:INFO:Starting cross validation
2023-04-26 17:59:44,252:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 18:00:29,301:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 18:00:29,514:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 18:00:30,008:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-26 18:00:52,890:INFO:Calculating mean and std
2023-04-26 18:00:52,891:INFO:Creating metrics dataframe
2023-04-26 18:00:53,018:INFO:Uploading results into container
2023-04-26 18:00:53,019:INFO:Uploading model into container now
2023-04-26 18:00:53,019:INFO:_master_model_container: 16
2023-04-26 18:00:53,019:INFO:_display_container: 2
2023-04-26 18:00:53,020:INFO:GradientBoostingRegressor(random_state=123)
2023-04-26 18:00:53,020:INFO:create_model() successfully completed......................................
2023-04-26 18:00:53,171:INFO:SubProcess create_model() end ==================================
2023-04-26 18:00:53,171:INFO:Creating metrics dataframe
2023-04-26 18:00:53,193:INFO:Initializing Extreme Gradient Boosting
2023-04-26 18:00:53,194:INFO:Total runtime is 12.278446765740712 minutes
2023-04-26 18:00:53,199:INFO:SubProcess create_model() called ==================================
2023-04-26 18:00:53,200:INFO:Initializing create_model()
2023-04-26 18:00:53,200:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 18:00:53,200:INFO:Checking exceptions
2023-04-26 18:00:53,200:INFO:Importing libraries
2023-04-26 18:00:53,200:INFO:Copying training dataset
2023-04-26 18:00:53,515:INFO:Defining folds
2023-04-26 18:00:53,515:INFO:Declaring metric variables
2023-04-26 18:00:53,519:INFO:Importing untrained model
2023-04-26 18:00:53,526:INFO:Extreme Gradient Boosting Imported successfully
2023-04-26 18:00:53,535:INFO:Starting cross validation
2023-04-26 18:00:53,541:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 18:05:00,300:INFO:Calculating mean and std
2023-04-26 18:05:00,302:INFO:Creating metrics dataframe
2023-04-26 18:05:00,433:INFO:Uploading results into container
2023-04-26 18:05:00,434:INFO:Uploading model into container now
2023-04-26 18:05:00,434:INFO:_master_model_container: 17
2023-04-26 18:05:00,435:INFO:_display_container: 2
2023-04-26 18:05:00,436:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, gamma=None,
             gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, n_estimators=100, n_jobs=-1,
             num_parallel_tree=None, predictor=None, random_state=123,
             reg_alpha=None, reg_lambda=None, ...)
2023-04-26 18:05:00,436:INFO:create_model() successfully completed......................................
2023-04-26 18:05:00,607:INFO:SubProcess create_model() end ==================================
2023-04-26 18:05:00,607:INFO:Creating metrics dataframe
2023-04-26 18:05:00,627:INFO:Initializing Light Gradient Boosting Machine
2023-04-26 18:05:00,629:INFO:Total runtime is 16.402365489800772 minutes
2023-04-26 18:05:00,633:INFO:SubProcess create_model() called ==================================
2023-04-26 18:05:00,633:INFO:Initializing create_model()
2023-04-26 18:05:00,634:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 18:05:00,634:INFO:Checking exceptions
2023-04-26 18:05:00,634:INFO:Importing libraries
2023-04-26 18:05:00,634:INFO:Copying training dataset
2023-04-26 18:05:00,935:INFO:Defining folds
2023-04-26 18:05:00,936:INFO:Declaring metric variables
2023-04-26 18:05:00,940:INFO:Importing untrained model
2023-04-26 18:05:00,945:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-26 18:05:00,953:INFO:Starting cross validation
2023-04-26 18:05:00,959:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 18:05:06,018:INFO:Calculating mean and std
2023-04-26 18:05:06,020:INFO:Creating metrics dataframe
2023-04-26 18:05:06,184:INFO:Uploading results into container
2023-04-26 18:05:06,185:INFO:Uploading model into container now
2023-04-26 18:05:06,186:INFO:_master_model_container: 18
2023-04-26 18:05:06,186:INFO:_display_container: 2
2023-04-26 18:05:06,186:INFO:LGBMRegressor(random_state=123)
2023-04-26 18:05:06,186:INFO:create_model() successfully completed......................................
2023-04-26 18:05:06,325:INFO:SubProcess create_model() end ==================================
2023-04-26 18:05:06,325:INFO:Creating metrics dataframe
2023-04-26 18:05:06,349:INFO:Initializing Dummy Regressor
2023-04-26 18:05:06,349:INFO:Total runtime is 16.497707307338715 minutes
2023-04-26 18:05:06,352:INFO:SubProcess create_model() called ==================================
2023-04-26 18:05:06,353:INFO:Initializing create_model()
2023-04-26 18:05:06,353:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A8378FBB08>, model_only=True, return_train_score=False, kwargs={})
2023-04-26 18:05:06,353:INFO:Checking exceptions
2023-04-26 18:05:06,353:INFO:Importing libraries
2023-04-26 18:05:06,353:INFO:Copying training dataset
2023-04-26 18:05:06,664:INFO:Defining folds
2023-04-26 18:05:06,664:INFO:Declaring metric variables
2023-04-26 18:05:06,668:INFO:Importing untrained model
2023-04-26 18:05:06,673:INFO:Dummy Regressor Imported successfully
2023-04-26 18:05:06,683:INFO:Starting cross validation
2023-04-26 18:05:06,688:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-26 18:05:09,901:INFO:Calculating mean and std
2023-04-26 18:05:09,904:INFO:Creating metrics dataframe
2023-04-26 18:05:10,048:INFO:Uploading results into container
2023-04-26 18:05:10,049:INFO:Uploading model into container now
2023-04-26 18:05:10,049:INFO:_master_model_container: 19
2023-04-26 18:05:10,049:INFO:_display_container: 2
2023-04-26 18:05:10,049:INFO:DummyRegressor()
2023-04-26 18:05:10,050:INFO:create_model() successfully completed......................................
2023-04-26 18:05:10,192:INFO:SubProcess create_model() end ==================================
2023-04-26 18:05:10,193:INFO:Creating metrics dataframe
2023-04-26 18:05:10,232:INFO:Initializing create_model()
2023-04-26 18:05:10,232:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-26 18:05:10,232:INFO:Checking exceptions
2023-04-26 18:05:10,241:INFO:Importing libraries
2023-04-26 18:05:10,241:INFO:Copying training dataset
2023-04-26 18:05:10,545:INFO:Defining folds
2023-04-26 18:05:10,545:INFO:Declaring metric variables
2023-04-26 18:05:10,546:INFO:Importing untrained model
2023-04-26 18:05:10,546:INFO:Declaring custom model
2023-04-26 18:05:10,546:INFO:Extra Trees Regressor Imported successfully
2023-04-26 18:05:10,551:INFO:Cross validation set to False
2023-04-26 18:05:10,551:INFO:Fitting Model
2023-04-26 18:05:28,858:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-04-26 18:05:28,858:INFO:create_model() successfully completed......................................
2023-04-26 18:05:29,017:INFO:Creating Dashboard logs
2023-04-26 18:05:29,023:INFO:Model: Extra Trees Regressor
2023-04-26 18:05:29,077:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2023-04-26 18:05:29,141:INFO:Initializing predict_model()
2023-04-26 18:05:29,141:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x000001A839B75C18>)
2023-04-26 18:05:29,141:INFO:Checking exceptions
2023-04-26 18:05:29,141:INFO:Preloading libraries
2023-04-26 18:05:30,947:INFO:Creating Dashboard logs
2023-04-26 18:05:30,952:INFO:Model: Extreme Gradient Boosting
2023-04-26 18:05:30,978:INFO:Logged params: {'objective': 'reg:squarederror', 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': -1, 'num_parallel_tree': None, 'predictor': None, 'random_state': 123, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'auto', 'validate_parameters': None, 'verbosity': 0}
2023-04-26 18:05:31,358:INFO:Creating Dashboard logs
2023-04-26 18:05:31,361:INFO:Model: Random Forest Regressor
2023-04-26 18:05:31,390:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2023-04-26 18:05:31,766:INFO:Creating Dashboard logs
2023-04-26 18:05:31,770:INFO:Model: Gradient Boosting Regressor
2023-04-26 18:05:31,798:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-04-26 18:05:32,159:INFO:Creating Dashboard logs
2023-04-26 18:05:32,163:INFO:Model: Decision Tree Regressor
2023-04-26 18:05:32,190:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 123, 'splitter': 'best'}
2023-04-26 18:05:32,546:INFO:Creating Dashboard logs
2023-04-26 18:05:32,551:INFO:Model: Bayesian Ridge
2023-04-26 18:05:32,576:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'normalize': 'deprecated', 'tol': 0.001, 'verbose': False}
2023-04-26 18:05:32,929:INFO:Creating Dashboard logs
2023-04-26 18:05:32,934:INFO:Model: Ridge Regression
2023-04-26 18:05:32,960:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': 'deprecated', 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.001}
2023-04-26 18:05:33,468:INFO:Creating Dashboard logs
2023-04-26 18:05:33,472:INFO:Model: Light Gradient Boosting Machine
2023-04-26 18:05:33,501:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2023-04-26 18:05:34,022:INFO:Creating Dashboard logs
2023-04-26 18:05:34,027:INFO:Model: Linear Regression
2023-04-26 18:05:34,058:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'normalize': 'deprecated', 'positive': False}
2023-04-26 18:05:34,408:INFO:Creating Dashboard logs
2023-04-26 18:05:34,413:INFO:Model: Lasso Regression
2023-04-26 18:05:34,440:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': 'deprecated', 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2023-04-26 18:05:34,802:INFO:Creating Dashboard logs
2023-04-26 18:05:34,807:INFO:Model: Orthogonal Matching Pursuit
2023-04-26 18:05:34,841:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'normalize': 'deprecated', 'precompute': 'auto', 'tol': None}
2023-04-26 18:05:35,181:INFO:Creating Dashboard logs
2023-04-26 18:05:35,185:INFO:Model: Lasso Least Angle Regression
2023-04-26 18:05:35,214:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'normalize': 'deprecated', 'positive': False, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2023-04-26 18:05:35,570:INFO:Creating Dashboard logs
2023-04-26 18:05:35,572:INFO:Model: K Neighbors Regressor
2023-04-26 18:05:35,599:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2023-04-26 18:05:35,954:INFO:Creating Dashboard logs
2023-04-26 18:05:35,958:INFO:Model: AdaBoost Regressor
2023-04-26 18:05:35,986:INFO:Logged params: {'base_estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 123}
2023-04-26 18:05:36,335:INFO:Creating Dashboard logs
2023-04-26 18:05:36,339:INFO:Model: Elastic Net
2023-04-26 18:05:36,372:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'normalize': 'deprecated', 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2023-04-26 18:05:36,852:INFO:Creating Dashboard logs
2023-04-26 18:05:36,859:INFO:Model: Huber Regressor
2023-04-26 18:05:36,909:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2023-04-26 18:05:37,326:INFO:Creating Dashboard logs
2023-04-26 18:05:37,335:INFO:Model: Dummy Regressor
2023-04-26 18:05:37,422:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2023-04-26 18:05:37,826:INFO:Creating Dashboard logs
2023-04-26 18:05:37,831:INFO:Model: Passive Aggressive Regressor
2023-04-26 18:05:37,857:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-04-26 18:05:38,207:INFO:Creating Dashboard logs
2023-04-26 18:05:38,212:INFO:Model: Least Angle Regression
2023-04-26 18:05:38,240:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'normalize': 'deprecated', 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2023-04-26 18:05:38,628:INFO:_master_model_container: 19
2023-04-26 18:05:38,628:INFO:_display_container: 2
2023-04-26 18:05:38,628:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-04-26 18:05:38,629:INFO:compare_models() successfully completed......................................
2023-04-26 18:59:36,662:INFO:gpu_param set to False
2023-04-26 18:59:37,662:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 18:59:37,674:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 18:59:38,243:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-26 18:59:38,248:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-26 19:04:30,361:INFO:Initializing automl()
2023-04-26 19:04:30,362:INFO:automl(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, optimize=mean_squared_error, use_holdout=False, turbo=True, return_train_score=False)
2023-04-26 19:04:50,902:INFO:Initializing automl()
2023-04-26 19:04:50,902:INFO:automl(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, optimize=Recall, use_holdout=False, turbo=True, return_train_score=False)
2023-04-26 19:05:05,578:INFO:Initializing automl()
2023-04-26 19:05:05,578:INFO:automl(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001A837972188>, optimize=Recall, use_holdout=False, turbo=True, return_train_score=False)
2023-04-27 11:50:29,750:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-27 11:50:29,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-27 11:50:29,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-27 11:50:29,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-27 11:50:32,192:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-04-27 11:50:33,148:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\seaborn\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)

2023-04-27 11:51:39,893:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\seaborn\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)

2023-04-27 11:53:00,004:INFO:PyCaret RegressionExperiment
2023-04-27 11:53:00,004:INFO:Logging name: price
2023-04-27 11:53:00,004:INFO:ML Usecase: MLUsecase.REGRESSION
2023-04-27 11:53:00,004:INFO:version 3.0.0
2023-04-27 11:53:00,005:INFO:Initializing setup()
2023-04-27 11:53:00,005:INFO:self.USI: f633
2023-04-27 11:53:00,005:INFO:self._variable_keys: {'exp_id', 'X_train', 'X_test', 'idx', 'fold_generator', 'fold_shuffle_param', '_ml_usecase', 'logging_param', 'gpu_n_jobs_param', 'USI', 'exp_name_log', 'X', 'y_test', 'seed', 'n_jobs_param', 'log_plots_param', '_available_plots', 'pipeline', 'html_param', 'memory', 'gpu_param', 'y', 'fold_groups_param', 'target_param', 'y_train', 'data', 'transform_target_param'}
2023-04-27 11:53:00,005:INFO:Checking environment
2023-04-27 11:53:00,005:INFO:python_version: 3.7.4
2023-04-27 11:53:00,005:INFO:python_build: ('tags/v3.7.4:e09359112e', 'Jul  8 2019 20:34:20')
2023-04-27 11:53:00,005:INFO:machine: AMD64
2023-04-27 11:53:00,005:INFO:platform: Windows-10-10.0.19041-SP0
2023-04-27 11:53:00,006:INFO:Memory: svmem(total=16907886592, available=515170304, percent=97.0, used=16392716288, free=515170304)
2023-04-27 11:53:00,006:INFO:Physical Core: 4
2023-04-27 11:53:00,006:INFO:Logical Core: 8
2023-04-27 11:53:00,006:INFO:Checking libraries
2023-04-27 11:53:00,006:INFO:System:
2023-04-27 11:53:00,006:INFO:    python: 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]
2023-04-27 11:53:00,006:INFO:executable: c:\Users\tonim\AppData\Local\Programs\Python\Python37\python.exe
2023-04-27 11:53:00,006:INFO:   machine: Windows-10-10.0.19041-SP0
2023-04-27 11:53:00,006:INFO:PyCaret required dependencies:
2023-04-27 11:53:00,007:INFO:                 pip: 22.2.2
2023-04-27 11:53:00,007:INFO:          setuptools: 65.3.0
2023-04-27 11:53:00,007:INFO:             pycaret: 3.0.0
2023-04-27 11:53:00,007:INFO:             IPython: 7.34.0
2023-04-27 11:53:00,007:INFO:          ipywidgets: 7.7.1
2023-04-27 11:53:00,007:INFO:                tqdm: 4.64.0
2023-04-27 11:53:00,007:INFO:               numpy: 1.21.6
2023-04-27 11:53:00,007:INFO:              pandas: 1.3.5
2023-04-27 11:53:00,007:INFO:              jinja2: 3.1.2
2023-04-27 11:53:00,007:INFO:               scipy: 1.5.4
2023-04-27 11:53:00,008:INFO:              joblib: 1.2.0
2023-04-27 11:53:00,008:INFO:             sklearn: 1.0.2
2023-04-27 11:53:00,008:INFO:                pyod: 1.0.9
2023-04-27 11:53:00,008:INFO:            imblearn: 0.9.0
2023-04-27 11:53:00,008:INFO:   category_encoders: 2.6.0
2023-04-27 11:53:00,008:INFO:            lightgbm: 3.3.5
2023-04-27 11:53:00,008:INFO:               numba: 0.56.0
2023-04-27 11:53:00,008:INFO:            requests: 2.28.1
2023-04-27 11:53:00,008:INFO:          matplotlib: 3.5.2
2023-04-27 11:53:00,008:INFO:          scikitplot: 0.3.7
2023-04-27 11:53:00,008:INFO:         yellowbrick: 1.5
2023-04-27 11:53:00,008:INFO:              plotly: 5.9.0
2023-04-27 11:53:00,008:INFO:             kaleido: 0.2.1
2023-04-27 11:53:00,008:INFO:         statsmodels: 0.13.2
2023-04-27 11:53:00,008:INFO:              sktime: 0.17.1
2023-04-27 11:53:00,008:INFO:               tbats: 1.1.3
2023-04-27 11:53:00,009:INFO:            pmdarima: 2.0.1
2023-04-27 11:53:00,009:INFO:              psutil: 5.9.1
2023-04-27 11:53:00,009:INFO:PyCaret optional dependencies:
2023-04-27 11:53:00,023:INFO:                shap: 0.41.0
2023-04-27 11:53:00,023:INFO:           interpret: Not installed
2023-04-27 11:53:00,023:INFO:                umap: Not installed
2023-04-27 11:53:00,023:INFO:    pandas_profiling: 3.3.0
2023-04-27 11:53:00,023:INFO:  explainerdashboard: Not installed
2023-04-27 11:53:00,024:INFO:             autoviz: Not installed
2023-04-27 11:53:00,024:INFO:           fairlearn: Not installed
2023-04-27 11:53:00,024:INFO:             xgboost: 1.6.2
2023-04-27 11:53:00,024:INFO:            catboost: Not installed
2023-04-27 11:53:00,024:INFO:              kmodes: Not installed
2023-04-27 11:53:00,024:INFO:             mlxtend: Not installed
2023-04-27 11:53:00,024:INFO:       statsforecast: Not installed
2023-04-27 11:53:00,024:INFO:        tune_sklearn: Not installed
2023-04-27 11:53:00,024:INFO:                 ray: Not installed
2023-04-27 11:53:00,024:INFO:            hyperopt: 0.2.7
2023-04-27 11:53:00,024:INFO:              optuna: Not installed
2023-04-27 11:53:00,024:INFO:               skopt: Not installed
2023-04-27 11:53:00,024:INFO:              mlflow: 1.30.1
2023-04-27 11:53:00,024:INFO:              gradio: Not installed
2023-04-27 11:53:00,024:INFO:             fastapi: Not installed
2023-04-27 11:53:00,024:INFO:             uvicorn: Not installed
2023-04-27 11:53:00,024:INFO:              m2cgen: Not installed
2023-04-27 11:53:00,024:INFO:           evidently: Not installed
2023-04-27 11:53:00,024:INFO:               fugue: Not installed
2023-04-27 11:53:00,024:INFO:           streamlit: 1.11.0
2023-04-27 11:53:00,024:INFO:             prophet: Not installed
2023-04-27 11:53:00,025:INFO:None
2023-04-27 11:53:00,025:INFO:Set up data.
2023-04-27 11:53:00,685:INFO:Set up train/test split.
2023-04-27 11:53:00,974:INFO:Set up index.
2023-04-27 11:53:00,980:INFO:Set up folding strategy.
2023-04-27 11:53:00,980:INFO:Assigning column types.
2023-04-27 11:53:01,181:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-27 11:53:01,182:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-27 11:53:01,187:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-27 11:53:01,193:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 11:53:01,545:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 11:53:01,605:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 11:53:01,606:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:01,997:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:01,997:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-27 11:53:02,002:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-27 11:53:02,007:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 11:53:02,340:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 11:53:02,396:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 11:53:02,397:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:02,400:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:02,400:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-04-27 11:53:02,405:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-27 11:53:02,409:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 11:53:02,738:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 11:53:02,792:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 11:53:02,793:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:02,796:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:02,803:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-27 11:53:02,810:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 11:53:03,150:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 11:53:03,211:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 11:53:03,212:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:03,214:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:03,214:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-04-27 11:53:03,226:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 11:53:03,607:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 11:53:03,676:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 11:53:03,680:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:03,683:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:03,693:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 11:53:04,089:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 11:53:04,155:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 11:53:04,157:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:04,159:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:04,160:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-04-27 11:53:04,543:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 11:53:04,603:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 11:53:04,604:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:04,608:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:04,961:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 11:53:05,016:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 11:53:05,017:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:05,021:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:05,022:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-27 11:53:05,361:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 11:53:05,414:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:05,417:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:05,756:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 11:53:05,808:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:05,811:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:05,811:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-04-27 11:53:06,194:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:06,198:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:06,577:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:06,579:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:06,586:INFO:Preparing preprocessing pipeline...
2023-04-27 11:53:06,586:INFO:Set up simple imputation.
2023-04-27 11:53:06,613:INFO:Set up column name cleaning.
2023-04-27 11:53:07,120:INFO:Finished creating preprocessing pipeline.
2023-04-27 11:53:07,141:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power',
                                             'bike_name_BMW F750 GS 850cc',
                                             'bike_name_BMW G 310 GS',
                                             'bike_name_BMW G 310 R',
                                             'bike_name_BMW S 1000 RR Pro',
                                             'bike_name_BMW S 1000 XR Pro',
                                             'bike_name_Bajaj  Pulsar 180cc',
                                             'bike_name_Bajaj Avenger 150cc',...
                                             'bike_name_Bajaj CT 100 Alloy',
                                             'bike_name_Bajaj CT 100 B',
                                             'bike_name_Bajaj CT 100 ES Alloy',
                                             'bike_name_Bajaj CT 100 KS Alloy',
                                             'bike_name_Bajaj CT 100 Spoke', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-04-27 11:53:07,141:INFO:Creating final display dataframe.
2023-04-27 11:53:09,329:INFO:Setup _display_container:                     Description         Value
0                    Session id           123
1                        Target         price
2                   Target type    Regression
3           Original data shape  (32648, 942)
4        Transformed data shape  (32648, 942)
5   Transformed train set shape  (22853, 942)
6    Transformed test set shape   (9795, 942)
7              Numeric features           941
8                    Preprocess          True
9               Imputation type        simple
10           Numeric imputation          mean
11       Categorical imputation          mode
12               Fold Generator         KFold
13                  Fold Number            10
14                     CPU Jobs            -1
15                      Use GPU         False
16               Log Experiment  MlflowLogger
17              Experiment Name         price
18                          USI          f633
2023-04-27 11:53:09,831:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:09,835:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:10,300:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:53:10,305:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:53:10,305:INFO:Logging experiment in loggers
2023-04-27 11:53:11,011:INFO:SubProcess save_model() called ==================================
2023-04-27 11:53:11,048:INFO:Initializing save_model()
2023-04-27 11:53:11,048:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power',
                                             'bike_name_BMW F750 GS 850cc',
                                             'bike_name_BMW G 310 GS',
                                             'bike_name_BMW G 310 R',
                                             'bike_name_BMW S 1000 RR Pro',
                                             'bike_name_BMW S 1000 XR Pro',
                                             'bike_name_Bajaj  Pulsar 180cc',
                                             'bike_name_Bajaj Avenger 150cc',...
                                             'bike_name_Bajaj CT 100 Alloy',
                                             'bike_name_Bajaj CT 100 B',
                                             'bike_name_Bajaj CT 100 ES Alloy',
                                             'bike_name_Bajaj CT 100 KS Alloy',
                                             'bike_name_Bajaj CT 100 Spoke', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))]), model_name=C:\Users\tonim\AppData\Local\Temp\tmpihewflw3\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power',
                                             'bike_name_BMW F750 GS 850cc',
                                             'bike_name_BMW G 310 GS',
                                             'bike_name_BMW G 310 R',
                                             'bike_name_BMW S 1000 RR Pro',
                                             'bike_name_BMW S 1000 XR Pro',
                                             'bike_name_Bajaj  Pulsar 180cc',
                                             'bike_name_Bajaj Avenger 150cc',...
                                             'bike_name_Bajaj CT 100 Alloy',
                                             'bike_name_Bajaj CT 100 B',
                                             'bike_name_Bajaj CT 100 ES Alloy',
                                             'bike_name_Bajaj CT 100 KS Alloy',
                                             'bike_name_Bajaj CT 100 Spoke', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2023-04-27 11:53:11,048:INFO:Adding model into prep_pipe
2023-04-27 11:53:11,055:WARNING:Only Model saved as it was a pipeline.
2023-04-27 11:53:11,083:INFO:C:\Users\tonim\AppData\Local\Temp\tmpihewflw3\Transformation Pipeline.pkl saved in current working directory
2023-04-27 11:53:11,100:INFO:Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power',
                                             'bike_name_BMW F750 GS 850cc',
                                             'bike_name_BMW G 310 GS',
                                             'bike_name_BMW G 310 R',
                                             'bike_name_BMW S 1000 RR Pro',
                                             'bike_name_BMW S 1000 XR Pro',
                                             'bike_name_Bajaj  Pulsar 180cc',
                                             'bike_name_Bajaj Avenger 150cc',...
                                             'bike_name_Bajaj CT 100 Alloy',
                                             'bike_name_Bajaj CT 100 B',
                                             'bike_name_Bajaj CT 100 ES Alloy',
                                             'bike_name_Bajaj CT 100 KS Alloy',
                                             'bike_name_Bajaj CT 100 Spoke', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-04-27 11:53:11,100:INFO:save_model() successfully completed......................................
2023-04-27 11:53:11,494:INFO:SubProcess save_model() end ==================================
2023-04-27 11:53:11,513:INFO:setup() successfully completed in 10.9s...............
2023-04-27 11:53:11,561:INFO:Initializing compare_models()
2023-04-27 11:53:11,562:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-04-27 11:53:11,562:INFO:Checking exceptions
2023-04-27 11:53:11,724:INFO:Preparing display monitor
2023-04-27 11:53:11,783:INFO:Initializing Linear Regression
2023-04-27 11:53:11,783:INFO:Total runtime is 0.0 minutes
2023-04-27 11:53:11,792:INFO:SubProcess create_model() called ==================================
2023-04-27 11:53:11,792:INFO:Initializing create_model()
2023-04-27 11:53:11,793:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:53:11,793:INFO:Checking exceptions
2023-04-27 11:53:11,793:INFO:Importing libraries
2023-04-27 11:53:11,793:INFO:Copying training dataset
2023-04-27 11:53:12,467:INFO:Defining folds
2023-04-27 11:53:12,467:INFO:Declaring metric variables
2023-04-27 11:53:12,478:INFO:Importing untrained model
2023-04-27 11:53:12,485:INFO:Linear Regression Imported successfully
2023-04-27 11:53:12,498:INFO:Starting cross validation
2023-04-27 11:53:12,532:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:53:25,219:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 11:53:31,640:INFO:Calculating mean and std
2023-04-27 11:53:31,642:INFO:Creating metrics dataframe
2023-04-27 11:53:31,916:INFO:Uploading results into container
2023-04-27 11:53:31,917:INFO:Uploading model into container now
2023-04-27 11:53:31,928:INFO:_master_model_container: 1
2023-04-27 11:53:31,928:INFO:_display_container: 2
2023-04-27 11:53:31,929:INFO:LinearRegression(n_jobs=-1)
2023-04-27 11:53:31,930:INFO:create_model() successfully completed......................................
2023-04-27 11:53:32,157:INFO:SubProcess create_model() end ==================================
2023-04-27 11:53:32,158:INFO:Creating metrics dataframe
2023-04-27 11:53:32,177:INFO:Initializing Lasso Regression
2023-04-27 11:53:32,178:INFO:Total runtime is 0.3399137934048971 minutes
2023-04-27 11:53:32,183:INFO:SubProcess create_model() called ==================================
2023-04-27 11:53:32,183:INFO:Initializing create_model()
2023-04-27 11:53:32,183:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:53:32,183:INFO:Checking exceptions
2023-04-27 11:53:32,184:INFO:Importing libraries
2023-04-27 11:53:32,184:INFO:Copying training dataset
2023-04-27 11:53:32,653:INFO:Defining folds
2023-04-27 11:53:32,653:INFO:Declaring metric variables
2023-04-27 11:53:32,664:INFO:Importing untrained model
2023-04-27 11:53:32,675:INFO:Lasso Regression Imported successfully
2023-04-27 11:53:32,692:INFO:Starting cross validation
2023-04-27 11:53:32,701:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:53:37,417:INFO:Calculating mean and std
2023-04-27 11:53:37,419:INFO:Creating metrics dataframe
2023-04-27 11:53:37,668:INFO:Uploading results into container
2023-04-27 11:53:37,669:INFO:Uploading model into container now
2023-04-27 11:53:37,669:INFO:_master_model_container: 2
2023-04-27 11:53:37,670:INFO:_display_container: 2
2023-04-27 11:53:37,670:INFO:Lasso(random_state=123)
2023-04-27 11:53:37,670:INFO:create_model() successfully completed......................................
2023-04-27 11:53:37,879:INFO:SubProcess create_model() end ==================================
2023-04-27 11:53:37,879:INFO:Creating metrics dataframe
2023-04-27 11:53:37,897:INFO:Initializing Ridge Regression
2023-04-27 11:53:37,897:INFO:Total runtime is 0.4352296551068624 minutes
2023-04-27 11:53:37,903:INFO:SubProcess create_model() called ==================================
2023-04-27 11:53:37,904:INFO:Initializing create_model()
2023-04-27 11:53:37,904:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:53:37,904:INFO:Checking exceptions
2023-04-27 11:53:37,904:INFO:Importing libraries
2023-04-27 11:53:37,904:INFO:Copying training dataset
2023-04-27 11:53:38,320:INFO:Defining folds
2023-04-27 11:53:38,320:INFO:Declaring metric variables
2023-04-27 11:53:38,327:INFO:Importing untrained model
2023-04-27 11:53:38,332:INFO:Ridge Regression Imported successfully
2023-04-27 11:53:38,344:INFO:Starting cross validation
2023-04-27 11:53:38,353:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:53:42,289:INFO:Calculating mean and std
2023-04-27 11:53:42,291:INFO:Creating metrics dataframe
2023-04-27 11:53:42,532:INFO:Uploading results into container
2023-04-27 11:53:42,532:INFO:Uploading model into container now
2023-04-27 11:53:42,533:INFO:_master_model_container: 3
2023-04-27 11:53:42,533:INFO:_display_container: 2
2023-04-27 11:53:42,534:INFO:Ridge(random_state=123)
2023-04-27 11:53:42,534:INFO:create_model() successfully completed......................................
2023-04-27 11:53:42,736:INFO:SubProcess create_model() end ==================================
2023-04-27 11:53:42,737:INFO:Creating metrics dataframe
2023-04-27 11:53:42,756:INFO:Initializing Elastic Net
2023-04-27 11:53:42,756:INFO:Total runtime is 0.516212530930837 minutes
2023-04-27 11:53:42,762:INFO:SubProcess create_model() called ==================================
2023-04-27 11:53:42,762:INFO:Initializing create_model()
2023-04-27 11:53:42,762:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:53:42,763:INFO:Checking exceptions
2023-04-27 11:53:42,763:INFO:Importing libraries
2023-04-27 11:53:42,763:INFO:Copying training dataset
2023-04-27 11:53:43,179:INFO:Defining folds
2023-04-27 11:53:43,180:INFO:Declaring metric variables
2023-04-27 11:53:43,186:INFO:Importing untrained model
2023-04-27 11:53:43,191:INFO:Elastic Net Imported successfully
2023-04-27 11:53:43,202:INFO:Starting cross validation
2023-04-27 11:53:43,210:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:53:47,203:INFO:Calculating mean and std
2023-04-27 11:53:47,206:INFO:Creating metrics dataframe
2023-04-27 11:53:47,432:INFO:Uploading results into container
2023-04-27 11:53:47,432:INFO:Uploading model into container now
2023-04-27 11:53:47,433:INFO:_master_model_container: 4
2023-04-27 11:53:47,433:INFO:_display_container: 2
2023-04-27 11:53:47,434:INFO:ElasticNet(random_state=123)
2023-04-27 11:53:47,434:INFO:create_model() successfully completed......................................
2023-04-27 11:53:47,628:INFO:SubProcess create_model() end ==================================
2023-04-27 11:53:47,628:INFO:Creating metrics dataframe
2023-04-27 11:53:47,644:INFO:Initializing Least Angle Regression
2023-04-27 11:53:47,644:INFO:Total runtime is 0.5976792931556701 minutes
2023-04-27 11:53:47,651:INFO:SubProcess create_model() called ==================================
2023-04-27 11:53:47,651:INFO:Initializing create_model()
2023-04-27 11:53:47,651:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:53:47,651:INFO:Checking exceptions
2023-04-27 11:53:47,651:INFO:Importing libraries
2023-04-27 11:53:47,652:INFO:Copying training dataset
2023-04-27 11:53:48,073:INFO:Defining folds
2023-04-27 11:53:48,073:INFO:Declaring metric variables
2023-04-27 11:53:48,080:INFO:Importing untrained model
2023-04-27 11:53:48,087:INFO:Least Angle Regression Imported successfully
2023-04-27 11:53:48,099:INFO:Starting cross validation
2023-04-27 11:53:48,108:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:53:51,791:INFO:Calculating mean and std
2023-04-27 11:53:51,793:INFO:Creating metrics dataframe
2023-04-27 11:53:52,012:INFO:Uploading results into container
2023-04-27 11:53:52,013:INFO:Uploading model into container now
2023-04-27 11:53:52,013:INFO:_master_model_container: 5
2023-04-27 11:53:52,013:INFO:_display_container: 2
2023-04-27 11:53:52,014:INFO:Lars(random_state=123)
2023-04-27 11:53:52,014:INFO:create_model() successfully completed......................................
2023-04-27 11:53:52,193:INFO:SubProcess create_model() end ==================================
2023-04-27 11:53:52,193:INFO:Creating metrics dataframe
2023-04-27 11:53:52,211:INFO:Initializing Lasso Least Angle Regression
2023-04-27 11:53:52,211:INFO:Total runtime is 0.6738048911094665 minutes
2023-04-27 11:53:52,217:INFO:SubProcess create_model() called ==================================
2023-04-27 11:53:52,217:INFO:Initializing create_model()
2023-04-27 11:53:52,217:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:53:52,217:INFO:Checking exceptions
2023-04-27 11:53:52,218:INFO:Importing libraries
2023-04-27 11:53:52,218:INFO:Copying training dataset
2023-04-27 11:53:52,571:INFO:Defining folds
2023-04-27 11:53:52,571:INFO:Declaring metric variables
2023-04-27 11:53:52,575:INFO:Importing untrained model
2023-04-27 11:53:52,580:INFO:Lasso Least Angle Regression Imported successfully
2023-04-27 11:53:52,591:INFO:Starting cross validation
2023-04-27 11:53:52,595:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:53:56,242:INFO:Calculating mean and std
2023-04-27 11:53:56,244:INFO:Creating metrics dataframe
2023-04-27 11:53:56,484:INFO:Uploading results into container
2023-04-27 11:53:56,485:INFO:Uploading model into container now
2023-04-27 11:53:56,486:INFO:_master_model_container: 6
2023-04-27 11:53:56,486:INFO:_display_container: 2
2023-04-27 11:53:56,487:INFO:LassoLars(random_state=123)
2023-04-27 11:53:56,487:INFO:create_model() successfully completed......................................
2023-04-27 11:53:56,695:INFO:SubProcess create_model() end ==================================
2023-04-27 11:53:56,695:INFO:Creating metrics dataframe
2023-04-27 11:53:56,717:INFO:Initializing Orthogonal Matching Pursuit
2023-04-27 11:53:56,717:INFO:Total runtime is 0.7488954583803812 minutes
2023-04-27 11:53:56,722:INFO:SubProcess create_model() called ==================================
2023-04-27 11:53:56,722:INFO:Initializing create_model()
2023-04-27 11:53:56,722:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:53:56,722:INFO:Checking exceptions
2023-04-27 11:53:56,723:INFO:Importing libraries
2023-04-27 11:53:56,723:INFO:Copying training dataset
2023-04-27 11:53:57,110:INFO:Defining folds
2023-04-27 11:53:57,110:INFO:Declaring metric variables
2023-04-27 11:53:57,115:INFO:Importing untrained model
2023-04-27 11:53:57,120:INFO:Orthogonal Matching Pursuit Imported successfully
2023-04-27 11:53:57,130:INFO:Starting cross validation
2023-04-27 11:53:57,135:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:54:00,138:INFO:Calculating mean and std
2023-04-27 11:54:00,140:INFO:Creating metrics dataframe
2023-04-27 11:54:00,340:INFO:Uploading results into container
2023-04-27 11:54:00,341:INFO:Uploading model into container now
2023-04-27 11:54:00,342:INFO:_master_model_container: 7
2023-04-27 11:54:00,342:INFO:_display_container: 2
2023-04-27 11:54:00,342:INFO:OrthogonalMatchingPursuit()
2023-04-27 11:54:00,342:INFO:create_model() successfully completed......................................
2023-04-27 11:54:00,513:INFO:SubProcess create_model() end ==================================
2023-04-27 11:54:00,513:INFO:Creating metrics dataframe
2023-04-27 11:54:00,530:INFO:Initializing Bayesian Ridge
2023-04-27 11:54:00,530:INFO:Total runtime is 0.812455399831136 minutes
2023-04-27 11:54:00,537:INFO:SubProcess create_model() called ==================================
2023-04-27 11:54:00,537:INFO:Initializing create_model()
2023-04-27 11:54:00,538:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:54:00,538:INFO:Checking exceptions
2023-04-27 11:54:00,538:INFO:Importing libraries
2023-04-27 11:54:00,538:INFO:Copying training dataset
2023-04-27 11:54:00,908:INFO:Defining folds
2023-04-27 11:54:00,909:INFO:Declaring metric variables
2023-04-27 11:54:00,915:INFO:Importing untrained model
2023-04-27 11:54:00,920:INFO:Bayesian Ridge Imported successfully
2023-04-27 11:54:00,929:INFO:Starting cross validation
2023-04-27 11:54:00,936:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:54:04,009:INFO:Calculating mean and std
2023-04-27 11:54:04,011:INFO:Creating metrics dataframe
2023-04-27 11:54:04,216:INFO:Uploading results into container
2023-04-27 11:54:04,216:INFO:Uploading model into container now
2023-04-27 11:54:04,217:INFO:_master_model_container: 8
2023-04-27 11:54:04,217:INFO:_display_container: 2
2023-04-27 11:54:04,217:INFO:BayesianRidge()
2023-04-27 11:54:04,217:INFO:create_model() successfully completed......................................
2023-04-27 11:54:04,396:INFO:SubProcess create_model() end ==================================
2023-04-27 11:54:04,396:INFO:Creating metrics dataframe
2023-04-27 11:54:04,414:INFO:Initializing Passive Aggressive Regressor
2023-04-27 11:54:04,414:INFO:Total runtime is 0.8771838188171386 minutes
2023-04-27 11:54:04,419:INFO:SubProcess create_model() called ==================================
2023-04-27 11:54:04,419:INFO:Initializing create_model()
2023-04-27 11:54:04,420:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:54:04,420:INFO:Checking exceptions
2023-04-27 11:54:04,420:INFO:Importing libraries
2023-04-27 11:54:04,420:INFO:Copying training dataset
2023-04-27 11:54:04,791:INFO:Defining folds
2023-04-27 11:54:04,791:INFO:Declaring metric variables
2023-04-27 11:54:04,795:INFO:Importing untrained model
2023-04-27 11:54:04,801:INFO:Passive Aggressive Regressor Imported successfully
2023-04-27 11:54:04,808:INFO:Starting cross validation
2023-04-27 11:54:04,816:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:54:07,974:INFO:Calculating mean and std
2023-04-27 11:54:07,976:INFO:Creating metrics dataframe
2023-04-27 11:54:08,174:INFO:Uploading results into container
2023-04-27 11:54:08,175:INFO:Uploading model into container now
2023-04-27 11:54:08,175:INFO:_master_model_container: 9
2023-04-27 11:54:08,175:INFO:_display_container: 2
2023-04-27 11:54:08,176:INFO:PassiveAggressiveRegressor(random_state=123)
2023-04-27 11:54:08,176:INFO:create_model() successfully completed......................................
2023-04-27 11:54:08,352:INFO:SubProcess create_model() end ==================================
2023-04-27 11:54:08,352:INFO:Creating metrics dataframe
2023-04-27 11:54:08,368:INFO:Initializing Huber Regressor
2023-04-27 11:54:08,369:INFO:Total runtime is 0.9430995941162108 minutes
2023-04-27 11:54:08,373:INFO:SubProcess create_model() called ==================================
2023-04-27 11:54:08,373:INFO:Initializing create_model()
2023-04-27 11:54:08,373:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:54:08,373:INFO:Checking exceptions
2023-04-27 11:54:08,373:INFO:Importing libraries
2023-04-27 11:54:08,373:INFO:Copying training dataset
2023-04-27 11:54:08,732:INFO:Defining folds
2023-04-27 11:54:08,732:INFO:Declaring metric variables
2023-04-27 11:54:08,736:INFO:Importing untrained model
2023-04-27 11:54:08,741:INFO:Huber Regressor Imported successfully
2023-04-27 11:54:08,749:INFO:Starting cross validation
2023-04-27 11:54:08,756:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:54:11,931:INFO:Calculating mean and std
2023-04-27 11:54:11,933:INFO:Creating metrics dataframe
2023-04-27 11:54:12,140:INFO:Uploading results into container
2023-04-27 11:54:12,141:INFO:Uploading model into container now
2023-04-27 11:54:12,141:INFO:_master_model_container: 10
2023-04-27 11:54:12,141:INFO:_display_container: 2
2023-04-27 11:54:12,142:INFO:HuberRegressor()
2023-04-27 11:54:12,142:INFO:create_model() successfully completed......................................
2023-04-27 11:54:12,314:INFO:SubProcess create_model() end ==================================
2023-04-27 11:54:12,315:INFO:Creating metrics dataframe
2023-04-27 11:54:12,335:INFO:Initializing K Neighbors Regressor
2023-04-27 11:54:12,335:INFO:Total runtime is 1.009199539820353 minutes
2023-04-27 11:54:12,340:INFO:SubProcess create_model() called ==================================
2023-04-27 11:54:12,340:INFO:Initializing create_model()
2023-04-27 11:54:12,340:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:54:12,340:INFO:Checking exceptions
2023-04-27 11:54:12,341:INFO:Importing libraries
2023-04-27 11:54:12,341:INFO:Copying training dataset
2023-04-27 11:54:12,721:INFO:Defining folds
2023-04-27 11:54:12,721:INFO:Declaring metric variables
2023-04-27 11:54:12,728:INFO:Importing untrained model
2023-04-27 11:54:12,732:INFO:K Neighbors Regressor Imported successfully
2023-04-27 11:54:12,740:INFO:Starting cross validation
2023-04-27 11:54:12,747:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:54:31,056:INFO:Calculating mean and std
2023-04-27 11:54:31,060:INFO:Creating metrics dataframe
2023-04-27 11:54:31,330:INFO:Uploading results into container
2023-04-27 11:54:31,331:INFO:Uploading model into container now
2023-04-27 11:54:31,332:INFO:_master_model_container: 11
2023-04-27 11:54:31,332:INFO:_display_container: 2
2023-04-27 11:54:31,333:INFO:KNeighborsRegressor(n_jobs=-1)
2023-04-27 11:54:31,333:INFO:create_model() successfully completed......................................
2023-04-27 11:54:31,542:INFO:SubProcess create_model() end ==================================
2023-04-27 11:54:31,542:INFO:Creating metrics dataframe
2023-04-27 11:54:31,562:INFO:Initializing Decision Tree Regressor
2023-04-27 11:54:31,562:INFO:Total runtime is 1.329649539788564 minutes
2023-04-27 11:54:31,566:INFO:SubProcess create_model() called ==================================
2023-04-27 11:54:31,566:INFO:Initializing create_model()
2023-04-27 11:54:31,566:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:54:31,566:INFO:Checking exceptions
2023-04-27 11:54:31,567:INFO:Importing libraries
2023-04-27 11:54:31,567:INFO:Copying training dataset
2023-04-27 11:54:31,984:INFO:Defining folds
2023-04-27 11:54:31,985:INFO:Declaring metric variables
2023-04-27 11:54:31,989:INFO:Importing untrained model
2023-04-27 11:54:31,995:INFO:Decision Tree Regressor Imported successfully
2023-04-27 11:54:32,005:INFO:Starting cross validation
2023-04-27 11:54:32,013:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:54:36,504:INFO:Calculating mean and std
2023-04-27 11:54:36,507:INFO:Creating metrics dataframe
2023-04-27 11:54:36,845:INFO:Uploading results into container
2023-04-27 11:54:36,846:INFO:Uploading model into container now
2023-04-27 11:54:36,846:INFO:_master_model_container: 12
2023-04-27 11:54:36,847:INFO:_display_container: 2
2023-04-27 11:54:36,847:INFO:DecisionTreeRegressor(random_state=123)
2023-04-27 11:54:36,848:INFO:create_model() successfully completed......................................
2023-04-27 11:54:37,044:INFO:SubProcess create_model() end ==================================
2023-04-27 11:54:37,044:INFO:Creating metrics dataframe
2023-04-27 11:54:37,067:INFO:Initializing Random Forest Regressor
2023-04-27 11:54:37,067:INFO:Total runtime is 1.4213995774586994 minutes
2023-04-27 11:54:37,071:INFO:SubProcess create_model() called ==================================
2023-04-27 11:54:37,071:INFO:Initializing create_model()
2023-04-27 11:54:37,071:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:54:37,071:INFO:Checking exceptions
2023-04-27 11:54:37,072:INFO:Importing libraries
2023-04-27 11:54:37,072:INFO:Copying training dataset
2023-04-27 11:54:37,477:INFO:Defining folds
2023-04-27 11:54:37,477:INFO:Declaring metric variables
2023-04-27 11:54:37,483:INFO:Importing untrained model
2023-04-27 11:54:37,488:INFO:Random Forest Regressor Imported successfully
2023-04-27 11:54:37,497:INFO:Starting cross validation
2023-04-27 11:54:37,505:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:54:42,110:INFO:Calculating mean and std
2023-04-27 11:54:42,113:INFO:Creating metrics dataframe
2023-04-27 11:54:42,425:INFO:Uploading results into container
2023-04-27 11:54:42,426:INFO:Uploading model into container now
2023-04-27 11:54:42,426:INFO:_master_model_container: 13
2023-04-27 11:54:42,426:INFO:_display_container: 2
2023-04-27 11:54:42,427:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-04-27 11:54:42,427:INFO:create_model() successfully completed......................................
2023-04-27 11:54:42,626:INFO:SubProcess create_model() end ==================================
2023-04-27 11:54:42,627:INFO:Creating metrics dataframe
2023-04-27 11:54:42,646:INFO:Initializing Extra Trees Regressor
2023-04-27 11:54:42,647:INFO:Total runtime is 1.5143995364507037 minutes
2023-04-27 11:54:42,652:INFO:SubProcess create_model() called ==================================
2023-04-27 11:54:42,652:INFO:Initializing create_model()
2023-04-27 11:54:42,653:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:54:42,653:INFO:Checking exceptions
2023-04-27 11:54:42,653:INFO:Importing libraries
2023-04-27 11:54:42,653:INFO:Copying training dataset
2023-04-27 11:54:43,087:INFO:Defining folds
2023-04-27 11:54:43,088:INFO:Declaring metric variables
2023-04-27 11:54:43,093:INFO:Importing untrained model
2023-04-27 11:54:43,101:INFO:Extra Trees Regressor Imported successfully
2023-04-27 11:54:43,109:INFO:Starting cross validation
2023-04-27 11:54:43,117:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:54:47,166:INFO:Calculating mean and std
2023-04-27 11:54:47,168:INFO:Creating metrics dataframe
2023-04-27 11:54:47,381:INFO:Uploading results into container
2023-04-27 11:54:47,383:INFO:Uploading model into container now
2023-04-27 11:54:47,383:INFO:_master_model_container: 14
2023-04-27 11:54:47,383:INFO:_display_container: 2
2023-04-27 11:54:47,384:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-04-27 11:54:47,384:INFO:create_model() successfully completed......................................
2023-04-27 11:54:47,585:INFO:SubProcess create_model() end ==================================
2023-04-27 11:54:47,585:INFO:Creating metrics dataframe
2023-04-27 11:54:47,604:INFO:Initializing AdaBoost Regressor
2023-04-27 11:54:47,604:INFO:Total runtime is 1.5970162312189735 minutes
2023-04-27 11:54:47,608:INFO:SubProcess create_model() called ==================================
2023-04-27 11:54:47,609:INFO:Initializing create_model()
2023-04-27 11:54:47,609:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:54:47,609:INFO:Checking exceptions
2023-04-27 11:54:47,609:INFO:Importing libraries
2023-04-27 11:54:47,609:INFO:Copying training dataset
2023-04-27 11:54:47,992:INFO:Defining folds
2023-04-27 11:54:47,992:INFO:Declaring metric variables
2023-04-27 11:54:47,998:INFO:Importing untrained model
2023-04-27 11:54:48,003:INFO:AdaBoost Regressor Imported successfully
2023-04-27 11:54:48,013:INFO:Starting cross validation
2023-04-27 11:54:48,020:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:54:51,934:INFO:Calculating mean and std
2023-04-27 11:54:51,937:INFO:Creating metrics dataframe
2023-04-27 11:54:52,591:INFO:Uploading results into container
2023-04-27 11:54:52,593:INFO:Uploading model into container now
2023-04-27 11:54:52,593:INFO:_master_model_container: 15
2023-04-27 11:54:52,609:INFO:_display_container: 2
2023-04-27 11:54:52,612:INFO:AdaBoostRegressor(random_state=123)
2023-04-27 11:54:52,612:INFO:create_model() successfully completed......................................
2023-04-27 11:54:52,877:INFO:SubProcess create_model() end ==================================
2023-04-27 11:54:52,877:INFO:Creating metrics dataframe
2023-04-27 11:54:52,900:INFO:Initializing Gradient Boosting Regressor
2023-04-27 11:54:52,900:INFO:Total runtime is 1.6852828701337175 minutes
2023-04-27 11:54:52,905:INFO:SubProcess create_model() called ==================================
2023-04-27 11:54:52,906:INFO:Initializing create_model()
2023-04-27 11:54:52,906:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:54:52,906:INFO:Checking exceptions
2023-04-27 11:54:52,907:INFO:Importing libraries
2023-04-27 11:54:52,907:INFO:Copying training dataset
2023-04-27 11:54:53,393:INFO:Defining folds
2023-04-27 11:54:53,393:INFO:Declaring metric variables
2023-04-27 11:54:53,403:INFO:Importing untrained model
2023-04-27 11:54:53,410:INFO:Gradient Boosting Regressor Imported successfully
2023-04-27 11:54:53,425:INFO:Starting cross validation
2023-04-27 11:54:53,441:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:54:56,619:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 11:54:56,619:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 11:54:56,619:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 11:54:56,620:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 11:54:56,620:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 11:55:01,260:INFO:Calculating mean and std
2023-04-27 11:55:01,262:INFO:Creating metrics dataframe
2023-04-27 11:55:02,257:INFO:Uploading results into container
2023-04-27 11:55:02,258:INFO:Uploading model into container now
2023-04-27 11:55:02,259:INFO:_master_model_container: 16
2023-04-27 11:55:02,259:INFO:_display_container: 2
2023-04-27 11:55:02,259:INFO:GradientBoostingRegressor(random_state=123)
2023-04-27 11:55:02,259:INFO:create_model() successfully completed......................................
2023-04-27 11:55:02,445:INFO:SubProcess create_model() end ==================================
2023-04-27 11:55:02,445:INFO:Creating metrics dataframe
2023-04-27 11:55:02,464:INFO:Initializing Extreme Gradient Boosting
2023-04-27 11:55:02,465:INFO:Total runtime is 1.844678676128387 minutes
2023-04-27 11:55:02,470:INFO:SubProcess create_model() called ==================================
2023-04-27 11:55:02,470:INFO:Initializing create_model()
2023-04-27 11:55:02,470:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:55:02,470:INFO:Checking exceptions
2023-04-27 11:55:02,471:INFO:Importing libraries
2023-04-27 11:55:02,471:INFO:Copying training dataset
2023-04-27 11:55:02,879:INFO:Defining folds
2023-04-27 11:55:02,879:INFO:Declaring metric variables
2023-04-27 11:55:02,884:INFO:Importing untrained model
2023-04-27 11:55:02,889:INFO:Extreme Gradient Boosting Imported successfully
2023-04-27 11:55:02,898:INFO:Starting cross validation
2023-04-27 11:55:02,905:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:55:10,046:INFO:Calculating mean and std
2023-04-27 11:55:10,048:INFO:Creating metrics dataframe
2023-04-27 11:55:10,301:INFO:Uploading results into container
2023-04-27 11:55:10,301:INFO:Uploading model into container now
2023-04-27 11:55:10,302:INFO:_master_model_container: 17
2023-04-27 11:55:10,302:INFO:_display_container: 2
2023-04-27 11:55:10,304:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, gamma=None,
             gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, n_estimators=100, n_jobs=-1,
             num_parallel_tree=None, predictor=None, random_state=123,
             reg_alpha=None, reg_lambda=None, ...)
2023-04-27 11:55:10,305:INFO:create_model() successfully completed......................................
2023-04-27 11:55:10,543:INFO:SubProcess create_model() end ==================================
2023-04-27 11:55:10,543:INFO:Creating metrics dataframe
2023-04-27 11:55:10,577:INFO:Initializing Light Gradient Boosting Machine
2023-04-27 11:55:10,577:INFO:Total runtime is 1.9798968633015948 minutes
2023-04-27 11:55:10,584:INFO:SubProcess create_model() called ==================================
2023-04-27 11:55:10,585:INFO:Initializing create_model()
2023-04-27 11:55:10,585:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:55:10,585:INFO:Checking exceptions
2023-04-27 11:55:10,585:INFO:Importing libraries
2023-04-27 11:55:10,585:INFO:Copying training dataset
2023-04-27 11:55:11,197:INFO:Defining folds
2023-04-27 11:55:11,197:INFO:Declaring metric variables
2023-04-27 11:55:11,204:INFO:Importing untrained model
2023-04-27 11:55:11,213:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-27 11:55:11,229:INFO:Starting cross validation
2023-04-27 11:55:11,243:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:55:16,650:INFO:Calculating mean and std
2023-04-27 11:55:16,653:INFO:Creating metrics dataframe
2023-04-27 11:55:16,900:INFO:Uploading results into container
2023-04-27 11:55:16,901:INFO:Uploading model into container now
2023-04-27 11:55:16,901:INFO:_master_model_container: 18
2023-04-27 11:55:16,901:INFO:_display_container: 2
2023-04-27 11:55:16,902:INFO:LGBMRegressor(random_state=123)
2023-04-27 11:55:16,902:INFO:create_model() successfully completed......................................
2023-04-27 11:55:17,098:INFO:SubProcess create_model() end ==================================
2023-04-27 11:55:17,099:INFO:Creating metrics dataframe
2023-04-27 11:55:17,124:INFO:Initializing Dummy Regressor
2023-04-27 11:55:17,124:INFO:Total runtime is 2.08902309735616 minutes
2023-04-27 11:55:17,128:INFO:SubProcess create_model() called ==================================
2023-04-27 11:55:17,129:INFO:Initializing create_model()
2023-04-27 11:55:17,129:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052D935F08>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:55:17,129:INFO:Checking exceptions
2023-04-27 11:55:17,129:INFO:Importing libraries
2023-04-27 11:55:17,129:INFO:Copying training dataset
2023-04-27 11:55:17,619:INFO:Defining folds
2023-04-27 11:55:17,619:INFO:Declaring metric variables
2023-04-27 11:55:17,626:INFO:Importing untrained model
2023-04-27 11:55:17,631:INFO:Dummy Regressor Imported successfully
2023-04-27 11:55:17,644:INFO:Starting cross validation
2023-04-27 11:55:17,655:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 11:55:21,485:INFO:Calculating mean and std
2023-04-27 11:55:21,488:INFO:Creating metrics dataframe
2023-04-27 11:55:21,730:INFO:Uploading results into container
2023-04-27 11:55:21,731:INFO:Uploading model into container now
2023-04-27 11:55:21,732:INFO:_master_model_container: 19
2023-04-27 11:55:21,732:INFO:_display_container: 2
2023-04-27 11:55:21,732:INFO:DummyRegressor()
2023-04-27 11:55:21,732:INFO:create_model() successfully completed......................................
2023-04-27 11:55:21,926:INFO:SubProcess create_model() end ==================================
2023-04-27 11:55:21,926:INFO:Creating metrics dataframe
2023-04-27 11:55:22,035:INFO:Initializing create_model()
2023-04-27 11:55:22,036:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-27 11:55:22,036:INFO:Checking exceptions
2023-04-27 11:55:22,077:INFO:Importing libraries
2023-04-27 11:55:22,078:INFO:Copying training dataset
2023-04-27 11:55:22,465:INFO:Defining folds
2023-04-27 11:55:22,465:INFO:Declaring metric variables
2023-04-27 11:55:22,465:INFO:Importing untrained model
2023-04-27 11:55:22,465:INFO:Declaring custom model
2023-04-27 11:55:22,466:INFO:Extra Trees Regressor Imported successfully
2023-04-27 11:55:22,471:INFO:Cross validation set to False
2023-04-27 11:55:22,472:INFO:Fitting Model
2023-04-27 11:55:23,453:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-04-27 11:55:23,454:INFO:create_model() successfully completed......................................
2023-04-27 11:55:23,634:INFO:Creating Dashboard logs
2023-04-27 11:55:23,638:INFO:Model: Extra Trees Regressor
2023-04-27 11:55:23,745:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2023-04-27 11:55:24,024:INFO:Initializing predict_model()
2023-04-27 11:55:24,024:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002052F657B48>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x000002052CA5ECA8>)
2023-04-27 11:55:24,024:INFO:Checking exceptions
2023-04-27 11:55:24,024:INFO:Preloading libraries
2023-04-27 11:55:25,224:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\_distutils_hack\__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")

2023-04-27 11:55:27,103:INFO:Creating Dashboard logs
2023-04-27 11:55:27,107:INFO:Model: Extreme Gradient Boosting
2023-04-27 11:55:27,167:INFO:Logged params: {'objective': 'reg:squarederror', 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': -1, 'num_parallel_tree': None, 'predictor': None, 'random_state': 123, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'auto', 'validate_parameters': None, 'verbosity': 0}
2023-04-27 11:55:28,100:INFO:Creating Dashboard logs
2023-04-27 11:55:28,105:INFO:Model: Random Forest Regressor
2023-04-27 11:55:28,198:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2023-04-27 11:55:28,879:INFO:Creating Dashboard logs
2023-04-27 11:55:28,883:INFO:Model: Gradient Boosting Regressor
2023-04-27 11:55:28,992:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-04-27 11:55:30,049:INFO:Creating Dashboard logs
2023-04-27 11:55:30,055:INFO:Model: Decision Tree Regressor
2023-04-27 11:55:30,236:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 123, 'splitter': 'best'}
2023-04-27 11:55:31,471:INFO:Creating Dashboard logs
2023-04-27 11:55:31,477:INFO:Model: Bayesian Ridge
2023-04-27 11:55:31,560:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'normalize': 'deprecated', 'tol': 0.001, 'verbose': False}
2023-04-27 11:55:32,615:INFO:Creating Dashboard logs
2023-04-27 11:55:32,620:INFO:Model: Ridge Regression
2023-04-27 11:55:32,684:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': 'deprecated', 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.001}
2023-04-27 11:55:33,864:INFO:Creating Dashboard logs
2023-04-27 11:55:33,869:INFO:Model: Light Gradient Boosting Machine
2023-04-27 11:55:34,012:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2023-04-27 11:55:34,727:INFO:Creating Dashboard logs
2023-04-27 11:55:34,732:INFO:Model: Linear Regression
2023-04-27 11:55:34,767:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'normalize': 'deprecated', 'positive': False}
2023-04-27 11:55:35,350:INFO:Creating Dashboard logs
2023-04-27 11:55:35,355:INFO:Model: Lasso Regression
2023-04-27 11:55:35,397:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': 'deprecated', 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2023-04-27 11:55:35,887:INFO:Creating Dashboard logs
2023-04-27 11:55:35,892:INFO:Model: Orthogonal Matching Pursuit
2023-04-27 11:55:35,926:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'normalize': 'deprecated', 'precompute': 'auto', 'tol': None}
2023-04-27 11:55:36,401:INFO:Creating Dashboard logs
2023-04-27 11:55:36,406:INFO:Model: Lasso Least Angle Regression
2023-04-27 11:55:36,445:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'normalize': 'deprecated', 'positive': False, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2023-04-27 11:55:36,899:INFO:Creating Dashboard logs
2023-04-27 11:55:36,903:INFO:Model: K Neighbors Regressor
2023-04-27 11:55:36,935:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2023-04-27 11:55:37,411:INFO:Creating Dashboard logs
2023-04-27 11:55:37,415:INFO:Model: AdaBoost Regressor
2023-04-27 11:55:37,452:INFO:Logged params: {'base_estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 123}
2023-04-27 11:55:37,899:INFO:Creating Dashboard logs
2023-04-27 11:55:37,903:INFO:Model: Elastic Net
2023-04-27 11:55:37,937:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'normalize': 'deprecated', 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2023-04-27 11:55:38,568:INFO:Creating Dashboard logs
2023-04-27 11:55:38,574:INFO:Model: Huber Regressor
2023-04-27 11:55:38,611:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2023-04-27 11:55:39,221:INFO:Creating Dashboard logs
2023-04-27 11:55:39,227:INFO:Model: Dummy Regressor
2023-04-27 11:55:39,264:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2023-04-27 11:55:39,735:INFO:Creating Dashboard logs
2023-04-27 11:55:39,740:INFO:Model: Passive Aggressive Regressor
2023-04-27 11:55:39,775:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-04-27 11:55:40,231:INFO:Creating Dashboard logs
2023-04-27 11:55:40,235:INFO:Model: Least Angle Regression
2023-04-27 11:55:40,267:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'normalize': 'deprecated', 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2023-04-27 11:55:40,742:INFO:_master_model_container: 19
2023-04-27 11:55:40,742:INFO:_display_container: 2
2023-04-27 11:55:40,743:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-04-27 11:55:40,744:INFO:compare_models() successfully completed......................................
2023-04-27 11:55:40,876:INFO:gpu_param set to False
2023-04-27 11:55:41,342:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:55:41,346:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 11:55:41,947:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 11:55:41,957:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:15,801:INFO:PyCaret RegressionExperiment
2023-04-27 14:45:15,801:INFO:Logging name: price
2023-04-27 14:45:15,802:INFO:ML Usecase: MLUsecase.REGRESSION
2023-04-27 14:45:15,802:INFO:version 3.0.0
2023-04-27 14:45:15,802:INFO:Initializing setup()
2023-04-27 14:45:15,802:INFO:self.USI: 92ce
2023-04-27 14:45:15,802:INFO:self._variable_keys: {'exp_id', 'X_train', 'X_test', 'idx', 'fold_generator', 'fold_shuffle_param', '_ml_usecase', 'logging_param', 'gpu_n_jobs_param', 'USI', 'exp_name_log', 'X', 'y_test', 'seed', 'n_jobs_param', 'log_plots_param', '_available_plots', 'pipeline', 'html_param', 'memory', 'gpu_param', 'y', 'fold_groups_param', 'target_param', 'y_train', 'data', 'transform_target_param'}
2023-04-27 14:45:15,802:INFO:Checking environment
2023-04-27 14:45:15,802:INFO:python_version: 3.7.4
2023-04-27 14:45:15,802:INFO:python_build: ('tags/v3.7.4:e09359112e', 'Jul  8 2019 20:34:20')
2023-04-27 14:45:15,802:INFO:machine: AMD64
2023-04-27 14:45:15,803:INFO:platform: Windows-10-10.0.19041-SP0
2023-04-27 14:45:15,805:INFO:Memory: svmem(total=16907886592, available=2216681472, percent=86.9, used=14691205120, free=2216681472)
2023-04-27 14:45:15,805:INFO:Physical Core: 4
2023-04-27 14:45:15,805:INFO:Logical Core: 8
2023-04-27 14:45:15,805:INFO:Checking libraries
2023-04-27 14:45:15,805:INFO:System:
2023-04-27 14:45:15,806:INFO:    python: 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]
2023-04-27 14:45:15,806:INFO:executable: c:\Users\tonim\AppData\Local\Programs\Python\Python37\python.exe
2023-04-27 14:45:15,806:INFO:   machine: Windows-10-10.0.19041-SP0
2023-04-27 14:45:15,806:INFO:PyCaret required dependencies:
2023-04-27 14:45:15,808:INFO:                 pip: 22.2.2
2023-04-27 14:45:15,808:INFO:          setuptools: 65.3.0
2023-04-27 14:45:15,809:INFO:             pycaret: 3.0.0
2023-04-27 14:45:15,809:INFO:             IPython: 7.34.0
2023-04-27 14:45:15,809:INFO:          ipywidgets: 7.7.1
2023-04-27 14:45:15,809:INFO:                tqdm: 4.64.0
2023-04-27 14:45:15,809:INFO:               numpy: 1.21.6
2023-04-27 14:45:15,809:INFO:              pandas: 1.3.5
2023-04-27 14:45:15,809:INFO:              jinja2: 3.1.2
2023-04-27 14:45:15,809:INFO:               scipy: 1.5.4
2023-04-27 14:45:15,809:INFO:              joblib: 1.2.0
2023-04-27 14:45:15,809:INFO:             sklearn: 1.0.2
2023-04-27 14:45:15,809:INFO:                pyod: 1.0.9
2023-04-27 14:45:15,809:INFO:            imblearn: 0.9.0
2023-04-27 14:45:15,809:INFO:   category_encoders: 2.6.0
2023-04-27 14:45:15,809:INFO:            lightgbm: 3.3.5
2023-04-27 14:45:15,809:INFO:               numba: 0.56.0
2023-04-27 14:45:15,809:INFO:            requests: 2.28.1
2023-04-27 14:45:15,809:INFO:          matplotlib: 3.5.2
2023-04-27 14:45:15,809:INFO:          scikitplot: 0.3.7
2023-04-27 14:45:15,809:INFO:         yellowbrick: 1.5
2023-04-27 14:45:15,809:INFO:              plotly: 5.9.0
2023-04-27 14:45:15,809:INFO:             kaleido: 0.2.1
2023-04-27 14:45:15,809:INFO:         statsmodels: 0.13.2
2023-04-27 14:45:15,809:INFO:              sktime: 0.17.1
2023-04-27 14:45:15,809:INFO:               tbats: 1.1.3
2023-04-27 14:45:15,809:INFO:            pmdarima: 2.0.1
2023-04-27 14:45:15,809:INFO:              psutil: 5.9.1
2023-04-27 14:45:15,809:INFO:PyCaret optional dependencies:
2023-04-27 14:45:15,810:INFO:                shap: 0.41.0
2023-04-27 14:45:15,810:INFO:           interpret: Not installed
2023-04-27 14:45:15,810:INFO:                umap: Not installed
2023-04-27 14:45:15,810:INFO:    pandas_profiling: 3.3.0
2023-04-27 14:45:15,810:INFO:  explainerdashboard: Not installed
2023-04-27 14:45:15,810:INFO:             autoviz: Not installed
2023-04-27 14:45:15,810:INFO:           fairlearn: Not installed
2023-04-27 14:45:15,810:INFO:             xgboost: 1.6.2
2023-04-27 14:45:15,810:INFO:            catboost: Not installed
2023-04-27 14:45:15,810:INFO:              kmodes: Not installed
2023-04-27 14:45:15,810:INFO:             mlxtend: Not installed
2023-04-27 14:45:15,810:INFO:       statsforecast: Not installed
2023-04-27 14:45:15,810:INFO:        tune_sklearn: Not installed
2023-04-27 14:45:15,810:INFO:                 ray: Not installed
2023-04-27 14:45:15,810:INFO:            hyperopt: 0.2.7
2023-04-27 14:45:15,810:INFO:              optuna: Not installed
2023-04-27 14:45:15,810:INFO:               skopt: Not installed
2023-04-27 14:45:15,810:INFO:              mlflow: 1.30.1
2023-04-27 14:45:15,810:INFO:              gradio: Not installed
2023-04-27 14:45:15,810:INFO:             fastapi: Not installed
2023-04-27 14:45:15,810:INFO:             uvicorn: Not installed
2023-04-27 14:45:15,810:INFO:              m2cgen: Not installed
2023-04-27 14:45:15,810:INFO:           evidently: Not installed
2023-04-27 14:45:15,810:INFO:               fugue: Not installed
2023-04-27 14:45:15,810:INFO:           streamlit: 1.11.0
2023-04-27 14:45:15,810:INFO:             prophet: Not installed
2023-04-27 14:45:15,810:INFO:None
2023-04-27 14:45:15,811:INFO:Set up data.
2023-04-27 14:45:17,955:INFO:Set up train/test split.
2023-04-27 14:45:18,188:INFO:Set up index.
2023-04-27 14:45:18,193:INFO:Set up folding strategy.
2023-04-27 14:45:18,193:INFO:Assigning column types.
2023-04-27 14:45:18,358:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-27 14:45:18,360:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-27 14:45:18,367:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-27 14:45:18,372:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 14:45:18,682:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 14:45:18,735:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 14:45:18,736:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:18,739:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:18,739:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-27 14:45:18,746:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-27 14:45:18,754:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 14:45:19,040:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 14:45:19,093:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 14:45:19,094:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:19,096:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:19,097:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-04-27 14:45:19,105:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-27 14:45:19,110:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 14:45:19,379:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 14:45:19,436:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 14:45:19,437:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:19,441:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:19,448:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-27 14:45:19,455:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 14:45:19,718:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 14:45:19,767:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 14:45:19,769:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:19,772:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:19,772:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-04-27 14:45:19,782:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 14:45:20,056:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 14:45:20,110:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 14:45:20,111:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:20,114:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:20,124:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 14:45:20,391:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 14:45:20,439:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 14:45:20,440:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:20,443:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:20,443:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-04-27 14:45:20,713:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 14:45:20,767:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 14:45:20,767:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:20,770:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:21,074:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 14:45:21,131:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 14:45:21,131:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:21,134:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:21,135:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-27 14:45:21,412:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 14:45:21,466:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:21,470:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:21,736:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 14:45:21,786:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:21,789:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:21,789:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-04-27 14:45:22,089:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:22,092:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:22,477:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:22,480:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:22,484:INFO:Preparing preprocessing pipeline...
2023-04-27 14:45:22,484:INFO:Set up simple imputation.
2023-04-27 14:45:22,502:INFO:Set up column name cleaning.
2023-04-27 14:45:23,376:INFO:Finished creating preprocessing pipeline.
2023-04-27 14:45:23,392:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power',
                                             'bike_name_BMW F750 GS 850cc',
                                             'bike_name_BMW G 310 GS',
                                             'bike_name_BMW G 310 R',
                                             'bike_name_BMW S 1000 RR Pro',
                                             'bike_name_BMW S 1000 XR Pro',
                                             'bike_name_Bajaj  Pulsar 180cc',
                                             'bike_name_Bajaj Avenger 150cc',...
                                             'bike_name_Bajaj CT 100 Alloy',
                                             'bike_name_Bajaj CT 100 B',
                                             'bike_name_Bajaj CT 100 ES Alloy',
                                             'bike_name_Bajaj CT 100 KS Alloy',
                                             'bike_name_Bajaj CT 100 Spoke', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-04-27 14:45:23,392:INFO:Creating final display dataframe.
2023-04-27 14:45:25,118:INFO:Setup _display_container:                     Description         Value
0                    Session id           123
1                        Target         price
2                   Target type    Regression
3           Original data shape  (26118, 942)
4        Transformed data shape  (26118, 942)
5   Transformed train set shape  (18282, 942)
6    Transformed test set shape   (7836, 942)
7              Numeric features           941
8                    Preprocess          True
9               Imputation type        simple
10           Numeric imputation          mean
11       Categorical imputation          mode
12               Fold Generator         KFold
13                  Fold Number            10
14                     CPU Jobs            -1
15                      Use GPU         False
16               Log Experiment  MlflowLogger
17              Experiment Name         price
18                          USI          92ce
2023-04-27 14:45:25,485:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:25,489:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:25,837:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 14:45:25,841:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 14:45:25,843:INFO:Logging experiment in loggers
2023-04-27 14:45:25,909:INFO:SubProcess save_model() called ==================================
2023-04-27 14:45:25,938:INFO:Initializing save_model()
2023-04-27 14:45:25,938:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power',
                                             'bike_name_BMW F750 GS 850cc',
                                             'bike_name_BMW G 310 GS',
                                             'bike_name_BMW G 310 R',
                                             'bike_name_BMW S 1000 RR Pro',
                                             'bike_name_BMW S 1000 XR Pro',
                                             'bike_name_Bajaj  Pulsar 180cc',
                                             'bike_name_Bajaj Avenger 150cc',...
                                             'bike_name_Bajaj CT 100 Alloy',
                                             'bike_name_Bajaj CT 100 B',
                                             'bike_name_Bajaj CT 100 ES Alloy',
                                             'bike_name_Bajaj CT 100 KS Alloy',
                                             'bike_name_Bajaj CT 100 Spoke', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))]), model_name=C:\Users\tonim\AppData\Local\Temp\tmpr1juioii\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power',
                                             'bike_name_BMW F750 GS 850cc',
                                             'bike_name_BMW G 310 GS',
                                             'bike_name_BMW G 310 R',
                                             'bike_name_BMW S 1000 RR Pro',
                                             'bike_name_BMW S 1000 XR Pro',
                                             'bike_name_Bajaj  Pulsar 180cc',
                                             'bike_name_Bajaj Avenger 150cc',...
                                             'bike_name_Bajaj CT 100 Alloy',
                                             'bike_name_Bajaj CT 100 B',
                                             'bike_name_Bajaj CT 100 ES Alloy',
                                             'bike_name_Bajaj CT 100 KS Alloy',
                                             'bike_name_Bajaj CT 100 Spoke', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2023-04-27 14:45:25,938:INFO:Adding model into prep_pipe
2023-04-27 14:45:25,944:WARNING:Only Model saved as it was a pipeline.
2023-04-27 14:45:25,962:INFO:C:\Users\tonim\AppData\Local\Temp\tmpr1juioii\Transformation Pipeline.pkl saved in current working directory
2023-04-27 14:45:25,976:INFO:Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power',
                                             'bike_name_BMW F750 GS 850cc',
                                             'bike_name_BMW G 310 GS',
                                             'bike_name_BMW G 310 R',
                                             'bike_name_BMW S 1000 RR Pro',
                                             'bike_name_BMW S 1000 XR Pro',
                                             'bike_name_Bajaj  Pulsar 180cc',
                                             'bike_name_Bajaj Avenger 150cc',...
                                             'bike_name_Bajaj CT 100 Alloy',
                                             'bike_name_Bajaj CT 100 B',
                                             'bike_name_Bajaj CT 100 ES Alloy',
                                             'bike_name_Bajaj CT 100 KS Alloy',
                                             'bike_name_Bajaj CT 100 Spoke', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-04-27 14:45:25,976:INFO:save_model() successfully completed......................................
2023-04-27 14:45:26,407:INFO:SubProcess save_model() end ==================================
2023-04-27 14:45:26,421:INFO:setup() successfully completed in 10.56s...............
2023-04-27 14:45:29,278:INFO:Initializing compare_models()
2023-04-27 14:45:29,278:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-04-27 14:45:29,278:INFO:Checking exceptions
2023-04-27 14:45:29,373:INFO:Preparing display monitor
2023-04-27 14:45:29,410:INFO:Initializing Linear Regression
2023-04-27 14:45:29,411:INFO:Total runtime is 1.632372538248698e-05 minutes
2023-04-27 14:45:29,415:INFO:SubProcess create_model() called ==================================
2023-04-27 14:45:29,416:INFO:Initializing create_model()
2023-04-27 14:45:29,416:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:45:29,416:INFO:Checking exceptions
2023-04-27 14:45:29,416:INFO:Importing libraries
2023-04-27 14:45:29,416:INFO:Copying training dataset
2023-04-27 14:45:29,740:INFO:Defining folds
2023-04-27 14:45:29,740:INFO:Declaring metric variables
2023-04-27 14:45:29,745:INFO:Importing untrained model
2023-04-27 14:45:29,748:INFO:Linear Regression Imported successfully
2023-04-27 14:45:29,786:INFO:Starting cross validation
2023-04-27 14:45:29,796:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:45:42,968:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:236: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  **fit_params_steps.get(name, {}),

2023-04-27 14:45:43,318:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 14:45:43,500:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 14:45:44,294:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 14:45:44,294:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 14:45:54,861:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:45:54,862:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:46:01,919:INFO:Calculating mean and std
2023-04-27 14:46:01,921:INFO:Creating metrics dataframe
2023-04-27 14:46:02,171:INFO:Uploading results into container
2023-04-27 14:46:02,172:INFO:Uploading model into container now
2023-04-27 14:46:02,180:INFO:_master_model_container: 1
2023-04-27 14:46:02,180:INFO:_display_container: 2
2023-04-27 14:46:02,181:INFO:LinearRegression(n_jobs=-1)
2023-04-27 14:46:02,181:INFO:create_model() successfully completed......................................
2023-04-27 14:46:02,458:INFO:SubProcess create_model() end ==================================
2023-04-27 14:46:02,458:INFO:Creating metrics dataframe
2023-04-27 14:46:02,475:INFO:Initializing Lasso Regression
2023-04-27 14:46:02,475:INFO:Total runtime is 0.5510713497797648 minutes
2023-04-27 14:46:02,479:INFO:SubProcess create_model() called ==================================
2023-04-27 14:46:02,479:INFO:Initializing create_model()
2023-04-27 14:46:02,479:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:46:02,479:INFO:Checking exceptions
2023-04-27 14:46:02,479:INFO:Importing libraries
2023-04-27 14:46:02,480:INFO:Copying training dataset
2023-04-27 14:46:02,762:INFO:Defining folds
2023-04-27 14:46:02,762:INFO:Declaring metric variables
2023-04-27 14:46:02,767:INFO:Importing untrained model
2023-04-27 14:46:02,771:INFO:Lasso Regression Imported successfully
2023-04-27 14:46:02,778:INFO:Starting cross validation
2023-04-27 14:46:02,783:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:46:34,024:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.674e+12, tolerance: 1.334e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:46:34,670:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:46:35,962:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:46:38,208:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.601e+12, tolerance: 1.329e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:46:38,826:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 14:46:39,362:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.764e+12, tolerance: 1.325e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:46:40,319:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:46:41,462:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.796e+12, tolerance: 1.366e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:46:41,999:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:46:42,154:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.699e+12, tolerance: 1.354e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:46:42,249:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.715e+12, tolerance: 1.370e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:46:42,425:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.735e+12, tolerance: 1.333e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:46:43,428:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.424e+12, tolerance: 1.389e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:46:56,917:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.743e+12, tolerance: 1.360e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:46:59,078:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.800e+12, tolerance: 1.383e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:46:59,688:INFO:Calculating mean and std
2023-04-27 14:46:59,690:INFO:Creating metrics dataframe
2023-04-27 14:46:59,889:INFO:Uploading results into container
2023-04-27 14:46:59,890:INFO:Uploading model into container now
2023-04-27 14:46:59,890:INFO:_master_model_container: 2
2023-04-27 14:46:59,890:INFO:_display_container: 2
2023-04-27 14:46:59,890:INFO:Lasso(random_state=123)
2023-04-27 14:46:59,891:INFO:create_model() successfully completed......................................
2023-04-27 14:47:00,241:INFO:SubProcess create_model() end ==================================
2023-04-27 14:47:00,241:INFO:Creating metrics dataframe
2023-04-27 14:47:00,256:INFO:Initializing Ridge Regression
2023-04-27 14:47:00,256:INFO:Total runtime is 1.514096736907959 minutes
2023-04-27 14:47:00,261:INFO:SubProcess create_model() called ==================================
2023-04-27 14:47:00,262:INFO:Initializing create_model()
2023-04-27 14:47:00,262:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:47:00,262:INFO:Checking exceptions
2023-04-27 14:47:00,262:INFO:Importing libraries
2023-04-27 14:47:00,262:INFO:Copying training dataset
2023-04-27 14:47:00,551:INFO:Defining folds
2023-04-27 14:47:00,551:INFO:Declaring metric variables
2023-04-27 14:47:00,555:INFO:Importing untrained model
2023-04-27 14:47:00,561:INFO:Ridge Regression Imported successfully
2023-04-27 14:47:00,568:INFO:Starting cross validation
2023-04-27 14:47:00,574:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:47:02,592:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.84032e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 14:47:02,595:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.93272e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 14:47:02,603:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.82412e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 14:47:02,657:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.85504e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 14:47:02,744:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.93895e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 14:47:02,855:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.74052e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 14:47:02,875:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.78036e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 14:47:03,000:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.74524e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 14:47:04,015:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:47:05,453:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.75029e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 14:47:05,531:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.65663e-14): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 14:47:07,015:INFO:Calculating mean and std
2023-04-27 14:47:07,018:INFO:Creating metrics dataframe
2023-04-27 14:47:07,307:INFO:Uploading results into container
2023-04-27 14:47:07,308:INFO:Uploading model into container now
2023-04-27 14:47:07,308:INFO:_master_model_container: 3
2023-04-27 14:47:07,308:INFO:_display_container: 2
2023-04-27 14:47:07,308:INFO:Ridge(random_state=123)
2023-04-27 14:47:07,308:INFO:create_model() successfully completed......................................
2023-04-27 14:47:07,572:INFO:SubProcess create_model() end ==================================
2023-04-27 14:47:07,572:INFO:Creating metrics dataframe
2023-04-27 14:47:07,589:INFO:Initializing Elastic Net
2023-04-27 14:47:07,589:INFO:Total runtime is 1.6363136092821757 minutes
2023-04-27 14:47:07,593:INFO:SubProcess create_model() called ==================================
2023-04-27 14:47:07,593:INFO:Initializing create_model()
2023-04-27 14:47:07,593:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:47:07,593:INFO:Checking exceptions
2023-04-27 14:47:07,593:INFO:Importing libraries
2023-04-27 14:47:07,594:INFO:Copying training dataset
2023-04-27 14:47:07,861:INFO:Defining folds
2023-04-27 14:47:07,861:INFO:Declaring metric variables
2023-04-27 14:47:07,865:INFO:Importing untrained model
2023-04-27 14:47:07,870:INFO:Elastic Net Imported successfully
2023-04-27 14:47:07,878:INFO:Starting cross validation
2023-04-27 14:47:07,883:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:47:44,400:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.517e+13, tolerance: 1.389e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:47:46,580:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.510e+13, tolerance: 1.333e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:47:48,005:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.412e+13, tolerance: 1.325e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:47:53,060:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.510e+13, tolerance: 1.334e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:47:54,382:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.514e+13, tolerance: 1.354e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:47:54,559:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.446e+13, tolerance: 1.370e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:47:56,324:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.534e+13, tolerance: 1.366e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:47:56,940:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.368e+13, tolerance: 1.329e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:48:11,439:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.473e+13, tolerance: 1.360e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:48:12,718:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.538e+13, tolerance: 1.383e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 14:48:13,420:INFO:Calculating mean and std
2023-04-27 14:48:13,421:INFO:Creating metrics dataframe
2023-04-27 14:48:13,637:INFO:Uploading results into container
2023-04-27 14:48:13,638:INFO:Uploading model into container now
2023-04-27 14:48:13,638:INFO:_master_model_container: 4
2023-04-27 14:48:13,638:INFO:_display_container: 2
2023-04-27 14:48:13,638:INFO:ElasticNet(random_state=123)
2023-04-27 14:48:13,638:INFO:create_model() successfully completed......................................
2023-04-27 14:48:13,890:INFO:SubProcess create_model() end ==================================
2023-04-27 14:48:13,890:INFO:Creating metrics dataframe
2023-04-27 14:48:13,904:INFO:Initializing Least Angle Regression
2023-04-27 14:48:13,904:INFO:Total runtime is 2.7415676712989807 minutes
2023-04-27 14:48:13,908:INFO:SubProcess create_model() called ==================================
2023-04-27 14:48:13,909:INFO:Initializing create_model()
2023-04-27 14:48:13,909:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:48:13,909:INFO:Checking exceptions
2023-04-27 14:48:13,909:INFO:Importing libraries
2023-04-27 14:48:13,909:INFO:Copying training dataset
2023-04-27 14:48:14,181:INFO:Defining folds
2023-04-27 14:48:14,181:INFO:Declaring metric variables
2023-04-27 14:48:14,184:INFO:Importing untrained model
2023-04-27 14:48:14,190:INFO:Least Angle Regression Imported successfully
2023-04-27 14:48:14,198:INFO:Starting cross validation
2023-04-27 14:48:14,204:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:48:15,028:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:15,147:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:15,325:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:15,384:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:15,387:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:15,409:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:15,423:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:15,773:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:15,994:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=5.215e+00, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,042:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=8.819e+00, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,108:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=4.486e+00, with an active set of 141 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,255:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=6.181e+01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,259:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-27 14:48:16,262:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.090e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,297:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.249e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,327:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 137 iterations, i.e. alpha=4.366e+00, with an active set of 136 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,345:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=7.061e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,346:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-27 14:48:16,413:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=3.979e+00, with an active set of 156 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,421:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-04-27 14:48:16,449:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 174 iterations, i.e. alpha=3.289e+00, with an active set of 172 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,454:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 184 iterations, i.e. alpha=2.867e+00, with an active set of 183 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,462:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 189 iterations, i.e. alpha=2.642e+00, with an active set of 188 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,542:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.360e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,573:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=8.616e+00, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,589:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 278 iterations, i.e. alpha=1.433e+00, with an active set of 277 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,604:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=5.979e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,605:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=5.429e+00, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,669:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 175 iterations, i.e. alpha=7.163e+00, with an active set of 137 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,692:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 330 iterations, i.e. alpha=1.128e+00, with an active set of 329 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,713:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 175 iterations, i.e. alpha=3.045e+00, with an active set of 173 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,714:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 176 iterations, i.e. alpha=2.989e+00, with an active set of 174 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,716:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 176 iterations, i.e. alpha=2.978e+00, with an active set of 174 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,725:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 202 iterations, i.e. alpha=4.226e+00, with an active set of 162 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,732:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 191 iterations, i.e. alpha=2.693e+00, with an active set of 188 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,771:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-04-27 14:48:16,799:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 243 iterations, i.e. alpha=3.524e+00, with an active set of 201 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,877:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.055e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,884:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=7.846e-01, with an active set of 411 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,908:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 381 iterations, i.e. alpha=1.565e+00, with an active set of 312 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,916:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 425 iterations, i.e. alpha=7.455e-01, with an active set of 424 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,919:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=5.770e+00, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,921:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-04-27 14:48:16,942:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 284 iterations, i.e. alpha=1.485e+00, with an active set of 281 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,946:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 436 iterations, i.e. alpha=7.140e-01, with an active set of 435 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,959:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=4.053e+00, with an active set of 145 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,961:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 296 iterations, i.e. alpha=1.426e+00, with an active set of 292 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:16,968:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 299 iterations, i.e. alpha=1.409e+00, with an active set of 295 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:17,005:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 176 iterations, i.e. alpha=3.006e+00, with an active set of 175 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:17,005:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 176 iterations, i.e. alpha=3.005e+00, with an active set of 175 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:17,023:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 321 iterations, i.e. alpha=1.265e+00, with an active set of 317 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:17,232:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 278 iterations, i.e. alpha=1.502e+00, with an active set of 277 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:17,377:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 342 iterations, i.e. alpha=1.083e+00, with an active set of 341 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:17,461:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 489 iterations, i.e. alpha=5.782e-01, with an active set of 482 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:17,525:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 421 iterations, i.e. alpha=7.461e-01, with an active set of 420 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:18,311:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:48:18,625:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:48:18,771:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:48:19,626:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:19,687:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:20,215:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.101e+01, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:20,219:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.213e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:20,257:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=5.962e+00, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:20,293:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-04-27 14:48:20,335:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-04-27 14:48:20,508:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-04-27 14:48:20,509:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-27 14:48:20,509:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:739: RuntimeWarning: overflow encountered in true_divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-04-27 14:48:20,509:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:740: RuntimeWarning: divide by zero encountered in true_divide
  gamma_ = min(g1, g2, C / AA)

2023-04-27 14:48:21,850:INFO:Calculating mean and std
2023-04-27 14:48:21,852:INFO:Creating metrics dataframe
2023-04-27 14:48:22,165:INFO:Uploading results into container
2023-04-27 14:48:22,166:INFO:Uploading model into container now
2023-04-27 14:48:22,166:INFO:_master_model_container: 5
2023-04-27 14:48:22,166:INFO:_display_container: 2
2023-04-27 14:48:22,168:INFO:Lars(random_state=123)
2023-04-27 14:48:22,168:INFO:create_model() successfully completed......................................
2023-04-27 14:48:22,436:INFO:SubProcess create_model() end ==================================
2023-04-27 14:48:22,436:INFO:Creating metrics dataframe
2023-04-27 14:48:22,456:INFO:Initializing Lasso Least Angle Regression
2023-04-27 14:48:22,456:INFO:Total runtime is 2.8840858658154804 minutes
2023-04-27 14:48:22,461:INFO:SubProcess create_model() called ==================================
2023-04-27 14:48:22,461:INFO:Initializing create_model()
2023-04-27 14:48:22,462:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:48:22,462:INFO:Checking exceptions
2023-04-27 14:48:22,462:INFO:Importing libraries
2023-04-27 14:48:22,462:INFO:Copying training dataset
2023-04-27 14:48:22,807:INFO:Defining folds
2023-04-27 14:48:22,807:INFO:Declaring metric variables
2023-04-27 14:48:22,811:INFO:Importing untrained model
2023-04-27 14:48:22,816:INFO:Lasso Least Angle Regression Imported successfully
2023-04-27 14:48:22,824:INFO:Starting cross validation
2023-04-27 14:48:22,830:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:48:23,740:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 14:48:23,911:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 14:48:24,057:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 14:48:24,076:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 14:48:24,080:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 14:48:24,081:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 14:48:24,222:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 14:48:24,309:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 14:48:24,475:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 77 iterations, alpha=1.012e+01, previous alpha=1.010e+01, with an active set of 78 regressors.
  ConvergenceWarning,

2023-04-27 14:48:24,930:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.360e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:24,967:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 105 iterations, alpha=7.002e+00, previous alpha=6.430e+00, with an active set of 104 regressors.
  ConvergenceWarning,

2023-04-27 14:48:25,002:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=1.430e+01, previous alpha=1.430e+01, with an active set of 59 regressors.
  ConvergenceWarning,

2023-04-27 14:48:25,065:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=5.215e+00, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:25,090:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=6.181e+01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:25,095:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-27 14:48:25,099:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.090e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:25,136:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.249e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:25,144:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 77 iterations, alpha=9.921e+00, previous alpha=9.907e+00, with an active set of 78 regressors.
  ConvergenceWarning,

2023-04-27 14:48:25,160:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 80 iterations, alpha=9.019e+00, previous alpha=9.010e+00, with an active set of 79 regressors.
  ConvergenceWarning,

2023-04-27 14:48:25,261:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=4.074e+00, with an active set of 141 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:25,273:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-27 14:48:25,309:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 188 iterations, i.e. alpha=2.696e+00, with an active set of 186 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:25,349:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.055e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:25,397:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=5.770e+00, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:25,399:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-04-27 14:48:25,426:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-04-27 14:48:25,432:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 146 iterations, alpha=4.079e+00, previous alpha=4.079e+00, with an active set of 145 regressors.
  ConvergenceWarning,

2023-04-27 14:48:27,413:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 14:48:27,419:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 14:48:27,950:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.101e+01, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:27,954:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.213e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:27,995:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=4.876e+00, with an active set of 123 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:28,003:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-04-27 14:48:28,011:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=4.227e+00, with an active set of 138 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:28,036:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 192 iterations, alpha=2.679e+00, previous alpha=2.679e+00, with an active set of 183 regressors.
  ConvergenceWarning,

2023-04-27 14:48:28,050:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 197 iterations, i.e. alpha=2.651e+00, with an active set of 193 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:28,076:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 234 iterations, i.e. alpha=2.114e+00, with an active set of 230 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2023-04-27 14:48:28,172:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 340 iterations, alpha=1.440e+00, previous alpha=1.236e+00, with an active set of 335 regressors.
  ConvergenceWarning,

2023-04-27 14:48:29,796:INFO:Calculating mean and std
2023-04-27 14:48:29,798:INFO:Creating metrics dataframe
2023-04-27 14:48:30,163:INFO:Uploading results into container
2023-04-27 14:48:30,164:INFO:Uploading model into container now
2023-04-27 14:48:30,165:INFO:_master_model_container: 6
2023-04-27 14:48:30,167:INFO:_display_container: 2
2023-04-27 14:48:30,167:INFO:LassoLars(random_state=123)
2023-04-27 14:48:30,168:INFO:create_model() successfully completed......................................
2023-04-27 14:48:30,475:INFO:SubProcess create_model() end ==================================
2023-04-27 14:48:30,476:INFO:Creating metrics dataframe
2023-04-27 14:48:30,492:INFO:Initializing Orthogonal Matching Pursuit
2023-04-27 14:48:30,493:INFO:Total runtime is 3.0180358131726583 minutes
2023-04-27 14:48:30,498:INFO:SubProcess create_model() called ==================================
2023-04-27 14:48:30,498:INFO:Initializing create_model()
2023-04-27 14:48:30,498:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:48:30,498:INFO:Checking exceptions
2023-04-27 14:48:30,498:INFO:Importing libraries
2023-04-27 14:48:30,498:INFO:Copying training dataset
2023-04-27 14:48:30,795:INFO:Defining folds
2023-04-27 14:48:30,795:INFO:Declaring metric variables
2023-04-27 14:48:30,800:INFO:Importing untrained model
2023-04-27 14:48:30,804:INFO:Orthogonal Matching Pursuit Imported successfully
2023-04-27 14:48:30,812:INFO:Starting cross validation
2023-04-27 14:48:30,819:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:48:31,992:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:32,052:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:32,053:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:32,061:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:32,117:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:32,122:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:32,128:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:32,178:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:34,362:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:48:35,727:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:35,747:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 14:48:37,896:INFO:Calculating mean and std
2023-04-27 14:48:37,898:INFO:Creating metrics dataframe
2023-04-27 14:48:38,175:INFO:Uploading results into container
2023-04-27 14:48:38,176:INFO:Uploading model into container now
2023-04-27 14:48:38,177:INFO:_master_model_container: 7
2023-04-27 14:48:38,177:INFO:_display_container: 2
2023-04-27 14:48:38,178:INFO:OrthogonalMatchingPursuit()
2023-04-27 14:48:38,178:INFO:create_model() successfully completed......................................
2023-04-27 14:48:38,432:INFO:SubProcess create_model() end ==================================
2023-04-27 14:48:38,432:INFO:Creating metrics dataframe
2023-04-27 14:48:38,446:INFO:Initializing Bayesian Ridge
2023-04-27 14:48:38,446:INFO:Total runtime is 3.150586207707723 minutes
2023-04-27 14:48:38,450:INFO:SubProcess create_model() called ==================================
2023-04-27 14:48:38,450:INFO:Initializing create_model()
2023-04-27 14:48:38,450:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:48:38,450:INFO:Checking exceptions
2023-04-27 14:48:38,450:INFO:Importing libraries
2023-04-27 14:48:38,450:INFO:Copying training dataset
2023-04-27 14:48:38,716:INFO:Defining folds
2023-04-27 14:48:38,716:INFO:Declaring metric variables
2023-04-27 14:48:38,721:INFO:Importing untrained model
2023-04-27 14:48:38,724:INFO:Bayesian Ridge Imported successfully
2023-04-27 14:48:38,735:INFO:Starting cross validation
2023-04-27 14:48:38,739:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:49:18,618:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:49:18,746:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:49:19,546:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:49:20,441:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:49:21,227:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:49:21,399:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:49:21,737:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:49:22,025:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:49:22,116:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-27 14:49:22,160:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:49:49,880:INFO:Calculating mean and std
2023-04-27 14:49:49,885:INFO:Creating metrics dataframe
2023-04-27 14:49:50,338:INFO:Uploading results into container
2023-04-27 14:49:50,340:INFO:Uploading model into container now
2023-04-27 14:49:50,342:INFO:_master_model_container: 8
2023-04-27 14:49:50,342:INFO:_display_container: 2
2023-04-27 14:49:50,343:INFO:BayesianRidge()
2023-04-27 14:49:50,343:INFO:create_model() successfully completed......................................
2023-04-27 14:49:50,787:INFO:SubProcess create_model() end ==================================
2023-04-27 14:49:50,787:INFO:Creating metrics dataframe
2023-04-27 14:49:50,811:INFO:Initializing Passive Aggressive Regressor
2023-04-27 14:49:50,811:INFO:Total runtime is 4.3566758592923485 minutes
2023-04-27 14:49:50,816:INFO:SubProcess create_model() called ==================================
2023-04-27 14:49:50,816:INFO:Initializing create_model()
2023-04-27 14:49:50,816:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:49:50,816:INFO:Checking exceptions
2023-04-27 14:49:50,817:INFO:Importing libraries
2023-04-27 14:49:50,817:INFO:Copying training dataset
2023-04-27 14:49:51,212:INFO:Defining folds
2023-04-27 14:49:51,212:INFO:Declaring metric variables
2023-04-27 14:49:51,218:INFO:Importing untrained model
2023-04-27 14:49:51,224:INFO:Passive Aggressive Regressor Imported successfully
2023-04-27 14:49:51,238:INFO:Starting cross validation
2023-04-27 14:49:51,245:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:49:56,146:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:49:56,507:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:49:57,104:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:50:02,822:INFO:Calculating mean and std
2023-04-27 14:50:02,837:INFO:Creating metrics dataframe
2023-04-27 14:50:03,350:INFO:Uploading results into container
2023-04-27 14:50:03,351:INFO:Uploading model into container now
2023-04-27 14:50:03,353:INFO:_master_model_container: 9
2023-04-27 14:50:03,354:INFO:_display_container: 2
2023-04-27 14:50:03,355:INFO:PassiveAggressiveRegressor(random_state=123)
2023-04-27 14:50:03,355:INFO:create_model() successfully completed......................................
2023-04-27 14:50:03,810:INFO:SubProcess create_model() end ==================================
2023-04-27 14:50:03,810:INFO:Creating metrics dataframe
2023-04-27 14:50:03,837:INFO:Initializing Huber Regressor
2023-04-27 14:50:03,837:INFO:Total runtime is 4.573773852984111 minutes
2023-04-27 14:50:03,843:INFO:SubProcess create_model() called ==================================
2023-04-27 14:50:03,844:INFO:Initializing create_model()
2023-04-27 14:50:03,844:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:50:03,844:INFO:Checking exceptions
2023-04-27 14:50:03,844:INFO:Importing libraries
2023-04-27 14:50:03,845:INFO:Copying training dataset
2023-04-27 14:50:04,303:INFO:Defining folds
2023-04-27 14:50:04,304:INFO:Declaring metric variables
2023-04-27 14:50:04,311:INFO:Importing untrained model
2023-04-27 14:50:04,319:INFO:Huber Regressor Imported successfully
2023-04-27 14:50:04,357:INFO:Starting cross validation
2023-04-27 14:50:04,366:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:50:53,179:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:51:27,051:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-04-27 14:51:36,049:INFO:Calculating mean and std
2023-04-27 14:51:36,052:INFO:Creating metrics dataframe
2023-04-27 14:51:36,317:INFO:Uploading results into container
2023-04-27 14:51:36,318:INFO:Uploading model into container now
2023-04-27 14:51:36,318:INFO:_master_model_container: 10
2023-04-27 14:51:36,319:INFO:_display_container: 2
2023-04-27 14:51:36,319:INFO:HuberRegressor()
2023-04-27 14:51:36,319:INFO:create_model() successfully completed......................................
2023-04-27 14:51:36,593:INFO:SubProcess create_model() end ==================================
2023-04-27 14:51:36,593:INFO:Creating metrics dataframe
2023-04-27 14:51:36,613:INFO:Initializing K Neighbors Regressor
2023-04-27 14:51:36,613:INFO:Total runtime is 6.120051618417104 minutes
2023-04-27 14:51:36,617:INFO:SubProcess create_model() called ==================================
2023-04-27 14:51:36,617:INFO:Initializing create_model()
2023-04-27 14:51:36,618:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:51:36,618:INFO:Checking exceptions
2023-04-27 14:51:36,618:INFO:Importing libraries
2023-04-27 14:51:36,618:INFO:Copying training dataset
2023-04-27 14:51:36,877:INFO:Defining folds
2023-04-27 14:51:36,877:INFO:Declaring metric variables
2023-04-27 14:51:36,880:INFO:Importing untrained model
2023-04-27 14:51:36,885:INFO:K Neighbors Regressor Imported successfully
2023-04-27 14:51:36,892:INFO:Starting cross validation
2023-04-27 14:51:36,899:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:51:38,701:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:51:38,817:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:51:38,966:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:51:38,981:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:51:39,136:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:51:39,218:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:51:48,918:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:51:48,938:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:51:53,338:INFO:Calculating mean and std
2023-04-27 14:51:53,342:INFO:Creating metrics dataframe
2023-04-27 14:51:53,788:INFO:Uploading results into container
2023-04-27 14:51:53,789:INFO:Uploading model into container now
2023-04-27 14:51:53,789:INFO:_master_model_container: 11
2023-04-27 14:51:53,790:INFO:_display_container: 2
2023-04-27 14:51:53,790:INFO:KNeighborsRegressor(n_jobs=-1)
2023-04-27 14:51:53,790:INFO:create_model() successfully completed......................................
2023-04-27 14:51:54,156:INFO:SubProcess create_model() end ==================================
2023-04-27 14:51:54,156:INFO:Creating metrics dataframe
2023-04-27 14:51:54,187:INFO:Initializing Decision Tree Regressor
2023-04-27 14:51:54,187:INFO:Total runtime is 6.412943450609843 minutes
2023-04-27 14:51:54,193:INFO:SubProcess create_model() called ==================================
2023-04-27 14:51:54,193:INFO:Initializing create_model()
2023-04-27 14:51:54,193:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:51:54,193:INFO:Checking exceptions
2023-04-27 14:51:54,193:INFO:Importing libraries
2023-04-27 14:51:54,194:INFO:Copying training dataset
2023-04-27 14:51:54,591:INFO:Defining folds
2023-04-27 14:51:54,591:INFO:Declaring metric variables
2023-04-27 14:51:54,597:INFO:Importing untrained model
2023-04-27 14:51:54,605:INFO:Decision Tree Regressor Imported successfully
2023-04-27 14:51:54,618:INFO:Starting cross validation
2023-04-27 14:51:54,628:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:51:58,236:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:51:58,423:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:52:02,812:INFO:Calculating mean and std
2023-04-27 14:52:02,814:INFO:Creating metrics dataframe
2023-04-27 14:52:03,212:INFO:Uploading results into container
2023-04-27 14:52:03,213:INFO:Uploading model into container now
2023-04-27 14:52:03,214:INFO:_master_model_container: 12
2023-04-27 14:52:03,214:INFO:_display_container: 2
2023-04-27 14:52:03,214:INFO:DecisionTreeRegressor(random_state=123)
2023-04-27 14:52:03,215:INFO:create_model() successfully completed......................................
2023-04-27 14:52:03,500:INFO:SubProcess create_model() end ==================================
2023-04-27 14:52:03,500:INFO:Creating metrics dataframe
2023-04-27 14:52:03,529:INFO:Initializing Random Forest Regressor
2023-04-27 14:52:03,529:INFO:Total runtime is 6.568635340531667 minutes
2023-04-27 14:52:03,534:INFO:SubProcess create_model() called ==================================
2023-04-27 14:52:03,535:INFO:Initializing create_model()
2023-04-27 14:52:03,535:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:52:03,535:INFO:Checking exceptions
2023-04-27 14:52:03,535:INFO:Importing libraries
2023-04-27 14:52:03,535:INFO:Copying training dataset
2023-04-27 14:52:03,880:INFO:Defining folds
2023-04-27 14:52:03,881:INFO:Declaring metric variables
2023-04-27 14:52:03,887:INFO:Importing untrained model
2023-04-27 14:52:03,893:INFO:Random Forest Regressor Imported successfully
2023-04-27 14:52:03,903:INFO:Starting cross validation
2023-04-27 14:52:03,912:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:52:05,445:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:236: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  **fit_params_steps.get(name, {}),

2023-04-27 14:52:59,261:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 2.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:53:00,559:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:53:00,666:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.40s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:53:01,536:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-27 14:53:01,758:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:53:01,991:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:53:02,111:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:53:02,368:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:53:02,754:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:53:03,328:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:53:19,556:INFO:Calculating mean and std
2023-04-27 14:53:19,558:INFO:Creating metrics dataframe
2023-04-27 14:53:19,951:INFO:Uploading results into container
2023-04-27 14:53:19,953:INFO:Uploading model into container now
2023-04-27 14:53:19,953:INFO:_master_model_container: 13
2023-04-27 14:53:19,953:INFO:_display_container: 2
2023-04-27 14:53:19,953:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-04-27 14:53:19,953:INFO:create_model() successfully completed......................................
2023-04-27 14:53:20,226:INFO:SubProcess create_model() end ==================================
2023-04-27 14:53:20,227:INFO:Creating metrics dataframe
2023-04-27 14:53:20,252:INFO:Initializing Extra Trees Regressor
2023-04-27 14:53:20,252:INFO:Total runtime is 7.847367389996847 minutes
2023-04-27 14:53:20,256:INFO:SubProcess create_model() called ==================================
2023-04-27 14:53:20,257:INFO:Initializing create_model()
2023-04-27 14:53:20,257:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:53:20,257:INFO:Checking exceptions
2023-04-27 14:53:20,257:INFO:Importing libraries
2023-04-27 14:53:20,257:INFO:Copying training dataset
2023-04-27 14:53:20,592:INFO:Defining folds
2023-04-27 14:53:20,592:INFO:Declaring metric variables
2023-04-27 14:53:20,596:INFO:Importing untrained model
2023-04-27 14:53:20,602:INFO:Extra Trees Regressor Imported successfully
2023-04-27 14:53:20,609:INFO:Starting cross validation
2023-04-27 14:53:20,616:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:54:55,761:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:54:55,796:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:54:55,852:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:54:59,620:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:55:02,029:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:55:02,591:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:55:37,811:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 14:55:38,811:INFO:Calculating mean and std
2023-04-27 14:55:38,814:INFO:Creating metrics dataframe
2023-04-27 14:55:39,808:INFO:Uploading results into container
2023-04-27 14:55:39,809:INFO:Uploading model into container now
2023-04-27 14:55:39,809:INFO:_master_model_container: 14
2023-04-27 14:55:39,809:INFO:_display_container: 2
2023-04-27 14:55:39,810:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-04-27 14:55:39,810:INFO:create_model() successfully completed......................................
2023-04-27 14:55:40,127:INFO:SubProcess create_model() end ==================================
2023-04-27 14:55:40,127:INFO:Creating metrics dataframe
2023-04-27 14:55:44,498:INFO:Initializing AdaBoost Regressor
2023-04-27 14:55:44,500:INFO:Total runtime is 10.251498373349508 minutes
2023-04-27 14:55:44,510:INFO:SubProcess create_model() called ==================================
2023-04-27 14:55:44,510:INFO:Initializing create_model()
2023-04-27 14:55:44,510:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:55:44,510:INFO:Checking exceptions
2023-04-27 14:55:44,511:INFO:Importing libraries
2023-04-27 14:55:44,511:INFO:Copying training dataset
2023-04-27 14:55:44,912:INFO:Defining folds
2023-04-27 14:55:44,912:INFO:Declaring metric variables
2023-04-27 14:55:44,919:INFO:Importing untrained model
2023-04-27 14:55:44,927:INFO:AdaBoost Regressor Imported successfully
2023-04-27 14:55:44,938:INFO:Starting cross validation
2023-04-27 14:55:44,948:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:56:17,404:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:56:45,426:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:56:46,208:INFO:Calculating mean and std
2023-04-27 14:56:46,227:INFO:Creating metrics dataframe
2023-04-27 14:56:48,372:INFO:Uploading results into container
2023-04-27 14:56:48,374:INFO:Uploading model into container now
2023-04-27 14:56:48,375:INFO:_master_model_container: 15
2023-04-27 14:56:48,375:INFO:_display_container: 2
2023-04-27 14:56:48,378:INFO:AdaBoostRegressor(random_state=123)
2023-04-27 14:56:48,378:INFO:create_model() successfully completed......................................
2023-04-27 14:56:48,895:INFO:SubProcess create_model() end ==================================
2023-04-27 14:56:48,896:INFO:Creating metrics dataframe
2023-04-27 14:56:48,939:INFO:Initializing Gradient Boosting Regressor
2023-04-27 14:56:48,940:INFO:Total runtime is 11.325493379433949 minutes
2023-04-27 14:56:48,949:INFO:SubProcess create_model() called ==================================
2023-04-27 14:56:48,950:INFO:Initializing create_model()
2023-04-27 14:56:48,950:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:56:48,950:INFO:Checking exceptions
2023-04-27 14:56:48,950:INFO:Importing libraries
2023-04-27 14:56:48,950:INFO:Copying training dataset
2023-04-27 14:56:49,530:INFO:Defining folds
2023-04-27 14:56:49,530:INFO:Declaring metric variables
2023-04-27 14:56:49,539:INFO:Importing untrained model
2023-04-27 14:56:49,550:INFO:Gradient Boosting Regressor Imported successfully
2023-04-27 14:56:49,576:INFO:Starting cross validation
2023-04-27 14:56:49,587:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 14:56:52,466:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 14:56:53,081:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:236: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  **fit_params_steps.get(name, {}),

2023-04-27 14:57:39,114:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:57:40,254:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:57:46,308:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 14:58:09,869:INFO:Calculating mean and std
2023-04-27 14:58:09,871:INFO:Creating metrics dataframe
2023-04-27 14:58:10,286:INFO:Uploading results into container
2023-04-27 14:58:10,287:INFO:Uploading model into container now
2023-04-27 14:58:10,288:INFO:_master_model_container: 16
2023-04-27 14:58:10,288:INFO:_display_container: 2
2023-04-27 14:58:10,288:INFO:GradientBoostingRegressor(random_state=123)
2023-04-27 14:58:10,288:INFO:create_model() successfully completed......................................
2023-04-27 14:58:10,609:INFO:SubProcess create_model() end ==================================
2023-04-27 14:58:10,609:INFO:Creating metrics dataframe
2023-04-27 14:58:10,636:INFO:Initializing Extreme Gradient Boosting
2023-04-27 14:58:10,636:INFO:Total runtime is 12.68709468046824 minutes
2023-04-27 14:58:10,641:INFO:SubProcess create_model() called ==================================
2023-04-27 14:58:10,642:INFO:Initializing create_model()
2023-04-27 14:58:10,642:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 14:58:10,642:INFO:Checking exceptions
2023-04-27 14:58:10,642:INFO:Importing libraries
2023-04-27 14:58:10,642:INFO:Copying training dataset
2023-04-27 14:58:10,924:INFO:Defining folds
2023-04-27 14:58:10,925:INFO:Declaring metric variables
2023-04-27 14:58:10,929:INFO:Importing untrained model
2023-04-27 14:58:10,935:INFO:Extreme Gradient Boosting Imported successfully
2023-04-27 14:58:10,943:INFO:Starting cross validation
2023-04-27 14:58:10,949:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 15:00:49,419:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 15:02:13,136:INFO:Calculating mean and std
2023-04-27 15:02:13,140:INFO:Creating metrics dataframe
2023-04-27 15:02:13,518:INFO:Uploading results into container
2023-04-27 15:02:13,518:INFO:Uploading model into container now
2023-04-27 15:02:13,519:INFO:_master_model_container: 17
2023-04-27 15:02:13,519:INFO:_display_container: 2
2023-04-27 15:02:13,522:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, gamma=None,
             gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, n_estimators=100, n_jobs=-1,
             num_parallel_tree=None, predictor=None, random_state=123,
             reg_alpha=None, reg_lambda=None, ...)
2023-04-27 15:02:13,523:INFO:create_model() successfully completed......................................
2023-04-27 15:02:13,843:INFO:SubProcess create_model() end ==================================
2023-04-27 15:02:13,843:INFO:Creating metrics dataframe
2023-04-27 15:02:13,861:INFO:Initializing Light Gradient Boosting Machine
2023-04-27 15:02:13,861:INFO:Total runtime is 16.74085091749827 minutes
2023-04-27 15:02:13,866:INFO:SubProcess create_model() called ==================================
2023-04-27 15:02:13,866:INFO:Initializing create_model()
2023-04-27 15:02:13,866:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 15:02:13,866:INFO:Checking exceptions
2023-04-27 15:02:13,866:INFO:Importing libraries
2023-04-27 15:02:13,866:INFO:Copying training dataset
2023-04-27 15:02:14,140:INFO:Defining folds
2023-04-27 15:02:14,140:INFO:Declaring metric variables
2023-04-27 15:02:14,144:INFO:Importing untrained model
2023-04-27 15:02:14,149:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-27 15:02:14,158:INFO:Starting cross validation
2023-04-27 15:02:14,164:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 15:02:17,708:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 15:02:17,816:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 15:02:18,091:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 15:02:18,128:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 15:02:23,498:INFO:Calculating mean and std
2023-04-27 15:02:23,500:INFO:Creating metrics dataframe
2023-04-27 15:02:23,941:INFO:Uploading results into container
2023-04-27 15:02:23,941:INFO:Uploading model into container now
2023-04-27 15:02:23,942:INFO:_master_model_container: 18
2023-04-27 15:02:23,942:INFO:_display_container: 2
2023-04-27 15:02:23,942:INFO:LGBMRegressor(random_state=123)
2023-04-27 15:02:23,942:INFO:create_model() successfully completed......................................
2023-04-27 15:02:24,213:INFO:SubProcess create_model() end ==================================
2023-04-27 15:02:24,213:INFO:Creating metrics dataframe
2023-04-27 15:02:24,236:INFO:Initializing Dummy Regressor
2023-04-27 15:02:24,236:INFO:Total runtime is 16.91376761198044 minutes
2023-04-27 15:02:24,240:INFO:SubProcess create_model() called ==================================
2023-04-27 15:02:24,240:INFO:Initializing create_model()
2023-04-27 15:02:24,240:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052853E448>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 15:02:24,240:INFO:Checking exceptions
2023-04-27 15:02:24,240:INFO:Importing libraries
2023-04-27 15:02:24,240:INFO:Copying training dataset
2023-04-27 15:02:24,525:INFO:Defining folds
2023-04-27 15:02:24,525:INFO:Declaring metric variables
2023-04-27 15:02:24,529:INFO:Importing untrained model
2023-04-27 15:02:24,535:INFO:Dummy Regressor Imported successfully
2023-04-27 15:02:24,544:INFO:Starting cross validation
2023-04-27 15:02:24,552:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 15:02:30,375:INFO:Calculating mean and std
2023-04-27 15:02:30,377:INFO:Creating metrics dataframe
2023-04-27 15:02:30,823:INFO:Uploading results into container
2023-04-27 15:02:30,824:INFO:Uploading model into container now
2023-04-27 15:02:30,824:INFO:_master_model_container: 19
2023-04-27 15:02:30,824:INFO:_display_container: 2
2023-04-27 15:02:30,825:INFO:DummyRegressor()
2023-04-27 15:02:30,825:INFO:create_model() successfully completed......................................
2023-04-27 15:02:31,137:INFO:SubProcess create_model() end ==================================
2023-04-27 15:02:31,138:INFO:Creating metrics dataframe
2023-04-27 15:02:31,185:INFO:Initializing create_model()
2023-04-27 15:02:31,185:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, gamma=None,
             gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, n_estimators=100, n_jobs=-1,
             num_parallel_tree=None, predictor=None, random_state=123,
             reg_alpha=None, reg_lambda=None, ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-27 15:02:31,185:INFO:Checking exceptions
2023-04-27 15:02:31,194:INFO:Importing libraries
2023-04-27 15:02:31,194:INFO:Copying training dataset
2023-04-27 15:02:31,526:INFO:Defining folds
2023-04-27 15:02:31,526:INFO:Declaring metric variables
2023-04-27 15:02:31,527:INFO:Importing untrained model
2023-04-27 15:02:31,527:INFO:Declaring custom model
2023-04-27 15:02:31,529:INFO:Extreme Gradient Boosting Imported successfully
2023-04-27 15:02:31,539:INFO:Cross validation set to False
2023-04-27 15:02:31,539:INFO:Fitting Model
2023-04-27 15:02:55,514:INFO:XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100,
             n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0, reg_lambda=1, ...)
2023-04-27 15:02:55,514:INFO:create_model() successfully completed......................................
2023-04-27 15:02:55,795:INFO:Creating Dashboard logs
2023-04-27 15:02:55,802:INFO:Model: Extreme Gradient Boosting
2023-04-27 15:02:55,857:INFO:Logged params: {'objective': 'reg:squarederror', 'base_score': 0.5, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'gamma': 0, 'gpu_id': -1, 'grow_policy': 'depthwise', 'importance_type': None, 'interaction_constraints': '', 'learning_rate': 0.300000012, 'max_bin': 256, 'max_cat_to_onehot': 4, 'max_delta_step': 0, 'max_depth': 6, 'max_leaves': 0, 'min_child_weight': 1, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 100, 'n_jobs': -1, 'num_parallel_tree': 1, 'predictor': 'auto', 'random_state': 123, 'reg_alpha': 0, 'reg_lambda': 1, 'sampling_method': 'uniform', 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'auto', 'validate_parameters': 1, 'verbosity': 0}
2023-04-27 15:02:55,955:INFO:Initializing predict_model()
2023-04-27 15:02:55,955:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100,
             n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0, reg_lambda=1, ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000020532B85CA8>)
2023-04-27 15:02:55,955:INFO:Checking exceptions
2023-04-27 15:02:55,955:INFO:Preloading libraries
2023-04-27 15:02:57,768:INFO:Creating Dashboard logs
2023-04-27 15:02:57,773:INFO:Model: Extra Trees Regressor
2023-04-27 15:02:57,807:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2023-04-27 15:02:58,480:INFO:Creating Dashboard logs
2023-04-27 15:02:58,484:INFO:Model: Random Forest Regressor
2023-04-27 15:02:58,511:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2023-04-27 15:02:59,101:INFO:Creating Dashboard logs
2023-04-27 15:02:59,104:INFO:Model: Bayesian Ridge
2023-04-27 15:02:59,132:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'normalize': 'deprecated', 'tol': 0.001, 'verbose': False}
2023-04-27 15:02:59,762:INFO:Creating Dashboard logs
2023-04-27 15:02:59,768:INFO:Model: Ridge Regression
2023-04-27 15:02:59,798:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': 'deprecated', 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.001}
2023-04-27 15:03:00,749:INFO:Creating Dashboard logs
2023-04-27 15:03:00,756:INFO:Model: Gradient Boosting Regressor
2023-04-27 15:03:00,804:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-04-27 15:03:01,519:INFO:Creating Dashboard logs
2023-04-27 15:03:01,523:INFO:Model: Light Gradient Boosting Machine
2023-04-27 15:03:01,552:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2023-04-27 15:03:02,211:INFO:Creating Dashboard logs
2023-04-27 15:03:02,215:INFO:Model: Linear Regression
2023-04-27 15:03:02,242:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'normalize': 'deprecated', 'positive': False}
2023-04-27 15:03:02,827:INFO:Creating Dashboard logs
2023-04-27 15:03:02,832:INFO:Model: Lasso Regression
2023-04-27 15:03:02,858:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': 'deprecated', 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2023-04-27 15:03:03,693:INFO:Creating Dashboard logs
2023-04-27 15:03:03,701:INFO:Model: Decision Tree Regressor
2023-04-27 15:03:03,742:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 123, 'splitter': 'best'}
2023-04-27 15:03:04,703:INFO:Creating Dashboard logs
2023-04-27 15:03:04,707:INFO:Model: Orthogonal Matching Pursuit
2023-04-27 15:03:04,740:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'normalize': 'deprecated', 'precompute': 'auto', 'tol': None}
2023-04-27 15:03:05,478:INFO:Creating Dashboard logs
2023-04-27 15:03:05,484:INFO:Model: Lasso Least Angle Regression
2023-04-27 15:03:05,524:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'normalize': 'deprecated', 'positive': False, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2023-04-27 15:03:06,241:INFO:Creating Dashboard logs
2023-04-27 15:03:06,245:INFO:Model: AdaBoost Regressor
2023-04-27 15:03:06,275:INFO:Logged params: {'base_estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 123}
2023-04-27 15:03:06,897:INFO:Creating Dashboard logs
2023-04-27 15:03:06,902:INFO:Model: K Neighbors Regressor
2023-04-27 15:03:06,930:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2023-04-27 15:03:07,867:INFO:Creating Dashboard logs
2023-04-27 15:03:07,872:INFO:Model: Elastic Net
2023-04-27 15:03:07,923:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'normalize': 'deprecated', 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2023-04-27 15:03:08,653:INFO:Creating Dashboard logs
2023-04-27 15:03:08,658:INFO:Model: Huber Regressor
2023-04-27 15:03:08,689:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2023-04-27 15:03:09,294:INFO:Creating Dashboard logs
2023-04-27 15:03:09,297:INFO:Model: Dummy Regressor
2023-04-27 15:03:09,325:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2023-04-27 15:03:09,966:INFO:Creating Dashboard logs
2023-04-27 15:03:09,971:INFO:Model: Passive Aggressive Regressor
2023-04-27 15:03:10,003:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-04-27 15:03:10,727:INFO:Creating Dashboard logs
2023-04-27 15:03:10,734:INFO:Model: Least Angle Regression
2023-04-27 15:03:10,778:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'normalize': 'deprecated', 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2023-04-27 15:03:11,696:INFO:_master_model_container: 19
2023-04-27 15:03:11,696:INFO:_display_container: 2
2023-04-27 15:03:11,700:INFO:XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100,
             n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0, reg_lambda=1, ...)
2023-04-27 15:03:11,700:INFO:compare_models() successfully completed......................................
2023-04-27 15:17:48,027:INFO:gpu_param set to False
2023-04-27 15:17:48,455:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 15:17:48,460:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 15:17:48,891:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 15:17:48,896:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 15:19:05,318:INFO:Initializing tune_model()
2023-04-27 15:19:05,318:INFO:tune_model(estimator=xgboost, fold=None, round=4, n_iter=10, custom_grid=None, optimize=R2, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>)
2023-04-27 15:19:05,318:INFO:Checking exceptions
2023-04-27 15:19:16,299:INFO:Initializing tune_model()
2023-04-27 15:19:16,299:INFO:tune_model(estimator=XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100,
             n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0, reg_lambda=1, ...), fold=None, round=4, n_iter=10, custom_grid=None, optimize=R2, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>)
2023-04-27 15:19:16,299:INFO:Checking exceptions
2023-04-27 15:19:16,474:INFO:Copying training dataset
2023-04-27 15:19:16,711:INFO:Checking base model
2023-04-27 15:19:16,712:INFO:Base model : Extreme Gradient Boosting
2023-04-27 15:19:16,719:INFO:Declaring metric variables
2023-04-27 15:19:16,722:INFO:Defining Hyperparameters
2023-04-27 15:19:17,013:INFO:Tuning with n_jobs=-1
2023-04-27 15:19:17,013:INFO:Initializing RandomizedSearchCV
2023-04-27 15:20:09,690:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 15:21:01,111:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 15:43:21,290:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 15:47:29,636:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:06:04,587:INFO:best_params: {'actual_estimator__subsample': 1, 'actual_estimator__scale_pos_weight': 47.900000000000006, 'actual_estimator__reg_lambda': 4, 'actual_estimator__reg_alpha': 0.3, 'actual_estimator__n_estimators': 70, 'actual_estimator__min_child_weight': 1, 'actual_estimator__max_depth': 5, 'actual_estimator__learning_rate': 0.3, 'actual_estimator__colsample_bytree': 0.5}
2023-04-27 16:06:04,652:INFO:Hyperparameter search completed
2023-04-27 16:06:04,666:INFO:SubProcess create_model() called ==================================
2023-04-27 16:06:04,690:INFO:Initializing create_model()
2023-04-27 16:06:04,690:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100,
             n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0, reg_lambda=1, ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002052852CD48>, model_only=True, return_train_score=False, kwargs={'subsample': 1, 'scale_pos_weight': 47.900000000000006, 'reg_lambda': 4, 'reg_alpha': 0.3, 'n_estimators': 70, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.3, 'colsample_bytree': 0.5})
2023-04-27 16:06:04,691:INFO:Checking exceptions
2023-04-27 16:06:04,696:INFO:Importing libraries
2023-04-27 16:06:04,697:INFO:Copying training dataset
2023-04-27 16:06:05,109:INFO:Defining folds
2023-04-27 16:06:05,111:INFO:Declaring metric variables
2023-04-27 16:06:05,161:INFO:Importing untrained model
2023-04-27 16:06:05,162:INFO:Declaring custom model
2023-04-27 16:06:05,192:INFO:Extreme Gradient Boosting Imported successfully
2023-04-27 16:06:05,205:INFO:Starting cross validation
2023-04-27 16:06:05,219:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:06:11,549:INFO:Calculating mean and std
2023-04-27 16:06:11,551:INFO:Creating metrics dataframe
2023-04-27 16:06:11,569:INFO:Finalizing model
2023-04-27 16:06:21,112:INFO:Uploading results into container
2023-04-27 16:06:21,113:INFO:Uploading model into container now
2023-04-27 16:06:21,118:INFO:_master_model_container: 20
2023-04-27 16:06:21,118:INFO:_display_container: 3
2023-04-27 16:06:21,121:INFO:XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.5,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=70, n_jobs=-1,
             num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0.3, reg_lambda=4, ...)
2023-04-27 16:06:21,121:INFO:create_model() successfully completed......................................
2023-04-27 16:06:23,410:INFO:SubProcess create_model() end ==================================
2023-04-27 16:06:23,410:INFO:choose_better activated
2023-04-27 16:06:23,415:INFO:SubProcess create_model() called ==================================
2023-04-27 16:06:23,419:INFO:Initializing create_model()
2023-04-27 16:06:23,419:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100,
             n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0, reg_lambda=1, ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:06:23,419:INFO:Checking exceptions
2023-04-27 16:06:23,423:INFO:Importing libraries
2023-04-27 16:06:23,423:INFO:Copying training dataset
2023-04-27 16:06:23,736:INFO:Defining folds
2023-04-27 16:06:23,736:INFO:Declaring metric variables
2023-04-27 16:06:23,736:INFO:Importing untrained model
2023-04-27 16:06:23,736:INFO:Declaring custom model
2023-04-27 16:06:23,739:INFO:Extreme Gradient Boosting Imported successfully
2023-04-27 16:06:23,739:INFO:Starting cross validation
2023-04-27 16:06:23,745:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:09:54,631:INFO:Calculating mean and std
2023-04-27 16:09:54,632:INFO:Creating metrics dataframe
2023-04-27 16:09:54,636:INFO:Finalizing model
2023-04-27 16:10:15,511:INFO:Uploading results into container
2023-04-27 16:10:15,512:INFO:Uploading model into container now
2023-04-27 16:10:15,512:INFO:_master_model_container: 21
2023-04-27 16:10:15,512:INFO:_display_container: 4
2023-04-27 16:10:15,515:INFO:XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100,
             n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0, reg_lambda=1, ...)
2023-04-27 16:10:15,515:INFO:create_model() successfully completed......................................
2023-04-27 16:10:15,812:INFO:SubProcess create_model() end ==================================
2023-04-27 16:10:15,816:INFO:XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100,
             n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0, reg_lambda=1, ...) result for R2 is 0.9545
2023-04-27 16:10:15,819:INFO:XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.5,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=70, n_jobs=-1,
             num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0.3, reg_lambda=4, ...) result for R2 is 0.9483
2023-04-27 16:10:15,821:INFO:XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100,
             n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0, reg_lambda=1, ...) is best model
2023-04-27 16:10:15,821:INFO:choose_better completed
2023-04-27 16:10:15,822:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-27 16:10:15,822:INFO:Creating Dashboard logs
2023-04-27 16:10:15,829:INFO:Model: Extreme Gradient Boosting
2023-04-27 16:10:15,886:INFO:Logged params: {'objective': 'reg:squarederror', 'base_score': 0.5, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'gamma': 0, 'gpu_id': -1, 'grow_policy': 'depthwise', 'importance_type': None, 'interaction_constraints': '', 'learning_rate': 0.300000012, 'max_bin': 256, 'max_cat_to_onehot': 4, 'max_delta_step': 0, 'max_depth': 6, 'max_leaves': 0, 'min_child_weight': 1, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 100, 'n_jobs': -1, 'num_parallel_tree': 1, 'predictor': 'auto', 'random_state': 123, 'reg_alpha': 0, 'reg_lambda': 1, 'sampling_method': 'uniform', 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'auto', 'validate_parameters': 1, 'verbosity': 0}
2023-04-27 16:10:15,986:INFO:Initializing predict_model()
2023-04-27 16:10:15,986:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100,
             n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0, reg_lambda=1, ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x000002054CF02A68>)
2023-04-27 16:10:15,986:INFO:Checking exceptions
2023-04-27 16:10:15,986:INFO:Preloading libraries
2023-04-27 16:10:17,906:INFO:_master_model_container: 21
2023-04-27 16:10:17,906:INFO:_display_container: 3
2023-04-27 16:10:17,909:INFO:XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100,
             n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=123,
             reg_alpha=0, reg_lambda=1, ...)
2023-04-27 16:10:17,909:INFO:tune_model() successfully completed......................................
2023-04-27 16:21:48,035:INFO:Initializing compare_models()
2023-04-27 16:21:48,035:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=1, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': 1, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-04-27 16:21:48,035:INFO:Checking exceptions
2023-04-27 16:21:48,215:INFO:Preparing display monitor
2023-04-27 16:21:48,273:INFO:Initializing Linear Regression
2023-04-27 16:21:48,274:INFO:Total runtime is 1.6729036966959637e-05 minutes
2023-04-27 16:21:48,279:INFO:SubProcess create_model() called ==================================
2023-04-27 16:21:48,280:INFO:Initializing create_model()
2023-04-27 16:21:48,280:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525C55748>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:21:48,281:INFO:Checking exceptions
2023-04-27 16:21:48,281:INFO:Importing libraries
2023-04-27 16:21:48,281:INFO:Copying training dataset
2023-04-27 16:21:48,582:INFO:Defining folds
2023-04-27 16:21:48,582:INFO:Declaring metric variables
2023-04-27 16:21:48,587:INFO:Importing untrained model
2023-04-27 16:21:48,591:INFO:Linear Regression Imported successfully
2023-04-27 16:21:48,601:INFO:Starting cross validation
2023-04-27 16:21:48,608:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:22:02,685:INFO:Calculating mean and std
2023-04-27 16:22:02,689:INFO:Creating metrics dataframe
2023-04-27 16:22:03,212:INFO:Uploading results into container
2023-04-27 16:22:03,213:INFO:Uploading model into container now
2023-04-27 16:22:03,213:INFO:_master_model_container: 22
2023-04-27 16:22:03,213:INFO:_display_container: 4
2023-04-27 16:22:03,213:INFO:LinearRegression(n_jobs=-1)
2023-04-27 16:22:03,214:INFO:create_model() successfully completed......................................
2023-04-27 16:22:03,512:INFO:SubProcess create_model() end ==================================
2023-04-27 16:22:03,512:INFO:Creating metrics dataframe
2023-04-27 16:22:03,528:INFO:Initializing Lasso Regression
2023-04-27 16:22:03,528:INFO:Total runtime is 0.2542500257492066 minutes
2023-04-27 16:22:03,531:INFO:SubProcess create_model() called ==================================
2023-04-27 16:22:03,532:INFO:Initializing create_model()
2023-04-27 16:22:03,532:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525C55748>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:22:03,532:INFO:Checking exceptions
2023-04-27 16:22:03,532:INFO:Importing libraries
2023-04-27 16:22:03,532:INFO:Copying training dataset
2023-04-27 16:22:03,802:INFO:Defining folds
2023-04-27 16:22:03,802:INFO:Declaring metric variables
2023-04-27 16:22:03,807:INFO:Importing untrained model
2023-04-27 16:22:03,813:INFO:Lasso Regression Imported successfully
2023-04-27 16:22:03,825:INFO:Starting cross validation
2023-04-27 16:22:03,831:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:22:35,011:INFO:Initializing compare_models()
2023-04-27 16:22:35,012:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-04-27 16:22:35,012:INFO:Checking exceptions
2023-04-27 16:22:35,134:INFO:Preparing display monitor
2023-04-27 16:22:35,191:INFO:Initializing Linear Regression
2023-04-27 16:22:35,191:INFO:Total runtime is 0.0 minutes
2023-04-27 16:22:35,198:INFO:SubProcess create_model() called ==================================
2023-04-27 16:22:35,199:INFO:Initializing create_model()
2023-04-27 16:22:35,199:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020549283D48>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020534EDD588>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:22:35,199:INFO:Checking exceptions
2023-04-27 16:22:35,200:INFO:Importing libraries
2023-04-27 16:22:35,200:INFO:Copying training dataset
2023-04-27 16:22:35,549:INFO:Defining folds
2023-04-27 16:22:35,549:INFO:Declaring metric variables
2023-04-27 16:22:35,553:INFO:Importing untrained model
2023-04-27 16:22:35,557:INFO:Linear Regression Imported successfully
2023-04-27 16:22:35,566:INFO:Starting cross validation
2023-04-27 16:22:35,571:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:25:04,987:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\seaborn\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)

2023-04-27 16:29:31,370:INFO:PyCaret RegressionExperiment
2023-04-27 16:29:31,370:INFO:Logging name: price2
2023-04-27 16:29:31,370:INFO:ML Usecase: MLUsecase.REGRESSION
2023-04-27 16:29:31,370:INFO:version 3.0.0
2023-04-27 16:29:31,370:INFO:Initializing setup()
2023-04-27 16:29:31,370:INFO:self.USI: 4e95
2023-04-27 16:29:31,370:INFO:self._variable_keys: {'exp_id', 'X_train', 'X_test', 'idx', 'fold_generator', 'fold_shuffle_param', '_ml_usecase', 'logging_param', 'gpu_n_jobs_param', 'USI', 'exp_name_log', 'X', 'y_test', 'seed', 'n_jobs_param', 'log_plots_param', '_available_plots', 'pipeline', 'html_param', 'memory', 'gpu_param', 'y', 'fold_groups_param', 'target_param', 'y_train', 'data', 'transform_target_param'}
2023-04-27 16:29:31,370:INFO:Checking environment
2023-04-27 16:29:31,370:INFO:python_version: 3.7.4
2023-04-27 16:29:31,370:INFO:python_build: ('tags/v3.7.4:e09359112e', 'Jul  8 2019 20:34:20')
2023-04-27 16:29:31,371:INFO:machine: AMD64
2023-04-27 16:29:31,371:INFO:platform: Windows-10-10.0.19041-SP0
2023-04-27 16:29:31,371:INFO:Memory: svmem(total=16907886592, available=4311302144, percent=74.5, used=12596584448, free=4311302144)
2023-04-27 16:29:31,371:INFO:Physical Core: 4
2023-04-27 16:29:31,371:INFO:Logical Core: 8
2023-04-27 16:29:31,371:INFO:Checking libraries
2023-04-27 16:29:31,371:INFO:System:
2023-04-27 16:29:31,372:INFO:    python: 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]
2023-04-27 16:29:31,372:INFO:executable: c:\Users\tonim\AppData\Local\Programs\Python\Python37\python.exe
2023-04-27 16:29:31,372:INFO:   machine: Windows-10-10.0.19041-SP0
2023-04-27 16:29:31,372:INFO:PyCaret required dependencies:
2023-04-27 16:29:31,372:INFO:                 pip: 22.2.2
2023-04-27 16:29:31,372:INFO:          setuptools: 65.3.0
2023-04-27 16:29:31,372:INFO:             pycaret: 3.0.0
2023-04-27 16:29:31,372:INFO:             IPython: 7.34.0
2023-04-27 16:29:31,372:INFO:          ipywidgets: 7.7.1
2023-04-27 16:29:31,372:INFO:                tqdm: 4.64.0
2023-04-27 16:29:31,372:INFO:               numpy: 1.21.6
2023-04-27 16:29:31,372:INFO:              pandas: 1.3.5
2023-04-27 16:29:31,372:INFO:              jinja2: 3.1.2
2023-04-27 16:29:31,372:INFO:               scipy: 1.5.4
2023-04-27 16:29:31,373:INFO:              joblib: 1.2.0
2023-04-27 16:29:31,373:INFO:             sklearn: 1.0.2
2023-04-27 16:29:31,373:INFO:                pyod: 1.0.9
2023-04-27 16:29:31,373:INFO:            imblearn: 0.9.0
2023-04-27 16:29:31,373:INFO:   category_encoders: 2.6.0
2023-04-27 16:29:31,373:INFO:            lightgbm: 3.3.5
2023-04-27 16:29:31,373:INFO:               numba: 0.56.0
2023-04-27 16:29:31,373:INFO:            requests: 2.28.1
2023-04-27 16:29:31,373:INFO:          matplotlib: 3.5.2
2023-04-27 16:29:31,373:INFO:          scikitplot: 0.3.7
2023-04-27 16:29:31,373:INFO:         yellowbrick: 1.5
2023-04-27 16:29:31,373:INFO:              plotly: 5.9.0
2023-04-27 16:29:31,373:INFO:             kaleido: 0.2.1
2023-04-27 16:29:31,373:INFO:         statsmodels: 0.13.2
2023-04-27 16:29:31,373:INFO:              sktime: 0.17.1
2023-04-27 16:29:31,373:INFO:               tbats: 1.1.3
2023-04-27 16:29:31,373:INFO:            pmdarima: 2.0.1
2023-04-27 16:29:31,373:INFO:              psutil: 5.9.1
2023-04-27 16:29:31,373:INFO:PyCaret optional dependencies:
2023-04-27 16:29:31,373:INFO:                shap: 0.41.0
2023-04-27 16:29:31,373:INFO:           interpret: Not installed
2023-04-27 16:29:31,373:INFO:                umap: Not installed
2023-04-27 16:29:31,373:INFO:    pandas_profiling: 3.3.0
2023-04-27 16:29:31,374:INFO:  explainerdashboard: Not installed
2023-04-27 16:29:31,374:INFO:             autoviz: Not installed
2023-04-27 16:29:31,374:INFO:           fairlearn: Not installed
2023-04-27 16:29:31,374:INFO:             xgboost: 1.6.2
2023-04-27 16:29:31,374:INFO:            catboost: Not installed
2023-04-27 16:29:31,374:INFO:              kmodes: Not installed
2023-04-27 16:29:31,374:INFO:             mlxtend: Not installed
2023-04-27 16:29:31,374:INFO:       statsforecast: Not installed
2023-04-27 16:29:31,374:INFO:        tune_sklearn: Not installed
2023-04-27 16:29:31,374:INFO:                 ray: Not installed
2023-04-27 16:29:31,374:INFO:            hyperopt: 0.2.7
2023-04-27 16:29:31,374:INFO:              optuna: Not installed
2023-04-27 16:29:31,374:INFO:               skopt: Not installed
2023-04-27 16:29:31,374:INFO:              mlflow: 1.30.1
2023-04-27 16:29:31,374:INFO:              gradio: Not installed
2023-04-27 16:29:31,374:INFO:             fastapi: Not installed
2023-04-27 16:29:31,374:INFO:             uvicorn: Not installed
2023-04-27 16:29:31,374:INFO:              m2cgen: Not installed
2023-04-27 16:29:31,374:INFO:           evidently: Not installed
2023-04-27 16:29:31,374:INFO:               fugue: Not installed
2023-04-27 16:29:31,374:INFO:           streamlit: 1.11.0
2023-04-27 16:29:31,374:INFO:             prophet: Not installed
2023-04-27 16:29:31,374:INFO:None
2023-04-27 16:29:31,375:INFO:Set up data.
2023-04-27 16:29:31,548:INFO:Set up train/test split.
2023-04-27 16:29:31,642:INFO:Set up index.
2023-04-27 16:29:31,645:INFO:Set up folding strategy.
2023-04-27 16:29:31,645:INFO:Assigning column types.
2023-04-27 16:29:31,721:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-27 16:29:31,722:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-27 16:29:31,727:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-27 16:29:31,732:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 16:29:31,894:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 16:29:31,944:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 16:29:31,945:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:31,947:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:31,948:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-27 16:29:31,953:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-27 16:29:31,958:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,120:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,181:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,182:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:32,186:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:32,186:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-04-27 16:29:32,196:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,203:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,363:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,410:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,411:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:32,414:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:32,420:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,424:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,582:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,625:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,626:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:32,630:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:32,630:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-04-27 16:29:32,640:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,810:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,856:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 16:29:32,857:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:32,859:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:32,870:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-27 16:29:33,038:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 16:29:33,083:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 16:29:33,084:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:33,086:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:33,087:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-04-27 16:29:33,266:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 16:29:33,320:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 16:29:33,320:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:33,323:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:33,500:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 16:29:33,563:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-27 16:29:33,564:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:33,566:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:33,567:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-27 16:29:33,742:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 16:29:33,791:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:33,794:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:33,966:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-27 16:29:34,015:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:34,018:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:34,018:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-04-27 16:29:34,251:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:34,255:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:34,474:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:34,480:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:34,482:INFO:Preparing preprocessing pipeline...
2023-04-27 16:29:34,482:INFO:Set up simple imputation.
2023-04-27 16:29:34,492:INFO:Set up column name cleaning.
2023-04-27 16:29:34,930:INFO:Finished creating preprocessing pipeline.
2023-04-27 16:29:34,940:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power', 'city_24 Pargana',
                                             'city_Abohar', 'city_Adalaj',
                                             'city_Adoni', 'city_Adyar',
                                             'city_Agra', 'city_Ahmedabad',
                                             'city_Ahmednagar', 'city_Ajmer',
                                             'city_Akot', 'city_Alappuzha',
                                             'city_Alibag', 'city_A...
                                             'city_Ambala', 'city_Ambikapur',
                                             'city_Amraoti', 'city_Amravati',
                                             'city_Amritsar', 'city_Anand',
                                             'city_Anantapur', 'city_Anantnag',
                                             'city_Anekal', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-04-27 16:29:34,940:INFO:Creating final display dataframe.
2023-04-27 16:29:36,023:INFO:Setup _display_container:                     Description         Value
0                    Session id           124
1                        Target         price
2                   Target type    Regression
3           Original data shape  (26118, 471)
4        Transformed data shape  (26118, 471)
5   Transformed train set shape  (18282, 471)
6    Transformed test set shape   (7836, 471)
7              Numeric features           470
8                    Preprocess          True
9               Imputation type        simple
10           Numeric imputation          mean
11       Categorical imputation          mode
12               Fold Generator         KFold
13                  Fold Number            10
14                     CPU Jobs            -1
15                      Use GPU         False
16               Log Experiment  MlflowLogger
17              Experiment Name        price2
18                          USI          4e95
2023-04-27 16:29:36,388:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:36,391:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:36,607:INFO:Soft dependency imported: xgboost: 1.6.2
2023-04-27 16:29:36,610:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-27 16:29:36,611:INFO:Logging experiment in loggers
2023-04-27 16:29:36,687:INFO:SubProcess save_model() called ==================================
2023-04-27 16:29:36,707:INFO:Initializing save_model()
2023-04-27 16:29:36,707:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power', 'city_24 Pargana',
                                             'city_Abohar', 'city_Adalaj',
                                             'city_Adoni', 'city_Adyar',
                                             'city_Agra', 'city_Ahmedabad',
                                             'city_Ahmednagar', 'city_Ajmer',
                                             'city_Akot', 'city_Alappuzha',
                                             'city_Alibag', 'city_A...
                                             'city_Ambala', 'city_Ambikapur',
                                             'city_Amraoti', 'city_Amravati',
                                             'city_Amritsar', 'city_Anand',
                                             'city_Anantapur', 'city_Anantnag',
                                             'city_Anekal', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))]), model_name=C:\Users\tonim\AppData\Local\Temp\tmpo_6_na2f\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power', 'city_24 Pargana',
                                             'city_Abohar', 'city_Adalaj',
                                             'city_Adoni', 'city_Adyar',
                                             'city_Agra', 'city_Ahmedabad',
                                             'city_Ahmednagar', 'city_Ajmer',
                                             'city_Akot', 'city_Alappuzha',
                                             'city_Alibag', 'city_A...
                                             'city_Ambala', 'city_Ambikapur',
                                             'city_Amraoti', 'city_Amravati',
                                             'city_Amritsar', 'city_Anand',
                                             'city_Anantapur', 'city_Anantnag',
                                             'city_Anekal', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2023-04-27 16:29:36,707:INFO:Adding model into prep_pipe
2023-04-27 16:29:36,711:WARNING:Only Model saved as it was a pipeline.
2023-04-27 16:29:36,722:INFO:C:\Users\tonim\AppData\Local\Temp\tmpo_6_na2f\Transformation Pipeline.pkl saved in current working directory
2023-04-27 16:29:36,729:INFO:Pipeline(memory=FastMemory(location=C:\Users\tonim\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['kms_driven', 'owner', 'age',
                                             'power', 'city_24 Pargana',
                                             'city_Abohar', 'city_Adalaj',
                                             'city_Adoni', 'city_Adyar',
                                             'city_Agra', 'city_Ahmedabad',
                                             'city_Ahmednagar', 'city_Ajmer',
                                             'city_Akot', 'city_Alappuzha',
                                             'city_Alibag', 'city_A...
                                             'city_Ambala', 'city_Ambikapur',
                                             'city_Amraoti', 'city_Amravati',
                                             'city_Amritsar', 'city_Anand',
                                             'city_Anantapur', 'city_Anantnag',
                                             'city_Anekal', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-04-27 16:29:36,729:INFO:save_model() successfully completed......................................
2023-04-27 16:29:38,374:INFO:SubProcess save_model() end ==================================
2023-04-27 16:29:38,386:INFO:setup() successfully completed in 5.65s...............
2023-04-27 16:30:01,456:INFO:Initializing compare_models()
2023-04-27 16:30:01,457:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-04-27 16:30:01,457:INFO:Checking exceptions
2023-04-27 16:30:01,508:INFO:Preparing display monitor
2023-04-27 16:30:01,553:INFO:Initializing Linear Regression
2023-04-27 16:30:01,554:INFO:Total runtime is 1.5163421630859375e-05 minutes
2023-04-27 16:30:01,559:INFO:SubProcess create_model() called ==================================
2023-04-27 16:30:01,559:INFO:Initializing create_model()
2023-04-27 16:30:01,559:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:30:01,560:INFO:Checking exceptions
2023-04-27 16:30:01,560:INFO:Importing libraries
2023-04-27 16:30:01,560:INFO:Copying training dataset
2023-04-27 16:30:01,717:INFO:Defining folds
2023-04-27 16:30:01,717:INFO:Declaring metric variables
2023-04-27 16:30:01,723:INFO:Importing untrained model
2023-04-27 16:30:01,729:INFO:Linear Regression Imported successfully
2023-04-27 16:30:01,737:INFO:Starting cross validation
2023-04-27 16:30:01,741:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:30:13,042:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:236: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  **fit_params_steps.get(name, {}),

2023-04-27 16:30:17,785:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:30:17,821:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:30:17,931:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:30:18,063:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:30:18,340:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:30:19,070:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:30:29,992:INFO:Calculating mean and std
2023-04-27 16:30:29,995:INFO:Creating metrics dataframe
2023-04-27 16:30:30,577:INFO:Uploading results into container
2023-04-27 16:30:30,577:INFO:Uploading model into container now
2023-04-27 16:30:30,578:INFO:_master_model_container: 1
2023-04-27 16:30:30,578:INFO:_display_container: 2
2023-04-27 16:30:30,579:INFO:LinearRegression(n_jobs=-1)
2023-04-27 16:30:30,579:INFO:create_model() successfully completed......................................
2023-04-27 16:30:30,925:INFO:SubProcess create_model() end ==================================
2023-04-27 16:30:30,925:INFO:Creating metrics dataframe
2023-04-27 16:30:30,937:INFO:Initializing Lasso Regression
2023-04-27 16:30:30,937:INFO:Total runtime is 0.4897257089614868 minutes
2023-04-27 16:30:30,941:INFO:SubProcess create_model() called ==================================
2023-04-27 16:30:30,942:INFO:Initializing create_model()
2023-04-27 16:30:30,942:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:30:30,942:INFO:Checking exceptions
2023-04-27 16:30:30,942:INFO:Importing libraries
2023-04-27 16:30:30,943:INFO:Copying training dataset
2023-04-27 16:30:31,104:INFO:Defining folds
2023-04-27 16:30:31,105:INFO:Declaring metric variables
2023-04-27 16:30:31,111:INFO:Importing untrained model
2023-04-27 16:30:31,117:INFO:Lasso Regression Imported successfully
2023-04-27 16:30:31,125:INFO:Starting cross validation
2023-04-27 16:30:31,130:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:30:32,242:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:236: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  **fit_params_steps.get(name, {}),

2023-04-27 16:30:50,514:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.587e+12, tolerance: 1.377e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:30:51,471:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.533e+12, tolerance: 1.331e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:30:51,658:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.906e+12, tolerance: 1.369e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:30:51,963:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.115e+12, tolerance: 1.361e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:30:54,167:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.038e+12, tolerance: 1.354e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:30:55,193:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.497e+12, tolerance: 1.326e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:30:55,451:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.795e+12, tolerance: 1.335e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:30:55,696:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.937e+12, tolerance: 1.380e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:31:05,487:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.374e+12, tolerance: 1.312e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:31:06,027:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.172e+12, tolerance: 1.396e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:31:06,411:INFO:Calculating mean and std
2023-04-27 16:31:06,414:INFO:Creating metrics dataframe
2023-04-27 16:31:07,026:INFO:Uploading results into container
2023-04-27 16:31:07,026:INFO:Uploading model into container now
2023-04-27 16:31:07,027:INFO:_master_model_container: 2
2023-04-27 16:31:07,027:INFO:_display_container: 2
2023-04-27 16:31:07,028:INFO:Lasso(random_state=124)
2023-04-27 16:31:07,028:INFO:create_model() successfully completed......................................
2023-04-27 16:31:07,370:INFO:SubProcess create_model() end ==================================
2023-04-27 16:31:07,370:INFO:Creating metrics dataframe
2023-04-27 16:31:07,389:INFO:Initializing Ridge Regression
2023-04-27 16:31:07,389:INFO:Total runtime is 1.0972565452257792 minutes
2023-04-27 16:31:07,394:INFO:SubProcess create_model() called ==================================
2023-04-27 16:31:07,395:INFO:Initializing create_model()
2023-04-27 16:31:07,395:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:31:07,395:INFO:Checking exceptions
2023-04-27 16:31:07,395:INFO:Importing libraries
2023-04-27 16:31:07,395:INFO:Copying training dataset
2023-04-27 16:31:07,551:INFO:Defining folds
2023-04-27 16:31:07,551:INFO:Declaring metric variables
2023-04-27 16:31:07,556:INFO:Importing untrained model
2023-04-27 16:31:07,589:INFO:Ridge Regression Imported successfully
2023-04-27 16:31:07,598:INFO:Starting cross validation
2023-04-27 16:31:07,601:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:31:08,367:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=1.1083e-13): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T


2023-04-27 16:31:08,367:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=1.11004e-13): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 16:31:08,368:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=1.25905e-13): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 16:31:08,368:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=1.25093e-13): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 16:31:08,368:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=1.18895e-13): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 16:31:08,369:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=1.21769e-13): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 16:31:10,668:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=1.10007e-13): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 16:31:10,735:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=1.19348e-13): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-27 16:31:14,617:INFO:Calculating mean and std
2023-04-27 16:31:14,619:INFO:Creating metrics dataframe
2023-04-27 16:31:15,182:INFO:Uploading results into container
2023-04-27 16:31:15,182:INFO:Uploading model into container now
2023-04-27 16:31:15,183:INFO:_master_model_container: 3
2023-04-27 16:31:15,183:INFO:_display_container: 2
2023-04-27 16:31:15,183:INFO:Ridge(random_state=124)
2023-04-27 16:31:15,183:INFO:create_model() successfully completed......................................
2023-04-27 16:31:15,483:INFO:SubProcess create_model() end ==================================
2023-04-27 16:31:15,483:INFO:Creating metrics dataframe
2023-04-27 16:31:15,499:INFO:Initializing Elastic Net
2023-04-27 16:31:15,499:INFO:Total runtime is 1.2324231306711833 minutes
2023-04-27 16:31:15,502:INFO:SubProcess create_model() called ==================================
2023-04-27 16:31:15,503:INFO:Initializing create_model()
2023-04-27 16:31:15,503:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:31:15,503:INFO:Checking exceptions
2023-04-27 16:31:15,504:INFO:Importing libraries
2023-04-27 16:31:15,504:INFO:Copying training dataset
2023-04-27 16:31:15,659:INFO:Defining folds
2023-04-27 16:31:15,659:INFO:Declaring metric variables
2023-04-27 16:31:15,665:INFO:Importing untrained model
2023-04-27 16:31:15,669:INFO:Elastic Net Imported successfully
2023-04-27 16:31:15,678:INFO:Starting cross validation
2023-04-27 16:31:15,683:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:31:31,283:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.561e+13, tolerance: 1.369e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:31:33,489:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.382e+13, tolerance: 1.331e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:31:33,765:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.440e+13, tolerance: 1.326e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:31:34,998:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.520e+13, tolerance: 1.354e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:31:36,412:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.516e+13, tolerance: 1.380e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:31:36,502:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.547e+13, tolerance: 1.377e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:31:36,897:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.354e+13, tolerance: 1.361e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:31:40,787:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.464e+13, tolerance: 1.312e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:31:44,672:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.589e+13, tolerance: 1.396e+10
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2023-04-27 16:31:44,959:INFO:Calculating mean and std
2023-04-27 16:31:44,961:INFO:Creating metrics dataframe
2023-04-27 16:31:45,466:INFO:Uploading results into container
2023-04-27 16:31:45,467:INFO:Uploading model into container now
2023-04-27 16:31:45,468:INFO:_master_model_container: 4
2023-04-27 16:31:45,468:INFO:_display_container: 2
2023-04-27 16:31:45,468:INFO:ElasticNet(random_state=124)
2023-04-27 16:31:45,468:INFO:create_model() successfully completed......................................
2023-04-27 16:31:45,775:INFO:SubProcess create_model() end ==================================
2023-04-27 16:31:45,775:INFO:Creating metrics dataframe
2023-04-27 16:31:45,790:INFO:Initializing Least Angle Regression
2023-04-27 16:31:45,790:INFO:Total runtime is 1.7372732003529867 minutes
2023-04-27 16:31:45,796:INFO:SubProcess create_model() called ==================================
2023-04-27 16:31:45,796:INFO:Initializing create_model()
2023-04-27 16:31:45,797:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:31:45,797:INFO:Checking exceptions
2023-04-27 16:31:45,797:INFO:Importing libraries
2023-04-27 16:31:45,797:INFO:Copying training dataset
2023-04-27 16:31:45,947:INFO:Defining folds
2023-04-27 16:31:45,947:INFO:Declaring metric variables
2023-04-27 16:31:45,953:INFO:Importing untrained model
2023-04-27 16:31:45,958:INFO:Least Angle Regression Imported successfully
2023-04-27 16:31:45,966:INFO:Starting cross validation
2023-04-27 16:31:45,970:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:31:46,414:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:31:46,414:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:31:46,431:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:31:46,536:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:31:46,539:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:31:46,547:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:31:46,550:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:31:46,610:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:31:48,989:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:31:49,041:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:31:52,777:INFO:Calculating mean and std
2023-04-27 16:31:52,779:INFO:Creating metrics dataframe
2023-04-27 16:31:53,308:INFO:Uploading results into container
2023-04-27 16:31:53,309:INFO:Uploading model into container now
2023-04-27 16:31:53,309:INFO:_master_model_container: 5
2023-04-27 16:31:53,309:INFO:_display_container: 2
2023-04-27 16:31:53,311:INFO:Lars(random_state=124)
2023-04-27 16:31:53,311:INFO:create_model() successfully completed......................................
2023-04-27 16:31:53,621:INFO:SubProcess create_model() end ==================================
2023-04-27 16:31:53,622:INFO:Creating metrics dataframe
2023-04-27 16:31:53,640:INFO:Initializing Lasso Least Angle Regression
2023-04-27 16:31:53,640:INFO:Total runtime is 1.868106496334076 minutes
2023-04-27 16:31:53,644:INFO:SubProcess create_model() called ==================================
2023-04-27 16:31:53,644:INFO:Initializing create_model()
2023-04-27 16:31:53,645:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:31:53,645:INFO:Checking exceptions
2023-04-27 16:31:53,645:INFO:Importing libraries
2023-04-27 16:31:53,645:INFO:Copying training dataset
2023-04-27 16:31:53,789:INFO:Defining folds
2023-04-27 16:31:53,789:INFO:Declaring metric variables
2023-04-27 16:31:53,795:INFO:Importing untrained model
2023-04-27 16:31:53,799:INFO:Lasso Least Angle Regression Imported successfully
2023-04-27 16:31:53,806:INFO:Starting cross validation
2023-04-27 16:31:53,810:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:31:54,120:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 16:31:54,150:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 16:31:54,156:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 16:31:54,316:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 16:31:54,378:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 16:31:54,427:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 16:31:54,660:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 16:31:54,694:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 16:31:56,177:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 16:31:56,341:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2023-04-27 16:32:00,027:INFO:Calculating mean and std
2023-04-27 16:32:00,030:INFO:Creating metrics dataframe
2023-04-27 16:32:00,527:INFO:Uploading results into container
2023-04-27 16:32:00,528:INFO:Uploading model into container now
2023-04-27 16:32:00,529:INFO:_master_model_container: 6
2023-04-27 16:32:00,529:INFO:_display_container: 2
2023-04-27 16:32:00,529:INFO:LassoLars(random_state=124)
2023-04-27 16:32:00,529:INFO:create_model() successfully completed......................................
2023-04-27 16:32:00,834:INFO:SubProcess create_model() end ==================================
2023-04-27 16:32:00,834:INFO:Creating metrics dataframe
2023-04-27 16:32:00,849:INFO:Initializing Orthogonal Matching Pursuit
2023-04-27 16:32:00,850:INFO:Total runtime is 1.9882731477419537 minutes
2023-04-27 16:32:00,855:INFO:SubProcess create_model() called ==================================
2023-04-27 16:32:00,856:INFO:Initializing create_model()
2023-04-27 16:32:00,856:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:32:00,856:INFO:Checking exceptions
2023-04-27 16:32:00,856:INFO:Importing libraries
2023-04-27 16:32:00,856:INFO:Copying training dataset
2023-04-27 16:32:01,001:INFO:Defining folds
2023-04-27 16:32:01,002:INFO:Declaring metric variables
2023-04-27 16:32:01,006:INFO:Importing untrained model
2023-04-27 16:32:01,011:INFO:Orthogonal Matching Pursuit Imported successfully
2023-04-27 16:32:01,020:INFO:Starting cross validation
2023-04-27 16:32:01,026:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:32:01,318:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:32:01,377:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:32:01,420:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:32:01,443:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:32:01,453:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:32:01,461:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:32:01,464:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:32:01,478:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:32:03,291:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:32:03,311:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2023-04-27 16:32:06,806:INFO:Calculating mean and std
2023-04-27 16:32:06,807:INFO:Creating metrics dataframe
2023-04-27 16:32:07,297:INFO:Uploading results into container
2023-04-27 16:32:07,298:INFO:Uploading model into container now
2023-04-27 16:32:07,298:INFO:_master_model_container: 7
2023-04-27 16:32:07,298:INFO:_display_container: 2
2023-04-27 16:32:07,299:INFO:OrthogonalMatchingPursuit()
2023-04-27 16:32:07,299:INFO:create_model() successfully completed......................................
2023-04-27 16:32:07,579:INFO:SubProcess create_model() end ==================================
2023-04-27 16:32:07,579:INFO:Creating metrics dataframe
2023-04-27 16:32:07,595:INFO:Initializing Bayesian Ridge
2023-04-27 16:32:07,595:INFO:Total runtime is 2.100690265496572 minutes
2023-04-27 16:32:07,601:INFO:SubProcess create_model() called ==================================
2023-04-27 16:32:07,602:INFO:Initializing create_model()
2023-04-27 16:32:07,602:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:32:07,602:INFO:Checking exceptions
2023-04-27 16:32:07,602:INFO:Importing libraries
2023-04-27 16:32:07,602:INFO:Copying training dataset
2023-04-27 16:32:07,737:INFO:Defining folds
2023-04-27 16:32:07,737:INFO:Declaring metric variables
2023-04-27 16:32:07,741:INFO:Importing untrained model
2023-04-27 16:32:07,746:INFO:Bayesian Ridge Imported successfully
2023-04-27 16:32:07,758:INFO:Starting cross validation
2023-04-27 16:32:07,761:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:32:21,862:INFO:Calculating mean and std
2023-04-27 16:32:21,864:INFO:Creating metrics dataframe
2023-04-27 16:32:22,470:INFO:Uploading results into container
2023-04-27 16:32:22,470:INFO:Uploading model into container now
2023-04-27 16:32:22,471:INFO:_master_model_container: 8
2023-04-27 16:32:22,471:INFO:_display_container: 2
2023-04-27 16:32:22,472:INFO:BayesianRidge()
2023-04-27 16:32:22,472:INFO:create_model() successfully completed......................................
2023-04-27 16:32:22,800:INFO:SubProcess create_model() end ==================================
2023-04-27 16:32:22,800:INFO:Creating metrics dataframe
2023-04-27 16:32:22,816:INFO:Initializing Passive Aggressive Regressor
2023-04-27 16:32:22,816:INFO:Total runtime is 2.3543731331825257 minutes
2023-04-27 16:32:22,820:INFO:SubProcess create_model() called ==================================
2023-04-27 16:32:22,821:INFO:Initializing create_model()
2023-04-27 16:32:22,821:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:32:22,821:INFO:Checking exceptions
2023-04-27 16:32:22,821:INFO:Importing libraries
2023-04-27 16:32:22,821:INFO:Copying training dataset
2023-04-27 16:32:22,961:INFO:Defining folds
2023-04-27 16:32:22,961:INFO:Declaring metric variables
2023-04-27 16:32:22,966:INFO:Importing untrained model
2023-04-27 16:32:22,971:INFO:Passive Aggressive Regressor Imported successfully
2023-04-27 16:32:22,980:INFO:Starting cross validation
2023-04-27 16:32:22,982:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:32:30,379:INFO:Calculating mean and std
2023-04-27 16:32:30,381:INFO:Creating metrics dataframe
2023-04-27 16:32:30,915:INFO:Uploading results into container
2023-04-27 16:32:30,916:INFO:Uploading model into container now
2023-04-27 16:32:30,916:INFO:_master_model_container: 9
2023-04-27 16:32:30,916:INFO:_display_container: 2
2023-04-27 16:32:30,917:INFO:PassiveAggressiveRegressor(random_state=124)
2023-04-27 16:32:30,917:INFO:create_model() successfully completed......................................
2023-04-27 16:32:31,226:INFO:SubProcess create_model() end ==================================
2023-04-27 16:32:31,226:INFO:Creating metrics dataframe
2023-04-27 16:32:31,240:INFO:Initializing Huber Regressor
2023-04-27 16:32:31,240:INFO:Total runtime is 2.4947733998298647 minutes
2023-04-27 16:32:31,245:INFO:SubProcess create_model() called ==================================
2023-04-27 16:32:31,245:INFO:Initializing create_model()
2023-04-27 16:32:31,245:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:32:31,245:INFO:Checking exceptions
2023-04-27 16:32:31,246:INFO:Importing libraries
2023-04-27 16:32:31,246:INFO:Copying training dataset
2023-04-27 16:32:31,406:INFO:Defining folds
2023-04-27 16:32:31,407:INFO:Declaring metric variables
2023-04-27 16:32:31,411:INFO:Importing untrained model
2023-04-27 16:32:31,416:INFO:Huber Regressor Imported successfully
2023-04-27 16:32:31,424:INFO:Starting cross validation
2023-04-27 16:32:31,428:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:33:00,152:INFO:Calculating mean and std
2023-04-27 16:33:00,155:INFO:Creating metrics dataframe
2023-04-27 16:33:00,805:INFO:Uploading results into container
2023-04-27 16:33:00,806:INFO:Uploading model into container now
2023-04-27 16:33:00,807:INFO:_master_model_container: 10
2023-04-27 16:33:00,807:INFO:_display_container: 2
2023-04-27 16:33:00,807:INFO:HuberRegressor()
2023-04-27 16:33:00,807:INFO:create_model() successfully completed......................................
2023-04-27 16:33:01,168:INFO:SubProcess create_model() end ==================================
2023-04-27 16:33:01,168:INFO:Creating metrics dataframe
2023-04-27 16:33:01,188:INFO:Initializing K Neighbors Regressor
2023-04-27 16:33:01,188:INFO:Total runtime is 2.9939064582188926 minutes
2023-04-27 16:33:01,195:INFO:SubProcess create_model() called ==================================
2023-04-27 16:33:01,196:INFO:Initializing create_model()
2023-04-27 16:33:01,196:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:33:01,196:INFO:Checking exceptions
2023-04-27 16:33:01,196:INFO:Importing libraries
2023-04-27 16:33:01,196:INFO:Copying training dataset
2023-04-27 16:33:01,356:INFO:Defining folds
2023-04-27 16:33:01,357:INFO:Declaring metric variables
2023-04-27 16:33:01,362:INFO:Importing untrained model
2023-04-27 16:33:01,367:INFO:K Neighbors Regressor Imported successfully
2023-04-27 16:33:01,377:INFO:Starting cross validation
2023-04-27 16:33:01,379:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:33:03,446:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.29s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:33:03,536:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-27 16:33:03,552:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:33:03,558:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-27 16:33:03,649:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:33:04,397:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:33:04,451:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:33:18,278:INFO:Calculating mean and std
2023-04-27 16:33:18,282:INFO:Creating metrics dataframe
2023-04-27 16:33:19,524:INFO:Uploading results into container
2023-04-27 16:33:19,526:INFO:Uploading model into container now
2023-04-27 16:33:19,527:INFO:_master_model_container: 11
2023-04-27 16:33:19,527:INFO:_display_container: 2
2023-04-27 16:33:19,527:INFO:KNeighborsRegressor(n_jobs=-1)
2023-04-27 16:33:19,528:INFO:create_model() successfully completed......................................
2023-04-27 16:33:20,036:INFO:SubProcess create_model() end ==================================
2023-04-27 16:33:20,037:INFO:Creating metrics dataframe
2023-04-27 16:33:20,071:INFO:Initializing Decision Tree Regressor
2023-04-27 16:33:20,071:INFO:Total runtime is 3.3086275299390158 minutes
2023-04-27 16:33:20,080:INFO:SubProcess create_model() called ==================================
2023-04-27 16:33:20,080:INFO:Initializing create_model()
2023-04-27 16:33:20,080:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:33:20,080:INFO:Checking exceptions
2023-04-27 16:33:20,081:INFO:Importing libraries
2023-04-27 16:33:20,081:INFO:Copying training dataset
2023-04-27 16:33:20,367:INFO:Defining folds
2023-04-27 16:33:20,367:INFO:Declaring metric variables
2023-04-27 16:33:20,377:INFO:Importing untrained model
2023-04-27 16:33:20,385:INFO:Decision Tree Regressor Imported successfully
2023-04-27 16:33:20,405:INFO:Starting cross validation
2023-04-27 16:33:20,412:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:33:22,557:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:33:22,919:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:33:23,247:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:33:23,548:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:33:33,331:INFO:Calculating mean and std
2023-04-27 16:33:33,334:INFO:Creating metrics dataframe
2023-04-27 16:33:34,487:INFO:Uploading results into container
2023-04-27 16:33:34,488:INFO:Uploading model into container now
2023-04-27 16:33:34,489:INFO:_master_model_container: 12
2023-04-27 16:33:34,490:INFO:_display_container: 2
2023-04-27 16:33:34,490:INFO:DecisionTreeRegressor(random_state=124)
2023-04-27 16:33:34,490:INFO:create_model() successfully completed......................................
2023-04-27 16:33:34,994:INFO:SubProcess create_model() end ==================================
2023-04-27 16:33:34,994:INFO:Creating metrics dataframe
2023-04-27 16:33:35,039:INFO:Initializing Random Forest Regressor
2023-04-27 16:33:35,040:INFO:Total runtime is 3.5581113974253338 minutes
2023-04-27 16:33:35,047:INFO:SubProcess create_model() called ==================================
2023-04-27 16:33:35,048:INFO:Initializing create_model()
2023-04-27 16:33:35,049:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:33:35,049:INFO:Checking exceptions
2023-04-27 16:33:35,049:INFO:Importing libraries
2023-04-27 16:33:35,049:INFO:Copying training dataset
2023-04-27 16:33:35,320:INFO:Defining folds
2023-04-27 16:33:35,321:INFO:Declaring metric variables
2023-04-27 16:33:35,329:INFO:Importing untrained model
2023-04-27 16:33:35,339:INFO:Random Forest Regressor Imported successfully
2023-04-27 16:33:35,355:INFO:Starting cross validation
2023-04-27 16:33:35,362:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:33:37,631:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 1.23s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 16:34:02,354:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:34:03,136:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.24s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:34:03,332:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:34:03,363:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:34:03,666:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:34:03,822:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-27 16:34:04,396:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:34:04,651:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-27 16:34:04,693:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-27 16:34:04,766:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:34:05,345:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:34:17,400:INFO:Calculating mean and std
2023-04-27 16:34:17,403:INFO:Creating metrics dataframe
2023-04-27 16:34:17,934:INFO:Uploading results into container
2023-04-27 16:34:17,934:INFO:Uploading model into container now
2023-04-27 16:34:17,935:INFO:_master_model_container: 13
2023-04-27 16:34:17,935:INFO:_display_container: 2
2023-04-27 16:34:17,935:INFO:RandomForestRegressor(n_jobs=-1, random_state=124)
2023-04-27 16:34:17,935:INFO:create_model() successfully completed......................................
2023-04-27 16:34:18,263:INFO:SubProcess create_model() end ==================================
2023-04-27 16:34:18,263:INFO:Creating metrics dataframe
2023-04-27 16:34:18,285:INFO:Initializing Extra Trees Regressor
2023-04-27 16:34:18,286:INFO:Total runtime is 4.278882745901743 minutes
2023-04-27 16:34:18,290:INFO:SubProcess create_model() called ==================================
2023-04-27 16:34:18,291:INFO:Initializing create_model()
2023-04-27 16:34:18,291:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:34:18,291:INFO:Checking exceptions
2023-04-27 16:34:18,291:INFO:Importing libraries
2023-04-27 16:34:18,291:INFO:Copying training dataset
2023-04-27 16:34:18,455:INFO:Defining folds
2023-04-27 16:34:18,456:INFO:Declaring metric variables
2023-04-27 16:34:18,460:INFO:Importing untrained model
2023-04-27 16:34:18,467:INFO:Extra Trees Regressor Imported successfully
2023-04-27 16:34:18,475:INFO:Starting cross validation
2023-04-27 16:34:18,479:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:35:02,778:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:35:03,405:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 2.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:35:03,903:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 2.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:35:05,301:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:35:05,609:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:35:06,037:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:35:23,920:INFO:Calculating mean and std
2023-04-27 16:35:23,924:INFO:Creating metrics dataframe
2023-04-27 16:35:25,010:INFO:Uploading results into container
2023-04-27 16:35:25,011:INFO:Uploading model into container now
2023-04-27 16:35:25,012:INFO:_master_model_container: 14
2023-04-27 16:35:25,012:INFO:_display_container: 2
2023-04-27 16:35:25,012:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=124)
2023-04-27 16:35:25,012:INFO:create_model() successfully completed......................................
2023-04-27 16:35:25,452:INFO:SubProcess create_model() end ==================================
2023-04-27 16:35:25,452:INFO:Creating metrics dataframe
2023-04-27 16:35:25,481:INFO:Initializing AdaBoost Regressor
2023-04-27 16:35:25,482:INFO:Total runtime is 5.398816827932993 minutes
2023-04-27 16:35:25,489:INFO:SubProcess create_model() called ==================================
2023-04-27 16:35:25,490:INFO:Initializing create_model()
2023-04-27 16:35:25,490:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:35:25,491:INFO:Checking exceptions
2023-04-27 16:35:25,491:INFO:Importing libraries
2023-04-27 16:35:25,491:INFO:Copying training dataset
2023-04-27 16:35:25,724:INFO:Defining folds
2023-04-27 16:35:25,725:INFO:Declaring metric variables
2023-04-27 16:35:25,731:INFO:Importing untrained model
2023-04-27 16:35:25,737:INFO:AdaBoost Regressor Imported successfully
2023-04-27 16:35:25,748:INFO:Starting cross validation
2023-04-27 16:35:25,754:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:35:50,917:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:36:16,726:INFO:Calculating mean and std
2023-04-27 16:36:16,730:INFO:Creating metrics dataframe
2023-04-27 16:36:17,727:INFO:Uploading results into container
2023-04-27 16:36:17,728:INFO:Uploading model into container now
2023-04-27 16:36:17,728:INFO:_master_model_container: 15
2023-04-27 16:36:17,729:INFO:_display_container: 2
2023-04-27 16:36:17,729:INFO:AdaBoostRegressor(random_state=124)
2023-04-27 16:36:17,729:INFO:create_model() successfully completed......................................
2023-04-27 16:36:18,185:INFO:SubProcess create_model() end ==================================
2023-04-27 16:36:18,185:INFO:Creating metrics dataframe
2023-04-27 16:36:18,220:INFO:Initializing Gradient Boosting Regressor
2023-04-27 16:36:18,220:INFO:Total runtime is 6.277781764666239 minutes
2023-04-27 16:36:18,229:INFO:SubProcess create_model() called ==================================
2023-04-27 16:36:18,229:INFO:Initializing create_model()
2023-04-27 16:36:18,230:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:36:18,230:INFO:Checking exceptions
2023-04-27 16:36:18,230:INFO:Importing libraries
2023-04-27 16:36:18,230:INFO:Copying training dataset
2023-04-27 16:36:18,542:INFO:Defining folds
2023-04-27 16:36:18,542:INFO:Declaring metric variables
2023-04-27 16:36:18,549:INFO:Importing untrained model
2023-04-27 16:36:18,557:INFO:Gradient Boosting Regressor Imported successfully
2023-04-27 16:36:18,571:INFO:Starting cross validation
2023-04-27 16:36:18,578:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:36:40,560:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:36:40,598:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:36:41,534:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:36:42,258:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:36:55,939:INFO:Calculating mean and std
2023-04-27 16:36:55,941:INFO:Creating metrics dataframe
2023-04-27 16:36:56,557:INFO:Uploading results into container
2023-04-27 16:36:56,558:INFO:Uploading model into container now
2023-04-27 16:36:56,558:INFO:_master_model_container: 16
2023-04-27 16:36:56,558:INFO:_display_container: 2
2023-04-27 16:36:56,559:INFO:GradientBoostingRegressor(random_state=124)
2023-04-27 16:36:56,559:INFO:create_model() successfully completed......................................
2023-04-27 16:36:56,868:INFO:SubProcess create_model() end ==================================
2023-04-27 16:36:56,868:INFO:Creating metrics dataframe
2023-04-27 16:36:56,890:INFO:Initializing Extreme Gradient Boosting
2023-04-27 16:36:56,890:INFO:Total runtime is 6.922270683447519 minutes
2023-04-27 16:36:56,897:INFO:SubProcess create_model() called ==================================
2023-04-27 16:36:56,898:INFO:Initializing create_model()
2023-04-27 16:36:56,898:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:36:56,898:INFO:Checking exceptions
2023-04-27 16:36:56,898:INFO:Importing libraries
2023-04-27 16:36:56,898:INFO:Copying training dataset
2023-04-27 16:36:57,056:INFO:Defining folds
2023-04-27 16:36:57,057:INFO:Declaring metric variables
2023-04-27 16:36:57,061:INFO:Importing untrained model
2023-04-27 16:36:57,067:INFO:Extreme Gradient Boosting Imported successfully
2023-04-27 16:36:57,077:INFO:Starting cross validation
2023-04-27 16:36:57,080:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:39:12,041:INFO:Calculating mean and std
2023-04-27 16:39:12,044:INFO:Creating metrics dataframe
2023-04-27 16:39:12,797:INFO:Uploading results into container
2023-04-27 16:39:12,798:INFO:Uploading model into container now
2023-04-27 16:39:12,799:INFO:_master_model_container: 17
2023-04-27 16:39:12,799:INFO:_display_container: 2
2023-04-27 16:39:12,800:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, gamma=None,
             gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, n_estimators=100, n_jobs=-1,
             num_parallel_tree=None, predictor=None, random_state=124,
             reg_alpha=None, reg_lambda=None, ...)
2023-04-27 16:39:12,801:INFO:create_model() successfully completed......................................
2023-04-27 16:39:13,178:INFO:SubProcess create_model() end ==================================
2023-04-27 16:39:13,178:INFO:Creating metrics dataframe
2023-04-27 16:39:13,205:INFO:Initializing Light Gradient Boosting Machine
2023-04-27 16:39:13,206:INFO:Total runtime is 9.194206380844115 minutes
2023-04-27 16:39:13,212:INFO:SubProcess create_model() called ==================================
2023-04-27 16:39:13,212:INFO:Initializing create_model()
2023-04-27 16:39:13,212:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:39:13,213:INFO:Checking exceptions
2023-04-27 16:39:13,213:INFO:Importing libraries
2023-04-27 16:39:13,213:INFO:Copying training dataset
2023-04-27 16:39:13,406:INFO:Defining folds
2023-04-27 16:39:13,406:INFO:Declaring metric variables
2023-04-27 16:39:13,412:INFO:Importing untrained model
2023-04-27 16:39:13,420:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-27 16:39:13,430:INFO:Starting cross validation
2023-04-27 16:39:13,436:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:39:22,655:INFO:Calculating mean and std
2023-04-27 16:39:22,658:INFO:Creating metrics dataframe
2023-04-27 16:39:23,260:INFO:Uploading results into container
2023-04-27 16:39:23,261:INFO:Uploading model into container now
2023-04-27 16:39:23,261:INFO:_master_model_container: 18
2023-04-27 16:39:23,261:INFO:_display_container: 2
2023-04-27 16:39:23,262:INFO:LGBMRegressor(random_state=124)
2023-04-27 16:39:23,262:INFO:create_model() successfully completed......................................
2023-04-27 16:39:23,579:INFO:SubProcess create_model() end ==================================
2023-04-27 16:39:23,579:INFO:Creating metrics dataframe
2023-04-27 16:39:23,601:INFO:Initializing Dummy Regressor
2023-04-27 16:39:23,602:INFO:Total runtime is 9.367471094926197 minutes
2023-04-27 16:39:23,607:INFO:SubProcess create_model() called ==================================
2023-04-27 16:39:23,607:INFO:Initializing create_model()
2023-04-27 16:39:23,607:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020525D95E48>, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:39:23,607:INFO:Checking exceptions
2023-04-27 16:39:23,607:INFO:Importing libraries
2023-04-27 16:39:23,607:INFO:Copying training dataset
2023-04-27 16:39:23,763:INFO:Defining folds
2023-04-27 16:39:23,763:INFO:Declaring metric variables
2023-04-27 16:39:23,768:INFO:Importing untrained model
2023-04-27 16:39:23,775:INFO:Dummy Regressor Imported successfully
2023-04-27 16:39:23,785:INFO:Starting cross validation
2023-04-27 16:39:23,788:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-27 16:39:29,757:INFO:Calculating mean and std
2023-04-27 16:39:29,759:INFO:Creating metrics dataframe
2023-04-27 16:39:30,320:INFO:Uploading results into container
2023-04-27 16:39:30,320:INFO:Uploading model into container now
2023-04-27 16:39:30,321:INFO:_master_model_container: 19
2023-04-27 16:39:30,321:INFO:_display_container: 2
2023-04-27 16:39:30,321:INFO:DummyRegressor()
2023-04-27 16:39:30,321:INFO:create_model() successfully completed......................................
2023-04-27 16:39:30,628:INFO:SubProcess create_model() end ==================================
2023-04-27 16:39:30,628:INFO:Creating metrics dataframe
2023-04-27 16:39:30,664:INFO:Initializing create_model()
2023-04-27 16:39:30,664:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=124), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-27 16:39:30,664:INFO:Checking exceptions
2023-04-27 16:39:30,669:INFO:Importing libraries
2023-04-27 16:39:30,669:INFO:Copying training dataset
2023-04-27 16:39:30,822:INFO:Defining folds
2023-04-27 16:39:30,822:INFO:Declaring metric variables
2023-04-27 16:39:30,822:INFO:Importing untrained model
2023-04-27 16:39:30,822:INFO:Declaring custom model
2023-04-27 16:39:30,824:INFO:Extra Trees Regressor Imported successfully
2023-04-27 16:39:30,830:INFO:Cross validation set to False
2023-04-27 16:39:30,830:INFO:Fitting Model
2023-04-27 16:39:39,045:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=124)
2023-04-27 16:39:39,045:INFO:create_model() successfully completed......................................
2023-04-27 16:39:39,381:INFO:Creating Dashboard logs
2023-04-27 16:39:39,387:INFO:Model: Extra Trees Regressor
2023-04-27 16:39:39,439:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 124, 'verbose': 0, 'warm_start': False}
2023-04-27 16:39:39,502:INFO:Initializing predict_model()
2023-04-27 16:39:39,502:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=124), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000020532DDC678>)
2023-04-27 16:39:39,502:INFO:Checking exceptions
2023-04-27 16:39:39,502:INFO:Preloading libraries
2023-04-27 16:39:41,553:INFO:Creating Dashboard logs
2023-04-27 16:39:41,558:INFO:Model: Random Forest Regressor
2023-04-27 16:39:41,594:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 124, 'verbose': 0, 'warm_start': False}
2023-04-27 16:39:42,529:INFO:Creating Dashboard logs
2023-04-27 16:39:42,539:INFO:Model: Extreme Gradient Boosting
2023-04-27 16:39:42,593:INFO:Logged params: {'objective': 'reg:squarederror', 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': -1, 'num_parallel_tree': None, 'predictor': None, 'random_state': 124, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'auto', 'validate_parameters': None, 'verbosity': 0}
2023-04-27 16:39:43,524:INFO:Creating Dashboard logs
2023-04-27 16:39:43,532:INFO:Model: Gradient Boosting Regressor
2023-04-27 16:39:43,571:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 124, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-04-27 16:39:44,832:INFO:Creating Dashboard logs
2023-04-27 16:39:44,838:INFO:Model: Light Gradient Boosting Machine
2023-04-27 16:39:44,875:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 124, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2023-04-27 16:39:45,985:INFO:Creating Dashboard logs
2023-04-27 16:39:45,990:INFO:Model: Decision Tree Regressor
2023-04-27 16:39:46,028:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 124, 'splitter': 'best'}
2023-04-27 16:39:46,890:INFO:Creating Dashboard logs
2023-04-27 16:39:46,895:INFO:Model: Ridge Regression
2023-04-27 16:39:46,928:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': 'deprecated', 'positive': False, 'random_state': 124, 'solver': 'auto', 'tol': 0.001}
2023-04-27 16:39:47,780:INFO:Creating Dashboard logs
2023-04-27 16:39:47,785:INFO:Model: Lasso Least Angle Regression
2023-04-27 16:39:47,825:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'normalize': 'deprecated', 'positive': False, 'precompute': 'auto', 'random_state': 124, 'verbose': False}
2023-04-27 16:39:49,000:INFO:Creating Dashboard logs
2023-04-27 16:39:49,005:INFO:Model: Orthogonal Matching Pursuit
2023-04-27 16:39:49,043:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'normalize': 'deprecated', 'precompute': 'auto', 'tol': None}
2023-04-27 16:39:50,179:INFO:Creating Dashboard logs
2023-04-27 16:39:50,183:INFO:Model: Bayesian Ridge
2023-04-27 16:39:50,224:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'normalize': 'deprecated', 'tol': 0.001, 'verbose': False}
2023-04-27 16:39:51,078:INFO:Creating Dashboard logs
2023-04-27 16:39:51,082:INFO:Model: Lasso Regression
2023-04-27 16:39:51,115:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': 'deprecated', 'positive': False, 'precompute': False, 'random_state': 124, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2023-04-27 16:39:51,956:INFO:Creating Dashboard logs
2023-04-27 16:39:51,960:INFO:Model: K Neighbors Regressor
2023-04-27 16:39:51,999:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2023-04-27 16:39:53,043:INFO:Creating Dashboard logs
2023-04-27 16:39:53,047:INFO:Model: Linear Regression
2023-04-27 16:39:53,083:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'normalize': 'deprecated', 'positive': False}
2023-04-27 16:39:54,204:INFO:Creating Dashboard logs
2023-04-27 16:39:54,209:INFO:Model: Elastic Net
2023-04-27 16:39:54,244:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'normalize': 'deprecated', 'positive': False, 'precompute': False, 'random_state': 124, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2023-04-27 16:39:55,101:INFO:Creating Dashboard logs
2023-04-27 16:39:55,107:INFO:Model: AdaBoost Regressor
2023-04-27 16:39:55,146:INFO:Logged params: {'base_estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 124}
2023-04-27 16:39:56,021:INFO:Creating Dashboard logs
2023-04-27 16:39:56,027:INFO:Model: Huber Regressor
2023-04-27 16:39:56,063:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2023-04-27 16:39:57,051:INFO:Creating Dashboard logs
2023-04-27 16:39:57,057:INFO:Model: Least Angle Regression
2023-04-27 16:39:57,094:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'normalize': 'deprecated', 'precompute': 'auto', 'random_state': 124, 'verbose': False}
2023-04-27 16:39:58,168:INFO:Creating Dashboard logs
2023-04-27 16:39:58,173:INFO:Model: Dummy Regressor
2023-04-27 16:39:58,211:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2023-04-27 16:39:59,112:INFO:Creating Dashboard logs
2023-04-27 16:39:59,117:INFO:Model: Passive Aggressive Regressor
2023-04-27 16:39:59,153:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 124, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-04-27 16:40:00,024:INFO:_master_model_container: 19
2023-04-27 16:40:00,025:INFO:_display_container: 2
2023-04-27 16:40:00,025:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=124)
2023-04-27 16:40:00,025:INFO:compare_models() successfully completed......................................
2023-04-27 16:50:45,043:INFO:Initializing tune_model()
2023-04-27 16:50:45,044:INFO:tune_model(estimator=ExtraTreesRegressor(n_jobs=-1, random_state=124), fold=None, round=4, n_iter=10, custom_grid=None, optimize=R2, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000020553B871C8>)
2023-04-27 16:50:45,044:INFO:Checking exceptions
2023-04-27 16:50:45,157:INFO:Copying training dataset
2023-04-27 16:50:45,272:INFO:Checking base model
2023-04-27 16:50:45,273:INFO:Base model : Extra Trees Regressor
2023-04-27 16:50:45,278:INFO:Declaring metric variables
2023-04-27 16:50:45,282:INFO:Defining Hyperparameters
2023-04-27 16:50:45,624:INFO:Tuning with n_jobs=-1
2023-04-27 16:50:45,624:INFO:Initializing RandomizedSearchCV
2023-04-27 16:50:56,181:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:51:01,320:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:51:01,675:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:51:07,230:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 4.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-27 16:51:11,767:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 8.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-27 16:51:13,857:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 6.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-27 16:51:14,744:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-27 16:51:15,653:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:51:16,513:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.46s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:51:33,844:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 16:51:34,804:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:51:34,804:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:51:34,806:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:51:34,813:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:236: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  **fit_params_steps.get(name, {}),

2023-04-27 16:51:34,899:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:51:35,600:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.36s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:51:36,266:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 16:51:36,591:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:51:36,803:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-04-27 16:51:37,686:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.33s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:51:37,689:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.44s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:51:37,844:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.38s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:51:37,980:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:51:38,103:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.20s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:51:38,906:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.23s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:51:46,971:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:236: UserWarning: Persisting input arguments took 1.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  **fit_params_steps.get(name, {}),

2023-04-27 16:51:47,124:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:236: UserWarning: Persisting input arguments took 1.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  **fit_params_steps.get(name, {}),

2023-04-27 16:51:47,507:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 1.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 16:51:47,897:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 16:51:48,250:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 16:51:48,403:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 16:51:49,076:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 1.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:51:53,914:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:51:55,988:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 3.20s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:52:02,636:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:52:11,446:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 1.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 16:52:13,643:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:236: UserWarning: Persisting input arguments took 1.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  **fit_params_steps.get(name, {}),

2023-04-27 16:52:20,267:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 1.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 16:52:22,279:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:236: UserWarning: Persisting input arguments took 1.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  **fit_params_steps.get(name, {}),

2023-04-27 16:52:27,343:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 3.31s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:52:28,851:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:261: UserWarning: Persisting input arguments took 3.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  clone(self.steps[-1][1]), X, y, **fit_params_last_step

2023-04-27 16:52:32,859:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:52:34,304:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-04-27 16:52:52,594:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:241: UserWarning: Persisting input arguments took 1.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  y=y,

2023-04-27 16:52:54,688:WARNING:c:\Users\tonim\AppData\Local\Programs\Python\Python37\lib\site-packages\pycaret\internal\pipeline.py:236: UserWarning: Persisting input arguments took 1.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  **fit_params_steps.get(name, {}),

